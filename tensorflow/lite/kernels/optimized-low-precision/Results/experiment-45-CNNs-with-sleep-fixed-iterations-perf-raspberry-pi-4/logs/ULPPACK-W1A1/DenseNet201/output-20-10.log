STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 160)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 160)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (3136, 64, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
(3136, 128, ), and the ID is 1
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 2
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 96, ), Input shape (3136, 96, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
(3136, 128, ), and the ID is 3
	Allocating LowPrecision Activations Tensors with Shape of (3136, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (3136, 128, ), and Output shape (3136, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
, and the ID is 5
	Allocating LowPrecision Activations Tensors with Shape of (3136, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
, and the ID is 6
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (3136, 160, ), and Output shape (3136, 128, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (3136, 192, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
(3136, 128, ), and the ID is 9
	Allocating LowPrecision Activations Tensors with Shape of (3136, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (3136, 224, ), and Output shape (3136, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
, and the ID is 11
	Allocating LowPrecision Activations Tensors with Shape of (3136, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (3136, 256, ), and Output shape (3136, 128, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 256)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 14	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)

	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 15
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (784, 160, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (784, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (784, 192, ), and Output shape (784, 128, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (784, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (784, 224, ), and Output shape (784, 128, ), and the ID is 20	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)

	Allocating LowPrecision Activations Tensors with Shape of (784, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
22
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 23
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (784, 288, ), and Output shape (784, 128, ), and the ID is 24
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
	Allocating LowPrecision Activations Tensors with Shape of (784, 288)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 25
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (784, 320, ), and Output shape (784, 128, ), and the ID is 26
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 320)
	Allocating LowPrecision Activations Tensors with Shape of (784, 320)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 27
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (784, 352, ), and Output shape (784, 128, ), and the ID is 28
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 352)
	Allocating LowPrecision Activations Tensors with Shape of (784, 352)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (784, 384, ), and Output shape (784, 128, ), and the ID is 30	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 384)

	Allocating LowPrecision Activations Tensors with Shape of (784, 384)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 31
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (784, 416, ), and Output shape (784, 128, ), and the ID is 32
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 416)
	Allocating LowPrecision Activations Tensors with Shape of (784, 416)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 33
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (784, 448, ), and Output shape (784, 128, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (784, 448)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (784, 480, ), and Output shape (784, 128, ), and the ID is 36
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (784, 480)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 37
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (784, 512, ), and Output shape (784, 256, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (196, 256, ), and Output shape (196, 128, ), and the ID is 39	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)

	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 40
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (196, 288, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
41
	Allocating LowPrecision Activations Tensors with Shape of (200, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 42
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (196, 320, ), and Output shape (196, 128, ), and the ID is 43
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 320)
	Allocating LowPrecision Activations Tensors with Shape of (200, 320)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 44
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (196, 352, ), and Output shape (196, 128, ), and the ID is 45
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 352)
	Allocating LowPrecision Activations Tensors with Shape of (200, 352)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 46
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (196, 384, ), and Output shape (196, 128, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 384)
	Allocating LowPrecision Activations Tensors with Shape of (200, 384)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 48
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (196, 416, ), and Output shape (196, 128, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 416)
	Allocating LowPrecision Activations Tensors with Shape of (200, 416)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 50
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (196, 448, ), and Output shape (196, 128, ), and the ID is 51
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (200, 448)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 52
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (196, 480, ), and Output shape (196, 128, ), and the ID is 53
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (200, 480)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 54
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (196, 512, ), and Output shape (196, 128, ), and the ID is 55
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (200, 512)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 56
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 544, ), Input shape (196, 544, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 544)
, and Output shape (196, 128, ), and the ID is 57
	Allocating LowPrecision Activations Tensors with Shape of (200, 544)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 58
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (196, 576, ), and Output shape (196, 128, ), and the ID is 59
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
	Allocating LowPrecision Activations Tensors with Shape of (200, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 60
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 608, ), Input shape (196, 608, ), and Output shape (196, 128, ), and the ID is 61
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 62
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 640, ), Input shape (196, 640, ), and Output shape (196, 128, ), and the ID is 63
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 640)
	Allocating LowPrecision Activations Tensors with Shape of (200, 640)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 64
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 672, ), Input shape (196, 672, ), and Output shape (196, 128, ), and the ID is 65	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 672)

	Allocating LowPrecision Activations Tensors with Shape of (200, 672)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 66
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 704, ), Input shape (196, 704, ), and Output shape (196, 128, ), and the ID is 67
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 704)
	Allocating LowPrecision Activations Tensors with Shape of (200, 704)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 68
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 736, ), Input shape (196, 736, ), and Output shape (196, 128, ), and the ID is 69	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 736)

	Allocating LowPrecision Activations Tensors with Shape of (200, 736)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 70
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 768, ), Input shape (196, 768, ), and Output shape (196, 128, ), and the ID is 71
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 768)
	Allocating LowPrecision Activations Tensors with Shape of (200, 768)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 72
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 800, ), Input shape (196, 800, ), and Output shape (196, 128, ), and the ID is 73	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 800)

	Allocating LowPrecision Activations Tensors with Shape of (200, 800)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 74
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 832, ), Input shape (196, 832, ), and Output shape (196, 128, ), and the ID is 75	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 832)

	Allocating LowPrecision Activations Tensors with Shape of (200, 832)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 76
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 864, ), Input shape (196, 864, ), and Output shape (196, 128, ), and the ID is 77
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 864)
	Allocating LowPrecision Activations Tensors with Shape of (200, 864)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 78
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (196, 896, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 896)
79
	Allocating LowPrecision Activations Tensors with Shape of (200, 896)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 80
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (196, 928, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 928)
81
	Allocating LowPrecision Activations Tensors with Shape of (200, 928)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 82
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (196, 960, ), and Output shape (196, 128, ), and the ID is 83
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 960)
	Allocating LowPrecision Activations Tensors with Shape of (200, 960)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 84
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (196, 992, ), and Output shape (196, 128, ), and the ID is 85
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 992)
	Allocating LowPrecision Activations Tensors with Shape of (200, 992)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 86
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (196, 1024, ), and Output shape (196, 128, ), and the ID is 87
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 88
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (196, 1056, ), and Output shape (196, 128, ), and the ID is 89
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1056)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1056)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 90
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (196, 1088, ), and Output shape (196, 128, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1088)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1088)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 92
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (196, 1120, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1120)
93
	Allocating LowPrecision Activations Tensors with Shape of (200, 1120)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (196, 1152, ), and Output shape (196, 128, ), and the ID is 95
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 96
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (196, 1184, ), and Output shape (196, 128, ), and the ID is 97
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1184)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1184)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 98
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (196, 1216, ), and Output shape (196, 128, ), and the ID is 99
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1216)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1216)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 100
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (196, 1248, ), and Output shape (196, 128, ), and the ID is 101
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1248)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1248)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 102
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (196, 1280, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1280)
, and the ID is 103
	Allocating LowPrecision Activations Tensors with Shape of (200, 1280)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 104
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (196, 1312, ), and Output shape (196, 128, ), and the ID is 105
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1312)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1312)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 106
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (196, 1344, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1344)
107
	Allocating LowPrecision Activations Tensors with Shape of (200, 1344)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 108
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (196, 1376, ), and Output shape (196, 128, ), and the ID is 109
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1376)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1376)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 110
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (196, 1408, ), and Output shape (196, 128, ), and the ID is 111	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1408)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1408)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 112
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (196, 1440, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1440)
, and the ID is 113
	Allocating LowPrecision Activations Tensors with Shape of (200, 1440)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 114
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (196, 1472, ), and Output shape (196, 128, ), and the ID is 115
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1472)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1472)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (196, 1504, ), and Output shape (196, 128, ), and the ID is 117	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1504)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1504)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 118
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (196, 1536, ), and Output shape (196, 128, ), and the ID is 119
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1536)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1536)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 120
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (196, 1568, ), and Output shape (196, 128, ), and the ID is 121	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1568)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1568)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 122
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (196, 1600, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1600)
(196, 128, ), and the ID is 123
	Allocating LowPrecision Activations Tensors with Shape of (200, 1600)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 124
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (196, 1632, ), and Output shape (196, 128, ), and the ID is 125	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1632)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1632)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 126
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (196, 1664, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1664)
, and the ID is 127
	Allocating LowPrecision Activations Tensors with Shape of (200, 1664)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 128
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (196, 1696, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1696)
, and the ID is 129
	Allocating LowPrecision Activations Tensors with Shape of (200, 1696)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 130
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (196, 1728, ), and Output shape (196, 128, ), and the ID is 131	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1728)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1728)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 132
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (196, 1760, ), and Output shape (196, 128, ), and the ID is 133	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1760)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1760)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 134
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (896, 1792, ), Input shape (196, 1792, ), and Output shape (196, 896, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 135
	Allocating LowPrecision Weight Tensors with Shape of (896, 1792)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1792)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (49, 896, ), and Output shape (49, 128, ), and the ID is 136
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 896)
	Allocating LowPrecision Activations Tensors with Shape of (56, 896)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 137
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (49, 928, ), and Output shape (49, 128, ), and the ID is 138	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 928)

	Allocating LowPrecision Activations Tensors with Shape of (56, 928)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (49, 960, ), and Output shape (49, 128, ), and the ID is 140
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 960)
	Allocating LowPrecision Activations Tensors with Shape of (56, 960)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 141
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (49, 992, ), and Output shape (49, 128, ), and the ID is 142
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 992)
	Allocating LowPrecision Activations Tensors with Shape of (56, 992)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 143
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (49, 1024, ), and Output shape (49, 128, ), and the ID is 144
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1024)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 145
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (49, 1056, ), and Output shape (49, 128, ), and the ID is 146	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1056)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1056)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 147
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (49, 1088, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1088)
148
	Allocating LowPrecision Activations Tensors with Shape of (56, 1088)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (49, 1120, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1120)
150
	Allocating LowPrecision Activations Tensors with Shape of (56, 1120)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 151
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (49, 1152, ), and Output shape (49, 128, ), and the ID is 152
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 153
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (49, 1184, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1184)
, and the ID is 154
	Allocating LowPrecision Activations Tensors with Shape of (56, 1184)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 155
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (49, 1216, ), and Output shape (49, 128, ), and the ID is 156
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1216)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1216)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 157
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (49, 1248, ), and Output shape (49, 128, ), and the ID is 158
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1248)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1248)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 159
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (49, 1280, ), and Output shape (49, 128, ), and the ID is 160
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1280)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1280)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 161
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (49, 1312, ), and Output shape (49, 128, ), and the ID is 162	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1312)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1312)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 163
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (49, 1344, ), and Output shape (49, 128, ), and the ID is 164
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1344)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1344)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 165
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (49, 1376, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1376)
166
	Allocating LowPrecision Activations Tensors with Shape of (56, 1376)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 167
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (49, 1408, ), and Output shape (49, 128, ), and the ID is 168
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1408)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1408)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 169
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (49, 1440, ), and Output shape (49, 128, ), and the ID is 170
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1440)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1440)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 171
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (49, 1472, ), and Output shape (49, 128, ), and the ID is 172
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1472)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1472)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 173
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (49, 1504, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1504)
174
	Allocating LowPrecision Activations Tensors with Shape of (56, 1504)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 175
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (49, 1536, ), and Output shape (49, 128, ), and the ID is 176	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1536)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1536)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 177
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (49, 1568, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1568)
178
	Allocating LowPrecision Activations Tensors with Shape of (56, 1568)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 179
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (49, 1600, ), and Output shape (49, 128, ), and the ID is 180	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1600)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1600)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 181
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (49, 1632, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1632)
, and the ID is 182
	Allocating LowPrecision Activations Tensors with Shape of (56, 1632)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
(49, 128, ), and Output shape (49, 32, ), and the ID is 183
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (49, 1664, ), and Output shape (49, 128, ), and the ID is 184	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1664)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1664)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 185
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (49, 1696, ), and Output shape (49, 128, ), and the ID is 186
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1696)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1696)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 187
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (49, 1728, ), and Output shape (49, 128, ), and the ID is 188	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1728)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1728)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 189
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (49, 1760, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1760)
, and the ID is 190
	Allocating LowPrecision Activations Tensors with Shape of (56, 1760)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 191
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1792, ), Input shape (49, 1792, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1792)
192
	Allocating LowPrecision Activations Tensors with Shape of (56, 1792)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 193
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1824, ), Input shape (49, 1824, ), and Output shape (49, 128, ), and the ID is 194	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1824)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1824)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 195
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1856, ), Input shape (49, 1856, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1856)
, and the ID is 196
	Allocating LowPrecision Activations Tensors with Shape of (56, 1856)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 197
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1888, ), Input shape (49, 1888, ), and Output shape (49, 128, ), and the ID is 198	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1888)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1888)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 199
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Low-Precision for shape (1000, 1920, ) and Input shape (1, 1920, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 1920)
	Transformed Activation Shape From: (1, 1920) To: (8, 1920)
The input model file size (MB): 20.5199
Initialized session in 300.143ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=13640797 curr=13634453 min=13617147 max=13657908 avg=1.36329e+07 std=11583

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=12 first=13622409 curr=13606638 min=13595700 max=13669469 avg=1.36237e+07 std=19838

Inference timings in us: Init: 300143, First inference: 13640797, Warmup (avg): 1.36329e+07, Inference (avg): 1.36237e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=42.8672 overall=53.3984
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  277.693	  277.693	100.000%	100.000%	 29388.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  277.693	  277.693	100.000%	100.000%	 29388.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   277.693	   100.000%	   100.000%	 29388.000	        1

Timings (microseconds): count=1 curr=277693
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.028	    3.699	    3.720	  0.027%	  0.027%	     0.000	        1	[densenet201/zero_padding2d/Pad]:0
	                 CONV_2D	            3.756	  328.475	  329.528	  2.420%	  2.447%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                     PAD	          333.296	   18.176	   18.209	  0.134%	  2.581%	     0.000	        1	[densenet201/zero_padding2d_1/Pad]:2
	             MAX_POOL_2D	          351.515	    5.300	    5.264	  0.039%	  2.619%	     0.000	        1	[densenet201/pool1/MaxPool]:3
	                     MUL	          356.789	   14.567	   14.675	  0.108%	  2.727%	     0.000	        1	[densenet201/conv2_block1_0_bn/FusedBatchNormV31]:4
	                     ADD	          371.474	   19.220	   19.291	  0.142%	  2.869%	     0.000	        1	[densenet201/conv2_block1_0_relu/Relu;densenet201/conv2_block1_0_bn/FusedBatchNormV3]:5
	                 CONV_2D	          390.774	   70.021	   70.758	  0.520%	  3.388%	     0.000	        1	[densenet201/conv2_block1_1_relu/Relu;densenet201/conv2_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block1_1_conv/Conv2D]:6
	                 CONV_2D	          461.545	  315.473	  318.480	  2.339%	  5.727%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	           CONCATENATION	          780.038	    0.371	    0.395	  0.003%	  5.730%	     0.000	        1	[densenet201/conv2_block1_concat/concat]:8
	                     MUL	          780.441	   21.523	   21.723	  0.160%	  5.889%	     0.000	        1	[densenet201/conv2_block2_0_bn/FusedBatchNormV31]:9
	                     ADD	          802.173	   28.514	   28.678	  0.211%	  6.100%	     0.000	        1	[densenet201/conv2_block2_0_relu/Relu;densenet201/conv2_block2_0_bn/FusedBatchNormV3]:10
	                 CONV_2D	          830.861	   96.656	   97.684	  0.717%	  6.817%	     0.000	        1	[densenet201/conv2_block2_1_relu/Relu;densenet201/conv2_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block2_1_conv/Conv2D]:11
	                 CONV_2D	          928.559	  318.623	  319.509	  2.346%	  9.163%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	           CONCATENATION	         1248.079	    0.465	    0.417	  0.003%	  9.166%	     0.000	        1	[densenet201/conv2_block2_concat/concat]:13
	                     MUL	         1248.505	   28.989	   28.727	  0.211%	  9.377%	     0.000	        1	[densenet201/conv2_block3_0_bn/FusedBatchNormV3]:14
	                     ADD	         1277.243	   38.546	   38.130	  0.280%	  9.657%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                 CONV_2D	         1315.383	  129.145	  124.599	  0.915%	 10.572%	     0.000	        1	[densenet201/conv2_block3_1_relu/Relu;densenet201/conv2_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block3_1_conv/Conv2D]:16
	                 CONV_2D	         1439.994	  321.416	  317.869	  2.334%	 12.906%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	           CONCATENATION	         1757.875	    0.422	    0.495	  0.004%	 12.910%	     0.000	        1	[densenet201/conv2_block3_concat/concat]:18
	                     MUL	         1758.382	   35.693	   35.852	  0.263%	 13.173%	     0.000	        1	[densenet201/conv2_block4_0_bn/FusedBatchNormV3]:19
	                     ADD	         1794.249	   47.118	   47.447	  0.348%	 13.521%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                 CONV_2D	         1841.707	  150.498	  151.610	  1.113%	 14.635%	     0.000	        1	[densenet201/conv2_block4_1_relu/Relu;densenet201/conv2_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block4_1_conv/Conv2D]:21
	                 CONV_2D	         1993.329	  315.258	  317.204	  2.329%	 16.964%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	           CONCATENATION	         2310.544	    0.548	    0.575	  0.004%	 16.968%	     0.000	        1	[densenet201/conv2_block4_concat/concat]:23
	                     MUL	         2311.129	   42.629	   42.808	  0.314%	 17.282%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	         2353.948	   56.675	   56.897	  0.418%	 17.700%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                 CONV_2D	         2410.855	  177.337	  178.337	  1.309%	 19.009%	     0.000	        1	[densenet201/conv2_block5_1_relu/Relu;densenet201/conv2_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block5_1_conv/Conv2D]:26
	                 CONV_2D	         2589.203	  316.215	  317.281	  2.330%	 21.339%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	           CONCATENATION	         2906.496	    0.613	    0.705	  0.005%	 21.344%	     0.000	        1	[densenet201/conv2_block5_concat/concat]:28
	                     MUL	         2907.210	   49.627	   49.876	  0.366%	 21.711%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	         2957.098	   66.045	   66.227	  0.486%	 22.197%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                 CONV_2D	         3023.336	  205.193	  206.619	  1.517%	 23.714%	     0.000	        1	[densenet201/conv2_block6_1_relu/Relu;densenet201/conv2_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block6_1_conv/Conv2D]:31
	                 CONV_2D	         3229.968	  315.617	  317.523	  2.331%	 26.046%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	           CONCATENATION	         3547.502	    0.855	    0.791	  0.006%	 26.051%	     0.000	        1	[densenet201/conv2_block6_concat/concat]:33
	                     MUL	         3548.303	   57.185	   56.845	  0.417%	 26.469%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     ADD	         3605.162	   76.204	   75.463	  0.554%	 27.023%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                 CONV_2D	         3680.636	  239.055	  234.027	  1.718%	 28.741%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	         AVERAGE_POOL_2D	         3914.675	   10.492	   10.371	  0.076%	 28.817%	     0.000	        1	[densenet201/pool2_pool/AvgPool]:37
	                     MUL	         3925.056	    7.317	    7.193	  0.053%	 28.870%	     0.000	        1	[densenet201/conv3_block1_0_bn/FusedBatchNormV3]:38
	                     ADD	         3932.258	    9.600	    9.526	  0.070%	 28.940%	     0.000	        1	[densenet201/conv3_block1_0_relu/Relu;densenet201/conv3_block1_0_bn/FusedBatchNormV3]:39
	                 CONV_2D	         3941.794	   31.333	   30.759	  0.226%	 29.166%	     0.000	        1	[densenet201/conv3_block1_1_relu/Relu;densenet201/conv3_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block1_1_conv/Conv2D]:40
	                 CONV_2D	         3972.563	   79.823	   79.165	  0.581%	 29.747%	     0.000	        1	[densenet201/conv3_block1_2_conv/Conv2D1]:41
	           CONCATENATION	         4051.740	    0.120	    0.135	  0.001%	 29.748%	     0.000	        1	[densenet201/conv3_block1_concat/concat]:42
	                     MUL	         4051.881	    9.000	    8.977	  0.066%	 29.814%	     0.000	        1	[densenet201/conv3_block2_0_bn/FusedBatchNormV31]:43
	                     ADD	         4060.868	   11.928	   11.842	  0.087%	 29.901%	     0.000	        1	[densenet201/conv3_block2_0_relu/Relu;densenet201/conv3_block2_0_bn/FusedBatchNormV3]:44
	                 CONV_2D	         4072.720	   37.875	   37.458	  0.275%	 30.176%	     0.000	        1	[densenet201/conv3_block2_1_relu/Relu;densenet201/conv3_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block2_1_conv/Conv2D]:45
	                 CONV_2D	         4110.188	   80.035	   79.576	  0.584%	 30.761%	     0.000	        1	[densenet201/conv3_block2_2_conv/Conv2D1]:46
	           CONCATENATION	         4189.776	    0.159	    0.154	  0.001%	 30.762%	     0.000	        1	[densenet201/conv3_block2_concat/concat]:47
	                     MUL	         4189.937	   10.775	   10.702	  0.079%	 30.840%	     0.000	        1	[densenet201/conv3_block3_0_bn/FusedBatchNormV31]:48
	                     ADD	         4200.648	   14.285	   14.230	  0.104%	 30.945%	     0.000	        1	[densenet201/conv3_block3_0_relu/Relu;densenet201/conv3_block3_0_bn/FusedBatchNormV3]:49
	                 CONV_2D	         4214.888	   44.827	   44.293	  0.325%	 31.270%	     0.000	        1	[densenet201/conv3_block3_1_relu/Relu;densenet201/conv3_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block3_1_conv/Conv2D]:50
	                 CONV_2D	         4259.193	   80.115	   79.677	  0.585%	 31.855%	     0.000	        1	[densenet201/conv3_block3_2_conv/Conv2D1]:51
	           CONCATENATION	         4338.881	    0.171	    0.173	  0.001%	 31.856%	     0.000	        1	[densenet201/conv3_block3_concat/concat]:52
	                     MUL	         4339.062	   12.629	   12.482	  0.092%	 31.948%	     0.000	        1	[densenet201/conv3_block4_0_bn/FusedBatchNormV31]:53
	                     ADD	         4351.553	   16.644	   16.591	  0.122%	 32.070%	     0.000	        1	[densenet201/conv3_block4_0_relu/Relu;densenet201/conv3_block4_0_bn/FusedBatchNormV3]:54
	                 CONV_2D	         4368.155	   51.826	   51.311	  0.377%	 32.447%	     0.000	        1	[densenet201/conv3_block4_1_relu/Relu;densenet201/conv3_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block4_1_conv/Conv2D]:55
	                 CONV_2D	         4419.477	   80.485	   79.445	  0.583%	 33.030%	     0.000	        1	[densenet201/conv3_block4_2_conv/Conv2D1]:56
	           CONCATENATION	         4498.933	    0.170	    0.200	  0.001%	 33.031%	     0.000	        1	[densenet201/conv3_block4_concat/concat]:57
	                     MUL	         4499.141	   14.265	   14.245	  0.105%	 33.136%	     0.000	        1	[densenet201/conv3_block5_0_bn/FusedBatchNormV3]:58
	                     ADD	         4513.395	   19.022	   18.917	  0.139%	 33.275%	     0.000	        1	[densenet201/conv3_block5_0_relu/Relu;densenet201/conv3_block5_0_bn/FusedBatchNormV3]:59
	                 CONV_2D	         4532.322	   58.653	   57.843	  0.425%	 33.700%	     0.000	        1	[densenet201/conv3_block5_1_relu/Relu;densenet201/conv3_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block5_1_conv/Conv2D]:60
	                 CONV_2D	         4590.177	   79.617	   79.208	  0.582%	 34.281%	     0.000	        1	[densenet201/conv3_block5_2_conv/Conv2D1]:61
	           CONCATENATION	         4669.397	    0.166	    0.207	  0.002%	 34.283%	     0.000	        1	[densenet201/conv3_block5_concat/concat]:62
	                     MUL	         4669.611	   15.922	   16.006	  0.118%	 34.400%	     0.000	        1	[densenet201/conv3_block6_0_bn/FusedBatchNormV3]:63
	                     ADD	         4685.627	   20.979	   21.187	  0.156%	 34.556%	     0.000	        1	[densenet201/conv3_block6_0_relu/Relu;densenet201/conv3_block6_0_bn/FusedBatchNormV3]:64
	                 CONV_2D	         4706.824	   64.088	   64.554	  0.474%	 35.030%	     0.000	        1	[densenet201/conv3_block6_1_relu/Relu;densenet201/conv3_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block6_1_conv/Conv2D]:65
	                 CONV_2D	         4771.390	   77.771	   79.271	  0.582%	 35.612%	     0.000	        1	[densenet201/conv3_block6_2_conv/Conv2D1]:66
	           CONCATENATION	         4850.672	    0.240	    0.234	  0.002%	 35.614%	     0.000	        1	[densenet201/conv3_block6_concat/concat]:67
	                     MUL	         4850.913	   17.657	   17.736	  0.130%	 35.744%	     0.000	        1	[densenet201/conv3_block7_0_bn/FusedBatchNormV3]:68
	                     ADD	         4868.659	   23.343	   23.482	  0.172%	 35.916%	     0.000	        1	[densenet201/conv3_block7_0_relu/Relu;densenet201/conv3_block7_0_bn/FusedBatchNormV3]:69
	                 CONV_2D	         4892.150	   70.119	   71.014	  0.521%	 36.438%	     0.000	        1	[densenet201/conv3_block7_1_relu/Relu;densenet201/conv3_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block7_1_conv/Conv2D]:70
	                 CONV_2D	         4963.177	   78.826	   79.365	  0.583%	 37.020%	     0.000	        1	[densenet201/conv3_block7_2_conv/Conv2D1]:71
	           CONCATENATION	         5042.553	    0.216	    0.234	  0.002%	 37.022%	     0.000	        1	[densenet201/conv3_block7_concat/concat]:72
	                     MUL	         5042.795	   19.454	   19.460	  0.143%	 37.165%	     0.000	        1	[densenet201/conv3_block8_0_bn/FusedBatchNormV3]:73
	                     ADD	         5062.266	   25.627	   25.756	  0.189%	 37.354%	     0.000	        1	[densenet201/conv3_block8_0_relu/Relu;densenet201/conv3_block8_0_bn/FusedBatchNormV3]:74
	                 CONV_2D	         5088.032	   76.953	   77.809	  0.571%	 37.925%	     0.000	        1	[densenet201/conv3_block8_1_relu/Relu;densenet201/conv3_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block8_1_conv/Conv2D]:75
	                 CONV_2D	         5165.852	   77.684	   78.774	  0.578%	 38.504%	     0.000	        1	[densenet201/conv3_block8_2_conv/Conv2D1]:76
	           CONCATENATION	         5244.638	    0.294	    0.288	  0.002%	 38.506%	     0.000	        1	[densenet201/conv3_block8_concat/concat]:77
	                     MUL	         5244.934	   21.205	   21.218	  0.156%	 38.662%	     0.000	        1	[densenet201/conv3_block9_0_bn/FusedBatchNormV3]:78
	                     ADD	         5266.162	   28.017	   28.152	  0.207%	 38.869%	     0.000	        1	[densenet201/conv3_block9_0_relu/Relu;densenet201/conv3_block9_0_bn/FusedBatchNormV3]:79
	                 CONV_2D	         5294.326	   83.652	   84.866	  0.623%	 39.492%	     0.000	        1	[densenet201/conv3_block9_1_relu/Relu;densenet201/conv3_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block9_1_conv/Conv2D]:80
	                 CONV_2D	         5379.203	   78.132	   79.034	  0.580%	 40.072%	     0.000	        1	[densenet201/conv3_block9_2_conv/Conv2D1]:81
	           CONCATENATION	         5458.248	    0.290	    0.299	  0.002%	 40.074%	     0.000	        1	[densenet201/conv3_block9_concat/concat]:82
	                     MUL	         5458.557	   22.917	   23.011	  0.169%	 40.243%	     0.000	        1	[densenet201/conv3_block10_0_bn/FusedBatchNormV3]:83
	                     ADD	         5481.579	   30.275	   30.466	  0.224%	 40.467%	     0.000	        1	[densenet201/conv3_block10_0_relu/Relu;densenet201/conv3_block10_0_bn/FusedBatchNormV3]:84
	                 CONV_2D	         5512.055	   90.767	   91.634	  0.673%	 41.140%	     0.000	        1	[densenet201/conv3_block10_1_relu/Relu;densenet201/conv3_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block10_1_conv/Conv2D]:85
	                 CONV_2D	         5603.701	   79.531	   79.597	  0.584%	 41.724%	     0.000	        1	[densenet201/conv3_block10_2_conv/Conv2D1]:86
	           CONCATENATION	         5683.312	    0.291	    0.311	  0.002%	 41.726%	     0.000	        1	[densenet201/conv3_block10_concat/concat]:87
	                     MUL	         5683.632	   24.689	   24.805	  0.182%	 41.909%	     0.000	        1	[densenet201/conv3_block11_0_bn/FusedBatchNormV3]:88
	                     ADD	         5708.448	   32.742	   32.812	  0.241%	 42.150%	     0.000	        1	[densenet201/conv3_block11_0_relu/Relu;densenet201/conv3_block11_0_bn/FusedBatchNormV3]:89
	                 CONV_2D	         5741.271	   97.627	   98.795	  0.725%	 42.875%	     0.000	        1	[densenet201/conv3_block11_1_relu/Relu;densenet201/conv3_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block11_1_conv/Conv2D]:90
	                 CONV_2D	         5840.080	   78.939	   79.234	  0.582%	 43.457%	     0.000	        1	[densenet201/conv3_block11_2_conv/Conv2D1]:91
	           CONCATENATION	         5919.332	    0.292	    0.341	  0.003%	 43.459%	     0.000	        1	[densenet201/conv3_block11_concat/concat]:92
	                     MUL	         5919.682	   26.575	   26.561	  0.195%	 43.654%	     0.000	        1	[densenet201/conv3_block12_0_bn/FusedBatchNormV3]:93
	                     ADD	         5946.253	   35.316	   35.249	  0.259%	 43.913%	     0.000	        1	[densenet201/conv3_block12_0_relu/Relu;densenet201/conv3_block12_0_bn/FusedBatchNormV3]:94
	                 CONV_2D	         5981.513	  107.129	  105.772	  0.777%	 44.690%	     0.000	        1	[densenet201/conv3_block12_1_relu/Relu;densenet201/conv3_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block12_1_conv/Conv2D]:95
	                 CONV_2D	         6087.296	   82.773	   79.607	  0.585%	 45.274%	     0.000	        1	[densenet201/conv3_block12_2_conv/Conv2D1]:96
	           CONCATENATION	         6166.914	    0.461	    0.362	  0.003%	 45.277%	     0.000	        1	[densenet201/conv3_block12_concat/concat]:97
	                     MUL	         6167.285	   28.602	   28.336	  0.208%	 45.485%	     0.000	        1	[densenet201/pool3_bn/FusedBatchNormV3]:98
	                     ADD	         6195.631	   37.990	   37.577	  0.276%	 45.761%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99
	                 CONV_2D	         6233.219	  220.497	  216.607	  1.590%	 47.351%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100
	         AVERAGE_POOL_2D	         6449.837	    5.241	    5.146	  0.038%	 47.389%	     0.000	        1	[densenet201/pool3_pool/AvgPool]:101
	                     MUL	         6454.991	    3.605	    3.572	  0.026%	 47.415%	     0.000	        1	[densenet201/conv4_block1_0_bn/FusedBatchNormV31]:102
	                     ADD	         6458.572	    4.764	    4.729	  0.035%	 47.450%	     0.000	        1	[densenet201/conv4_block1_0_relu/Relu;densenet201/conv4_block1_0_bn/FusedBatchNormV3]:103
	                 CONV_2D	         6463.309	   14.691	   14.559	  0.107%	 47.557%	     0.000	        1	[densenet201/conv4_block1_1_relu/Relu;densenet201/conv4_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block1_1_conv/Conv2D]:104
	                 CONV_2D	         6477.877	   19.464	   19.117	  0.140%	 47.697%	     0.000	        1	[densenet201/conv4_block1_2_conv/Conv2D1]:105
	           CONCATENATION	         6497.003	    0.067	    0.083	  0.001%	 47.698%	     0.000	        1	[densenet201/conv4_block1_concat/concat]:106
	                     MUL	         6497.093	    4.041	    4.001	  0.029%	 47.727%	     0.000	        1	[densenet201/conv4_block2_0_bn/FusedBatchNormV31]:107
	                     ADD	         6501.101	    5.400	    5.312	  0.039%	 47.766%	     0.000	        1	[densenet201/conv4_block2_0_relu/Relu;densenet201/conv4_block2_0_bn/FusedBatchNormV3]:108
	                 CONV_2D	         6506.422	   16.380	   16.224	  0.119%	 47.886%	     0.000	        1	[densenet201/conv4_block2_1_relu/Relu;densenet201/conv4_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block2_1_conv/Conv2D]:109
	                 CONV_2D	         6522.655	   19.544	   19.050	  0.140%	 48.025%	     0.000	        1	[densenet201/conv4_block2_2_conv/Conv2D1]:110
	           CONCATENATION	         6541.715	    0.076	    0.091	  0.001%	 48.026%	     0.000	        1	[densenet201/conv4_block2_concat/concat]:111
	                     MUL	         6541.813	    4.481	    4.436	  0.033%	 48.059%	     0.000	        1	[densenet201/conv4_block3_0_bn/FusedBatchNormV31]:112
	                     ADD	         6546.257	    5.932	    5.873	  0.043%	 48.102%	     0.000	        1	[densenet201/conv4_block3_0_relu/Relu;densenet201/conv4_block3_0_bn/FusedBatchNormV3]:113
	                 CONV_2D	         6552.139	   18.089	   17.873	  0.131%	 48.233%	     0.000	        1	[densenet201/conv4_block3_1_relu/Relu;densenet201/conv4_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block3_1_conv/Conv2D]:114
	                 CONV_2D	         6570.020	   19.473	   19.059	  0.140%	 48.373%	     0.000	        1	[densenet201/conv4_block3_2_conv/Conv2D1]:115
	           CONCATENATION	         6589.090	    0.077	    0.088	  0.001%	 48.374%	     0.000	        1	[densenet201/conv4_block3_concat/concat]:116
	                     MUL	         6589.185	    4.926	    4.885	  0.036%	 48.409%	     0.000	        1	[densenet201/conv4_block4_0_bn/FusedBatchNormV31]:117
	                     ADD	         6594.078	    6.535	    6.459	  0.047%	 48.457%	     0.000	        1	[densenet201/conv4_block4_0_relu/Relu;densenet201/conv4_block4_0_bn/FusedBatchNormV3]:118
	                 CONV_2D	         6600.546	   19.871	   19.649	  0.144%	 48.601%	     0.000	        1	[densenet201/conv4_block4_1_relu/Relu;densenet201/conv4_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block4_1_conv/Conv2D]:119
	                 CONV_2D	         6620.207	   19.429	   19.065	  0.140%	 48.741%	     0.000	        1	[densenet201/conv4_block4_2_conv/Conv2D1]:120
	           CONCATENATION	         6639.283	    0.098	    0.095	  0.001%	 48.742%	     0.000	        1	[densenet201/conv4_block4_concat/concat]:121
	                     MUL	         6639.384	    5.337	    5.338	  0.039%	 48.781%	     0.000	        1	[densenet201/conv4_block5_0_bn/FusedBatchNormV31]:122
	                     ADD	         6644.730	    7.129	    7.051	  0.052%	 48.833%	     0.000	        1	[densenet201/conv4_block5_0_relu/Relu;densenet201/conv4_block5_0_bn/FusedBatchNormV3]:123
	                 CONV_2D	         6651.790	   21.492	   21.332	  0.157%	 48.989%	     0.000	        1	[densenet201/conv4_block5_1_relu/Relu;densenet201/conv4_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block5_1_conv/Conv2D]:124
	                 CONV_2D	         6673.132	   19.424	   19.074	  0.140%	 49.130%	     0.000	        1	[densenet201/conv4_block5_2_conv/Conv2D1]:125
	           CONCATENATION	         6692.216	    0.100	    0.100	  0.001%	 49.130%	     0.000	        1	[densenet201/conv4_block5_concat/concat]:126
	                     MUL	         6692.323	    5.786	    5.812	  0.043%	 49.173%	     0.000	        1	[densenet201/conv4_block6_0_bn/FusedBatchNormV31]:127
	                     ADD	         6698.143	    7.716	    7.643	  0.056%	 49.229%	     0.000	        1	[densenet201/conv4_block6_0_relu/Relu;densenet201/conv4_block6_0_bn/FusedBatchNormV3]:128
	                 CONV_2D	         6705.796	   23.288	   23.091	  0.170%	 49.399%	     0.000	        1	[densenet201/conv4_block6_1_relu/Relu;densenet201/conv4_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block6_1_conv/Conv2D]:129
	                 CONV_2D	         6728.896	   19.677	   19.145	  0.141%	 49.539%	     0.000	        1	[densenet201/conv4_block6_2_conv/Conv2D1]:130
	           CONCATENATION	         6748.052	    0.100	    0.107	  0.001%	 49.540%	     0.000	        1	[densenet201/conv4_block6_concat/concat]:131
	                     MUL	         6748.166	    6.268	    6.237	  0.046%	 49.586%	     0.000	        1	[densenet201/conv4_block7_0_bn/FusedBatchNormV31]:132
	                     ADD	         6754.411	    8.333	    8.215	  0.060%	 49.646%	     0.000	        1	[densenet201/conv4_block7_0_relu/Relu;densenet201/conv4_block7_0_bn/FusedBatchNormV3]:133
	                 CONV_2D	         6762.635	   25.053	   24.811	  0.182%	 49.828%	     0.000	        1	[densenet201/conv4_block7_1_relu/Relu;densenet201/conv4_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block7_1_conv/Conv2D]:134
	                 CONV_2D	         6787.458	   19.305	   19.325	  0.142%	 49.970%	     0.000	        1	[densenet201/conv4_block7_2_conv/Conv2D1]:135
	           CONCATENATION	         6806.794	    0.112	    0.106	  0.001%	 49.971%	     0.000	        1	[densenet201/conv4_block7_concat/concat]:136
	                     MUL	         6806.906	    6.620	    6.661	  0.049%	 50.020%	     0.000	        1	[densenet201/conv4_block8_0_bn/FusedBatchNormV31]:137
	                     ADD	         6813.576	    8.749	    8.816	  0.065%	 50.085%	     0.000	        1	[densenet201/conv4_block8_0_relu/Relu;densenet201/conv4_block8_0_bn/FusedBatchNormV3]:138
	                 CONV_2D	         6822.400	   26.134	   26.476	  0.194%	 50.279%	     0.000	        1	[densenet201/conv4_block8_1_relu/Relu;densenet201/conv4_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block8_1_conv/Conv2D]:139
	                 CONV_2D	         6848.886	   18.775	   19.154	  0.141%	 50.420%	     0.000	        1	[densenet201/conv4_block8_2_conv/Conv2D1]:140
	           CONCATENATION	         6868.054	    0.114	    0.123	  0.001%	 50.421%	     0.000	        1	[densenet201/conv4_block8_concat/concat]:141
	                     MUL	         6868.184	    7.047	    7.093	  0.052%	 50.473%	     0.000	        1	[densenet201/conv4_block9_0_bn/FusedBatchNormV31]:142
	                     ADD	         6875.286	    9.287	    9.388	  0.069%	 50.542%	     0.000	        1	[densenet201/conv4_block9_0_relu/Relu;densenet201/conv4_block9_0_bn/FusedBatchNormV3]:143
	                 CONV_2D	         6884.682	   27.909	   28.203	  0.207%	 50.749%	     0.000	        1	[densenet201/conv4_block9_1_relu/Relu;densenet201/conv4_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block9_1_conv/Conv2D]:144
	                 CONV_2D	         6912.895	   18.750	   19.147	  0.141%	 50.889%	     0.000	        1	[densenet201/conv4_block9_2_conv/Conv2D1]:145
	           CONCATENATION	         6932.053	    0.173	    0.129	  0.001%	 50.890%	     0.000	        1	[densenet201/conv4_block9_concat/concat]:146
	                     MUL	         6932.189	    7.497	    7.528	  0.055%	 50.945%	     0.000	        1	[densenet201/conv4_block10_0_bn/FusedBatchNormV31]:147
	                     ADD	         6939.730	    9.929	   10.007	  0.073%	 51.019%	     0.000	        1	[densenet201/conv4_block10_0_relu/Relu;densenet201/conv4_block10_0_bn/FusedBatchNormV3]:148
	                 CONV_2D	         6949.745	   30.145	   30.630	  0.225%	 51.244%	     0.000	        1	[densenet201/conv4_block10_1_relu/Relu;densenet201/conv4_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block10_1_conv/Conv2D]:149
	                 CONV_2D	         6980.387	   18.804	   19.164	  0.141%	 51.385%	     0.000	        1	[densenet201/conv4_block10_2_conv/Conv2D1]:150
	           CONCATENATION	         6999.561	    0.110	    0.142	  0.001%	 51.386%	     0.000	        1	[densenet201/conv4_block10_concat/concat]:151
	                     MUL	         6999.710	    7.921	    7.954	  0.058%	 51.444%	     0.000	        1	[densenet201/conv4_block11_0_bn/FusedBatchNormV31]:152
	                     ADD	         7007.673	   10.529	   10.608	  0.078%	 51.522%	     0.000	        1	[densenet201/conv4_block11_0_relu/Relu;densenet201/conv4_block11_0_bn/FusedBatchNormV3]:153
	                 CONV_2D	         7018.289	   31.800	   32.212	  0.237%	 51.758%	     0.000	        1	[densenet201/conv4_block11_1_relu/Relu;densenet201/conv4_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block11_1_conv/Conv2D]:154
	                 CONV_2D	         7050.510	   18.730	   19.145	  0.141%	 51.899%	     0.000	        1	[densenet201/conv4_block11_2_conv/Conv2D1]:155
	           CONCATENATION	         7069.667	    0.157	    0.220	  0.002%	 51.901%	     0.000	        1	[densenet201/conv4_block11_concat/concat]:156
	                     MUL	         7069.894	    8.351	    8.419	  0.062%	 51.962%	     0.000	        1	[densenet201/conv4_block12_0_bn/FusedBatchNormV31]:157
	                     ADD	         7078.322	   11.101	   11.244	  0.083%	 52.045%	     0.000	        1	[densenet201/conv4_block12_0_relu/Relu;densenet201/conv4_block12_0_bn/FusedBatchNormV3]:158
	                 CONV_2D	         7089.575	   33.517	   34.175	  0.251%	 52.296%	     0.000	        1	[densenet201/conv4_block12_1_relu/Relu;densenet201/conv4_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block12_1_conv/Conv2D]:159
	                 CONV_2D	         7123.760	   18.813	   19.164	  0.141%	 52.437%	     0.000	        1	[densenet201/conv4_block12_2_conv/Conv2D1]:160
	           CONCATENATION	         7142.934	    0.213	    0.151	  0.001%	 52.438%	     0.000	        1	[densenet201/conv4_block12_concat/concat]:161
	                     MUL	         7143.091	    8.770	    8.822	  0.065%	 52.503%	     0.000	        1	[densenet201/conv4_block13_0_bn/FusedBatchNormV31]:162
	                     ADD	         7151.922	   11.663	   11.770	  0.086%	 52.589%	     0.000	        1	[densenet201/conv4_block13_0_relu/Relu;densenet201/conv4_block13_0_bn/FusedBatchNormV3]:163
	                 CONV_2D	         7163.701	   35.130	   35.522	  0.261%	 52.850%	     0.000	        1	[densenet201/conv4_block13_1_relu/Relu;densenet201/conv4_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block13_1_conv/Conv2D]:164
	                 CONV_2D	         7199.232	   18.846	   18.956	  0.139%	 52.989%	     0.000	        1	[densenet201/conv4_block13_2_conv/Conv2D1]:165
	           CONCATENATION	         7218.198	    0.142	    0.164	  0.001%	 52.990%	     0.000	        1	[densenet201/conv4_block13_concat/concat]:166
	                     MUL	         7218.369	    9.221	    9.249	  0.068%	 53.058%	     0.000	        1	[densenet201/conv4_block14_0_bn/FusedBatchNormV31]:167
	                     ADD	         7227.626	   12.254	   12.332	  0.091%	 53.149%	     0.000	        1	[densenet201/conv4_block14_0_relu/Relu;densenet201/conv4_block14_0_bn/FusedBatchNormV3]:168
	                 CONV_2D	         7239.967	   37.051	   37.243	  0.273%	 53.422%	     0.000	        1	[densenet201/conv4_block14_1_relu/Relu;densenet201/conv4_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block14_1_conv/Conv2D]:169
	                 CONV_2D	         7277.220	   18.622	   18.987	  0.139%	 53.562%	     0.000	        1	[densenet201/conv4_block14_2_conv/Conv2D1]:170
	           CONCATENATION	         7296.217	    0.180	    0.150	  0.001%	 53.563%	     0.000	        1	[densenet201/conv4_block14_concat/concat]:171
	                     MUL	         7296.374	    9.634	    9.705	  0.071%	 53.634%	     0.000	        1	[densenet201/conv4_block15_0_bn/FusedBatchNormV31]:172
	                     ADD	         7306.087	   12.951	   12.946	  0.095%	 53.729%	     0.000	        1	[densenet201/conv4_block15_0_relu/Relu;densenet201/conv4_block15_0_bn/FusedBatchNormV3]:173
	                 CONV_2D	         7319.042	   38.519	   38.884	  0.286%	 54.014%	     0.000	        1	[densenet201/conv4_block15_1_relu/Relu;densenet201/conv4_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block15_1_conv/Conv2D]:174
	                 CONV_2D	         7357.935	   18.846	   19.001	  0.140%	 54.154%	     0.000	        1	[densenet201/conv4_block15_2_conv/Conv2D1]:175
	           CONCATENATION	         7376.945	    0.165	    0.179	  0.001%	 54.155%	     0.000	        1	[densenet201/conv4_block15_concat/concat]:176
	                     MUL	         7377.132	   10.086	   10.147	  0.075%	 54.230%	     0.000	        1	[densenet201/conv4_block16_0_bn/FusedBatchNormV31]:177
	                     ADD	         7387.289	   13.411	   13.495	  0.099%	 54.329%	     0.000	        1	[densenet201/conv4_block16_0_relu/Relu;densenet201/conv4_block16_0_bn/FusedBatchNormV3]:178
	                 CONV_2D	         7400.792	   40.196	   40.667	  0.299%	 54.627%	     0.000	        1	[densenet201/conv4_block16_1_relu/Relu;densenet201/conv4_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block16_1_conv/Conv2D]:179
	                 CONV_2D	         7441.470	   18.747	   18.948	  0.139%	 54.767%	     0.000	        1	[densenet201/conv4_block16_2_conv/Conv2D1]:180
	           CONCATENATION	         7460.428	    0.168	    0.172	  0.001%	 54.768%	     0.000	        1	[densenet201/conv4_block16_concat/concat]:181
	                     MUL	         7460.607	   10.521	   10.569	  0.078%	 54.846%	     0.000	        1	[densenet201/conv4_block17_0_bn/FusedBatchNormV31]:182
	                     ADD	         7471.185	   13.984	   14.081	  0.103%	 54.949%	     0.000	        1	[densenet201/conv4_block17_0_relu/Relu;densenet201/conv4_block17_0_bn/FusedBatchNormV3]:183
	                 CONV_2D	         7485.274	   42.028	   42.348	  0.311%	 55.260%	     0.000	        1	[densenet201/conv4_block17_1_relu/Relu;densenet201/conv4_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block17_1_conv/Conv2D]:184
	                 CONV_2D	         7527.632	   18.765	   18.989	  0.139%	 55.399%	     0.000	        1	[densenet201/conv4_block17_2_conv/Conv2D1]:185
	           CONCATENATION	         7546.631	    0.199	    0.190	  0.001%	 55.401%	     0.000	        1	[densenet201/conv4_block17_concat/concat]:186
	                     MUL	         7546.828	   10.970	   11.016	  0.081%	 55.482%	     0.000	        1	[densenet201/conv4_block18_0_bn/FusedBatchNormV31]:187
	                     ADD	         7557.853	   14.650	   14.679	  0.108%	 55.589%	     0.000	        1	[densenet201/conv4_block18_0_relu/Relu;densenet201/conv4_block18_0_bn/FusedBatchNormV3]:188
	                 CONV_2D	         7572.541	   43.940	   44.071	  0.324%	 55.913%	     0.000	        1	[densenet201/conv4_block18_1_relu/Relu;densenet201/conv4_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block18_1_conv/Conv2D]:189
	                 CONV_2D	         7616.622	   19.046	   18.941	  0.139%	 56.052%	     0.000	        1	[densenet201/conv4_block18_2_conv/Conv2D1]:190
	           CONCATENATION	         7635.573	    0.178	    0.186	  0.001%	 56.053%	     0.000	        1	[densenet201/conv4_block18_concat/concat]:191
	                     MUL	         7635.766	   11.434	   11.431	  0.084%	 56.137%	     0.000	        1	[densenet201/conv4_block19_0_bn/FusedBatchNormV31]:192
	                     ADD	         7647.206	   15.216	   15.283	  0.112%	 56.250%	     0.000	        1	[densenet201/conv4_block19_0_relu/Relu;densenet201/conv4_block19_0_bn/FusedBatchNormV3]:193
	                 CONV_2D	         7662.499	   45.481	   45.732	  0.336%	 56.585%	     0.000	        1	[densenet201/conv4_block19_1_relu/Relu;densenet201/conv4_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block19_1_conv/Conv2D]:194
	                 CONV_2D	         7708.240	   19.005	   19.105	  0.140%	 56.726%	     0.000	        1	[densenet201/conv4_block19_2_conv/Conv2D1]:195
	           CONCATENATION	         7727.356	    0.206	    0.197	  0.001%	 56.727%	     0.000	        1	[densenet201/conv4_block19_concat/concat]:196
	                     MUL	         7727.559	   11.837	   11.881	  0.087%	 56.814%	     0.000	        1	[densenet201/conv4_block20_0_bn/FusedBatchNormV31]:197
	                     ADD	         7739.449	   15.801	   15.852	  0.116%	 56.931%	     0.000	        1	[densenet201/conv4_block20_0_relu/Relu;densenet201/conv4_block20_0_bn/FusedBatchNormV3]:198
	                 CONV_2D	         7755.311	   47.112	   47.635	  0.350%	 57.280%	     0.000	        1	[densenet201/conv4_block20_1_relu/Relu;densenet201/conv4_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block20_1_conv/Conv2D]:199
	                 CONV_2D	         7802.956	   19.390	   19.093	  0.140%	 57.421%	     0.000	        1	[densenet201/conv4_block20_2_conv/Conv2D1]:200
	           CONCATENATION	         7822.058	    0.210	    0.207	  0.002%	 57.422%	     0.000	        1	[densenet201/conv4_block20_concat/concat]:201
	                     MUL	         7822.273	   12.372	   12.325	  0.091%	 57.513%	     0.000	        1	[densenet201/conv4_block21_0_bn/FusedBatchNormV3]:202
	                     ADD	         7834.607	   16.317	   16.435	  0.121%	 57.633%	     0.000	        1	[densenet201/conv4_block21_0_relu/Relu;densenet201/conv4_block21_0_bn/FusedBatchNormV3]:203
	                 CONV_2D	         7851.050	   48.577	   49.076	  0.360%	 57.994%	     0.000	        1	[densenet201/conv4_block21_1_relu/Relu;densenet201/conv4_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block21_1_conv/Conv2D]:204
	                 CONV_2D	         7900.136	   18.807	   18.940	  0.139%	 58.133%	     0.000	        1	[densenet201/conv4_block21_2_conv/Conv2D1]:205
	           CONCATENATION	         7919.086	    0.283	    0.206	  0.002%	 58.134%	     0.000	        1	[densenet201/conv4_block21_concat/concat]:206
	                     MUL	         7919.300	   12.837	   12.780	  0.094%	 58.228%	     0.000	        1	[densenet201/conv4_block22_0_bn/FusedBatchNormV3]:207
	                     ADD	         7932.089	   16.963	   17.081	  0.125%	 58.354%	     0.000	        1	[densenet201/conv4_block22_0_relu/Relu;densenet201/conv4_block22_0_bn/FusedBatchNormV3]:208
	                 CONV_2D	         7949.179	   50.413	   51.130	  0.375%	 58.729%	     0.000	        1	[densenet201/conv4_block22_1_relu/Relu;densenet201/conv4_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block22_1_conv/Conv2D]:209
	                 CONV_2D	         8000.320	   18.695	   19.106	  0.140%	 58.869%	     0.000	        1	[densenet201/conv4_block22_2_conv/Conv2D1]:210
	           CONCATENATION	         8019.435	    0.205	    0.209	  0.002%	 58.871%	     0.000	        1	[densenet201/conv4_block22_concat/concat]:211
	                     MUL	         8019.651	   13.215	   13.229	  0.097%	 58.968%	     0.000	        1	[densenet201/conv4_block23_0_bn/FusedBatchNormV3]:212
	                     ADD	         8032.889	   17.483	   17.617	  0.129%	 59.097%	     0.000	        1	[densenet201/conv4_block23_0_relu/Relu;densenet201/conv4_block23_0_bn/FusedBatchNormV3]:213
	                 CONV_2D	         8050.516	   52.328	   52.791	  0.388%	 59.485%	     0.000	        1	[densenet201/conv4_block23_1_relu/Relu;densenet201/conv4_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block23_1_conv/Conv2D]:214
	                 CONV_2D	         8103.318	   18.774	   18.999	  0.140%	 59.624%	     0.000	        1	[densenet201/conv4_block23_2_conv/Conv2D1]:215
	           CONCATENATION	         8122.327	    0.245	    0.216	  0.002%	 59.626%	     0.000	        1	[densenet201/conv4_block23_concat/concat]:216
	                     MUL	         8122.550	   13.582	   13.650	  0.100%	 59.726%	     0.000	        1	[densenet201/conv4_block24_0_bn/FusedBatchNormV3]:217
	                     ADD	         8136.208	   18.078	   18.179	  0.133%	 59.860%	     0.000	        1	[densenet201/conv4_block24_0_relu/Relu;densenet201/conv4_block24_0_bn/FusedBatchNormV3]:218
	                 CONV_2D	         8154.396	   53.852	   54.504	  0.400%	 60.260%	     0.000	        1	[densenet201/conv4_block24_1_relu/Relu;densenet201/conv4_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block24_1_conv/Conv2D]:219
	                 CONV_2D	         8208.911	   18.682	   18.996	  0.139%	 60.399%	     0.000	        1	[densenet201/conv4_block24_2_conv/Conv2D1]:220
	           CONCATENATION	         8227.917	    0.227	    0.216	  0.002%	 60.401%	     0.000	        1	[densenet201/conv4_block24_concat/concat]:221
	                     MUL	         8228.141	   14.002	   14.113	  0.104%	 60.505%	     0.000	        1	[densenet201/conv4_block25_0_bn/FusedBatchNormV3]:222
	                     ADD	         8242.262	   18.806	   18.871	  0.139%	 60.643%	     0.000	        1	[densenet201/conv4_block25_0_relu/Relu;densenet201/conv4_block25_0_bn/FusedBatchNormV3]:223
	                 CONV_2D	         8261.143	   55.504	   56.394	  0.414%	 61.057%	     0.000	        1	[densenet201/conv4_block25_1_relu/Relu;densenet201/conv4_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block25_1_conv/Conv2D]:224
	                 CONV_2D	         8317.547	   19.420	   19.116	  0.140%	 61.198%	     0.000	        1	[densenet201/conv4_block25_2_conv/Conv2D1]:225
	           CONCATENATION	         8336.674	    0.239	    0.214	  0.002%	 61.199%	     0.000	        1	[densenet201/conv4_block25_concat/concat]:226
	                     MUL	         8336.896	   14.647	   14.569	  0.107%	 61.306%	     0.000	        1	[densenet201/conv4_block26_0_bn/FusedBatchNormV3]:227
	                     ADD	         8351.476	   19.484	   19.415	  0.143%	 61.449%	     0.000	        1	[densenet201/conv4_block26_0_relu/Relu;densenet201/conv4_block26_0_bn/FusedBatchNormV3]:228
	                 CONV_2D	         8370.901	   58.225	   58.672	  0.431%	 61.880%	     0.000	        1	[densenet201/conv4_block26_1_relu/Relu;densenet201/conv4_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block26_1_conv/Conv2D]:229
	                 CONV_2D	         8429.583	   18.956	   19.063	  0.140%	 62.020%	     0.000	        1	[densenet201/conv4_block26_2_conv/Conv2D1]:230
	           CONCATENATION	         8448.656	    0.199	    0.233	  0.002%	 62.021%	     0.000	        1	[densenet201/conv4_block26_concat/concat]:231
	                     MUL	         8448.900	   15.089	   15.218	  0.112%	 62.133%	     0.000	        1	[densenet201/conv4_block27_0_bn/FusedBatchNormV3]:232
	                     ADD	         8464.127	   20.083	   20.051	  0.147%	 62.280%	     0.000	        1	[densenet201/conv4_block27_0_relu/Relu;densenet201/conv4_block27_0_bn/FusedBatchNormV3]:233
	                 CONV_2D	         8484.188	   61.611	   60.425	  0.444%	 62.724%	     0.000	        1	[densenet201/conv4_block27_1_relu/Relu;densenet201/conv4_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block27_1_conv/Conv2D]:234
	                 CONV_2D	         8544.624	   19.876	   19.047	  0.140%	 62.864%	     0.000	        1	[densenet201/conv4_block27_2_conv/Conv2D1]:235
	           CONCATENATION	         8563.682	    0.212	    0.225	  0.002%	 62.865%	     0.000	        1	[densenet201/conv4_block27_concat/concat]:236
	                     MUL	         8563.915	   15.588	   15.422	  0.113%	 62.979%	     0.000	        1	[densenet201/conv4_block28_0_bn/FusedBatchNormV3]:237
	                     ADD	         8579.346	   20.888	   20.587	  0.151%	 63.130%	     0.000	        1	[densenet201/conv4_block28_0_relu/Relu;densenet201/conv4_block28_0_bn/FusedBatchNormV3]:238
	                 CONV_2D	         8599.944	   63.908	   62.388	  0.458%	 63.588%	     0.000	        1	[densenet201/conv4_block28_1_relu/Relu;densenet201/conv4_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block28_1_conv/Conv2D]:239
	                 CONV_2D	         8662.343	   19.985	   19.182	  0.141%	 63.729%	     0.000	        1	[densenet201/conv4_block28_2_conv/Conv2D1]:240
	           CONCATENATION	         8681.535	    0.200	    0.227	  0.002%	 63.730%	     0.000	        1	[densenet201/conv4_block28_concat/concat]:241
	                     MUL	         8681.769	   16.030	   15.884	  0.117%	 63.847%	     0.000	        1	[densenet201/conv4_block29_0_bn/FusedBatchNormV3]:242
	                     ADD	         8697.663	   21.439	   21.170	  0.155%	 64.003%	     0.000	        1	[densenet201/conv4_block29_0_relu/Relu;densenet201/conv4_block29_0_bn/FusedBatchNormV3]:243
	                 CONV_2D	         8718.842	   65.490	   63.801	  0.468%	 64.471%	     0.000	        1	[densenet201/conv4_block29_1_relu/Relu;densenet201/conv4_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block29_1_conv/Conv2D]:244
	                 CONV_2D	         8782.653	   19.356	   19.014	  0.140%	 64.611%	     0.000	        1	[densenet201/conv4_block29_2_conv/Conv2D1]:245
	           CONCATENATION	         8801.677	    0.170	    0.220	  0.002%	 64.612%	     0.000	        1	[densenet201/conv4_block29_concat/concat]:246
	                     MUL	         8801.905	   16.480	   16.295	  0.120%	 64.732%	     0.000	        1	[densenet201/conv4_block30_0_bn/FusedBatchNormV3]:247
	                     ADD	         8818.209	   22.181	   21.760	  0.160%	 64.892%	     0.000	        1	[densenet201/conv4_block30_0_relu/Relu;densenet201/conv4_block30_0_bn/FusedBatchNormV3]:248
	                 CONV_2D	         8839.980	   66.767	   65.700	  0.482%	 65.374%	     0.000	        1	[densenet201/conv4_block30_1_relu/Relu;densenet201/conv4_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block30_1_conv/Conv2D]:249
	                 CONV_2D	         8905.691	   19.270	   19.127	  0.140%	 65.515%	     0.000	        1	[densenet201/conv4_block30_2_conv/Conv2D1]:250
	           CONCATENATION	         8924.828	    0.193	    0.225	  0.002%	 65.516%	     0.000	        1	[densenet201/conv4_block30_concat/concat]:251
	                     MUL	         8925.061	   16.865	   16.793	  0.123%	 65.639%	     0.000	        1	[densenet201/conv4_block31_0_bn/FusedBatchNormV3]:252
	                     ADD	         8941.864	   22.462	   22.289	  0.164%	 65.803%	     0.000	        1	[densenet201/conv4_block31_0_relu/Relu;densenet201/conv4_block31_0_bn/FusedBatchNormV3]:253
	                 CONV_2D	         8964.163	   68.214	   67.047	  0.492%	 66.295%	     0.000	        1	[densenet201/conv4_block31_1_relu/Relu;densenet201/conv4_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block31_1_conv/Conv2D]:254
	                 CONV_2D	         9031.221	   19.753	   19.080	  0.140%	 66.436%	     0.000	        1	[densenet201/conv4_block31_2_conv/Conv2D1]:255
	           CONCATENATION	         9050.311	    0.200	    0.218	  0.002%	 66.437%	     0.000	        1	[densenet201/conv4_block31_concat/concat]:256
	                     MUL	         9050.538	   17.312	   17.168	  0.126%	 66.563%	     0.000	        1	[densenet201/conv4_block32_0_bn/FusedBatchNormV3]:257
	                     ADD	         9067.715	   23.123	   22.963	  0.169%	 66.732%	     0.000	        1	[densenet201/conv4_block32_0_relu/Relu;densenet201/conv4_block32_0_bn/FusedBatchNormV3]:258
	                 CONV_2D	         9090.688	   70.122	   69.104	  0.507%	 67.239%	     0.000	        1	[densenet201/conv4_block32_1_relu/Relu;densenet201/conv4_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block32_1_conv/Conv2D]:259
	                 CONV_2D	         9159.803	   19.290	   19.086	  0.140%	 67.379%	     0.000	        1	[densenet201/conv4_block32_2_conv/Conv2D1]:260
	           CONCATENATION	         9178.899	    0.285	    0.238	  0.002%	 67.381%	     0.000	        1	[densenet201/conv4_block32_concat/concat]:261
	                     MUL	         9179.145	   17.741	   17.680	  0.130%	 67.511%	     0.000	        1	[densenet201/conv4_block33_0_bn/FusedBatchNormV3]:262
	                     ADD	         9196.835	   23.766	   23.542	  0.173%	 67.684%	     0.000	        1	[densenet201/conv4_block33_0_relu/Relu;densenet201/conv4_block33_0_bn/FusedBatchNormV3]:263
	                 CONV_2D	         9220.387	   71.797	   70.938	  0.521%	 68.205%	     0.000	        1	[densenet201/conv4_block33_1_relu/Relu;densenet201/conv4_block33_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block33_1_conv/Conv2D]:264
	                 CONV_2D	         9291.337	   18.660	   19.078	  0.140%	 68.345%	     0.000	        1	[densenet201/conv4_block33_2_conv/Conv2D1]:265
	           CONCATENATION	         9310.425	    0.261	    0.239	  0.002%	 68.347%	     0.000	        1	[densenet201/conv4_block33_concat/concat]:266
	                     MUL	         9310.673	   17.961	   18.108	  0.133%	 68.480%	     0.000	        1	[densenet201/conv4_block34_0_bn/FusedBatchNormV3]:267
	                     ADD	         9328.790	   24.049	   24.191	  0.178%	 68.657%	     0.000	        1	[densenet201/conv4_block34_0_relu/Relu;densenet201/conv4_block34_0_bn/FusedBatchNormV3]:268
	                 CONV_2D	         9352.993	   72.173	   72.893	  0.535%	 69.192%	     0.000	        1	[densenet201/conv4_block34_1_relu/Relu;densenet201/conv4_block34_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block34_1_conv/Conv2D]:269
	                 CONV_2D	         9425.896	   18.675	   19.010	  0.140%	 69.332%	     0.000	        1	[densenet201/conv4_block34_2_conv/Conv2D1]:270
	           CONCATENATION	         9444.916	    0.274	    0.244	  0.002%	 69.334%	     0.000	        1	[densenet201/conv4_block34_concat/concat]:271
	                     MUL	         9445.167	   18.473	   18.520	  0.136%	 69.470%	     0.000	        1	[densenet201/conv4_block35_0_bn/FusedBatchNormV3]:272
	                     ADD	         9463.697	   24.497	   24.664	  0.181%	 69.651%	     0.000	        1	[densenet201/conv4_block35_0_relu/Relu;densenet201/conv4_block35_0_bn/FusedBatchNormV3]:273
	                 CONV_2D	         9488.371	   73.351	   74.734	  0.549%	 70.200%	     0.000	        1	[densenet201/conv4_block35_1_relu/Relu;densenet201/conv4_block35_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block35_1_conv/Conv2D]:274
	                 CONV_2D	         9563.116	   18.646	   19.262	  0.141%	 70.341%	     0.000	        1	[densenet201/conv4_block35_2_conv/Conv2D1]:275
	           CONCATENATION	         9582.388	    0.261	    0.265	  0.002%	 70.343%	     0.000	        1	[densenet201/conv4_block35_concat/concat]:276
	                     MUL	         9582.662	   18.818	   18.993	  0.139%	 70.482%	     0.000	        1	[densenet201/conv4_block36_0_bn/FusedBatchNormV3]:277
	                     ADD	         9601.665	   25.178	   25.296	  0.186%	 70.668%	     0.000	        1	[densenet201/conv4_block36_0_relu/Relu;densenet201/conv4_block36_0_bn/FusedBatchNormV3]:278
	                 CONV_2D	         9626.972	   75.003	   75.920	  0.557%	 71.226%	     0.000	        1	[densenet201/conv4_block36_1_relu/Relu;densenet201/conv4_block36_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block36_1_conv/Conv2D]:279
	                 CONV_2D	         9702.903	   18.738	   19.130	  0.140%	 71.366%	     0.000	        1	[densenet201/conv4_block36_2_conv/Conv2D1]:280
	           CONCATENATION	         9722.042	    0.257	    0.255	  0.002%	 71.368%	     0.000	        1	[densenet201/conv4_block36_concat/concat]:281
	                     MUL	         9722.304	   19.278	   19.384	  0.142%	 71.510%	     0.000	        1	[densenet201/conv4_block37_0_bn/FusedBatchNormV3]:282
	                     ADD	         9741.698	   25.810	   25.904	  0.190%	 71.701%	     0.000	        1	[densenet201/conv4_block37_0_relu/Relu;densenet201/conv4_block37_0_bn/FusedBatchNormV3]:283
	                 CONV_2D	         9767.611	   76.746	   77.552	  0.569%	 72.270%	     0.000	        1	[densenet201/conv4_block37_1_relu/Relu;densenet201/conv4_block37_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block37_1_conv/Conv2D]:284
	                 CONV_2D	         9845.175	   18.848	   19.092	  0.140%	 72.410%	     0.000	        1	[densenet201/conv4_block37_2_conv/Conv2D1]:285
	           CONCATENATION	         9864.276	    0.368	    0.271	  0.002%	 72.412%	     0.000	        1	[densenet201/conv4_block37_concat/concat]:286
	                     MUL	         9864.556	   19.731	   19.828	  0.146%	 72.558%	     0.000	        1	[densenet201/conv4_block38_0_bn/FusedBatchNormV3]:287
	                     ADD	         9884.394	   26.394	   26.434	  0.194%	 72.752%	     0.000	        1	[densenet201/conv4_block38_0_relu/Relu;densenet201/conv4_block38_0_bn/FusedBatchNormV3]:288
	                 CONV_2D	         9910.838	   79.242	   79.325	  0.582%	 73.334%	     0.000	        1	[densenet201/conv4_block38_1_relu/Relu;densenet201/conv4_block38_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block38_1_conv/Conv2D]:289
	                 CONV_2D	         9990.174	   18.979	   19.103	  0.140%	 73.475%	     0.000	        1	[densenet201/conv4_block38_2_conv/Conv2D1]:290
	           CONCATENATION	        10009.288	    0.263	    0.253	  0.002%	 73.476%	     0.000	        1	[densenet201/conv4_block38_concat/concat]:291
	                     MUL	        10009.549	   20.172	   20.244	  0.149%	 73.625%	     0.000	        1	[densenet201/conv4_block39_0_bn/FusedBatchNormV3]:292
	                     ADD	        10029.803	   26.827	   26.964	  0.198%	 73.823%	     0.000	        1	[densenet201/conv4_block39_0_relu/Relu;densenet201/conv4_block39_0_bn/FusedBatchNormV3]:293
	                 CONV_2D	        10056.777	   80.547	   80.895	  0.594%	 74.417%	     0.000	        1	[densenet201/conv4_block39_1_relu/Relu;densenet201/conv4_block39_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block39_1_conv/Conv2D]:294
	                 CONV_2D	        10137.683	   19.182	   19.091	  0.140%	 74.557%	     0.000	        1	[densenet201/conv4_block39_2_conv/Conv2D1]:295
	           CONCATENATION	        10156.784	    0.244	    0.271	  0.002%	 74.559%	     0.000	        1	[densenet201/conv4_block39_concat/concat]:296
	                     MUL	        10157.063	   20.703	   20.737	  0.152%	 74.711%	     0.000	        1	[densenet201/conv4_block40_0_bn/FusedBatchNormV3]:297
	                     ADD	        10177.811	   27.584	   27.648	  0.203%	 74.914%	     0.000	        1	[densenet201/conv4_block40_0_relu/Relu;densenet201/conv4_block40_0_bn/FusedBatchNormV3]:298
	                 CONV_2D	        10205.469	   81.957	   83.223	  0.611%	 75.526%	     0.000	        1	[densenet201/conv4_block40_1_relu/Relu;densenet201/conv4_block40_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block40_1_conv/Conv2D]:299
	                 CONV_2D	        10288.703	   18.755	   19.215	  0.141%	 75.667%	     0.000	        1	[densenet201/conv4_block40_2_conv/Conv2D1]:300
	           CONCATENATION	        10307.928	    0.276	    0.282	  0.002%	 75.669%	     0.000	        1	[densenet201/conv4_block40_concat/concat]:301
	                     MUL	        10308.219	   21.042	   21.199	  0.156%	 75.824%	     0.000	        1	[densenet201/conv4_block41_0_bn/FusedBatchNormV3]:302
	                     ADD	        10329.429	   28.090	   28.193	  0.207%	 76.031%	     0.000	        1	[densenet201/conv4_block41_0_relu/Relu;densenet201/conv4_block41_0_bn/FusedBatchNormV3]:303
	                 CONV_2D	        10357.632	   83.381	   84.309	  0.619%	 76.650%	     0.000	        1	[densenet201/conv4_block41_1_relu/Relu;densenet201/conv4_block41_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block41_1_conv/Conv2D]:304
	                 CONV_2D	        10441.953	   18.974	   19.087	  0.140%	 76.791%	     0.000	        1	[densenet201/conv4_block41_2_conv/Conv2D1]:305
	           CONCATENATION	        10461.049	    0.315	    0.297	  0.002%	 76.793%	     0.000	        1	[densenet201/conv4_block41_concat/concat]:306
	                     MUL	        10461.355	   21.518	   21.634	  0.159%	 76.952%	     0.000	        1	[densenet201/conv4_block42_0_bn/FusedBatchNormV3]:307
	                     ADD	        10482.998	   28.630	   28.877	  0.212%	 77.164%	     0.000	        1	[densenet201/conv4_block42_0_relu/Relu;densenet201/conv4_block42_0_bn/FusedBatchNormV3]:308
	                 CONV_2D	        10511.885	   86.044	   87.410	  0.642%	 77.806%	     0.000	        1	[densenet201/conv4_block42_1_relu/Relu;densenet201/conv4_block42_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block42_1_conv/Conv2D]:309
	                 CONV_2D	        10599.307	   18.730	   19.098	  0.140%	 77.946%	     0.000	        1	[densenet201/conv4_block42_2_conv/Conv2D1]:310
	           CONCATENATION	        10618.415	    0.304	    0.286	  0.002%	 77.948%	     0.000	        1	[densenet201/conv4_block42_concat/concat]:311
	                     MUL	        10618.708	   21.944	   22.077	  0.162%	 78.110%	     0.000	        1	[densenet201/conv4_block43_0_bn/FusedBatchNormV3]:312
	                     ADD	        10640.796	   29.162	   29.358	  0.216%	 78.326%	     0.000	        1	[densenet201/conv4_block43_0_relu/Relu;densenet201/conv4_block43_0_bn/FusedBatchNormV3]:313
	                 CONV_2D	        10670.165	   87.306	   88.936	  0.653%	 78.979%	     0.000	        1	[densenet201/conv4_block43_1_relu/Relu;densenet201/conv4_block43_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block43_1_conv/Conv2D]:314
	                 CONV_2D	        10759.112	   18.996	   19.077	  0.140%	 79.119%	     0.000	        1	[densenet201/conv4_block43_2_conv/Conv2D1]:315
	           CONCATENATION	        10778.199	    0.414	    0.311	  0.002%	 79.121%	     0.000	        1	[densenet201/conv4_block43_concat/concat]:316
	                     MUL	        10778.519	   22.578	   22.504	  0.165%	 79.286%	     0.000	        1	[densenet201/conv4_block44_0_bn/FusedBatchNormV3]:317
	                     ADD	        10801.033	   29.994	   30.020	  0.220%	 79.507%	     0.000	        1	[densenet201/conv4_block44_0_relu/Relu;densenet201/conv4_block44_0_bn/FusedBatchNormV3]:318
	                 CONV_2D	        10831.063	   91.143	   91.004	  0.668%	 80.175%	     0.000	        1	[densenet201/conv4_block44_1_relu/Relu;densenet201/conv4_block44_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block44_1_conv/Conv2D]:319
	                 CONV_2D	        10922.078	   19.543	   19.208	  0.141%	 80.316%	     0.000	        1	[densenet201/conv4_block44_2_conv/Conv2D1]:320
	           CONCATENATION	        10941.296	    0.294	    0.285	  0.002%	 80.318%	     0.000	        1	[densenet201/conv4_block44_concat/concat]:321
	                     MUL	        10941.589	   23.123	   22.975	  0.169%	 80.487%	     0.000	        1	[densenet201/conv4_block45_0_bn/FusedBatchNormV3]:322
	                     ADD	        10964.574	   30.971	   30.608	  0.225%	 80.711%	     0.000	        1	[densenet201/conv4_block45_0_relu/Relu;densenet201/conv4_block45_0_bn/FusedBatchNormV3]:323
	                 CONV_2D	        10995.192	   94.581	   92.308	  0.678%	 81.389%	     0.000	        1	[densenet201/conv4_block45_1_relu/Relu;densenet201/conv4_block45_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block45_1_conv/Conv2D]:324
	                 CONV_2D	        11087.511	   20.153	   19.144	  0.141%	 81.530%	     0.000	        1	[densenet201/conv4_block45_2_conv/Conv2D1]:325
	           CONCATENATION	        11106.665	    0.304	    0.309	  0.002%	 81.532%	     0.000	        1	[densenet201/conv4_block45_concat/concat]:326
	                     MUL	        11106.984	   23.596	   23.415	  0.172%	 81.704%	     0.000	        1	[densenet201/conv4_block46_0_bn/FusedBatchNormV3]:327
	                     ADD	        11130.409	   31.518	   31.181	  0.229%	 81.933%	     0.000	        1	[densenet201/conv4_block46_0_relu/Relu;densenet201/conv4_block46_0_bn/FusedBatchNormV3]:328
	                 CONV_2D	        11161.600	   96.602	   94.308	  0.692%	 82.625%	     0.000	        1	[densenet201/conv4_block46_1_relu/Relu;densenet201/conv4_block46_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block46_1_conv/Conv2D]:329
	                 CONV_2D	        11255.919	   19.619	   19.013	  0.140%	 82.765%	     0.000	        1	[densenet201/conv4_block46_2_conv/Conv2D1]:330
	           CONCATENATION	        11274.943	    0.290	    0.316	  0.002%	 82.767%	     0.000	        1	[densenet201/conv4_block46_concat/concat]:331
	                     MUL	        11275.267	   24.075	   23.776	  0.175%	 82.942%	     0.000	        1	[densenet201/conv4_block47_0_bn/FusedBatchNormV3]:332
	                     ADD	        11299.053	   31.849	   31.709	  0.233%	 83.175%	     0.000	        1	[densenet201/conv4_block47_0_relu/Relu;densenet201/conv4_block47_0_bn/FusedBatchNormV3]:333
	                 CONV_2D	        11330.772	   97.296	   95.808	  0.703%	 83.878%	     0.000	        1	[densenet201/conv4_block47_1_relu/Relu;densenet201/conv4_block47_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block47_1_conv/Conv2D]:334
	                 CONV_2D	        11426.592	   19.695	   19.126	  0.140%	 84.019%	     0.000	        1	[densenet201/conv4_block47_2_conv/Conv2D1]:335
	           CONCATENATION	        11445.728	    0.259	    0.330	  0.002%	 84.021%	     0.000	        1	[densenet201/conv4_block47_concat/concat]:336
	                     MUL	        11446.066	   24.413	   24.263	  0.178%	 84.199%	     0.000	        1	[densenet201/conv4_block48_0_bn/FusedBatchNormV3]:337
	                     ADD	        11470.340	   32.627	   32.389	  0.238%	 84.437%	     0.000	        1	[densenet201/conv4_block48_0_relu/Relu;densenet201/conv4_block48_0_bn/FusedBatchNormV3]:338
	                 CONV_2D	        11502.740	   99.501	   98.173	  0.721%	 85.158%	     0.000	        1	[densenet201/conv4_block48_1_relu/Relu;densenet201/conv4_block48_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block48_1_conv/Conv2D]:339
	                 CONV_2D	        11600.927	   19.264	   19.162	  0.141%	 85.299%	     0.000	        1	[densenet201/conv4_block48_2_conv/Conv2D1]:340
	           CONCATENATION	        11620.100	    0.248	    0.274	  0.002%	 85.301%	     0.000	        1	[densenet201/conv4_block48_concat/concat]:341
	                     MUL	        11620.382	   24.836	   24.721	  0.182%	 85.482%	     0.000	        1	[densenet201/pool4_bn/FusedBatchNormV3]:342
	                     ADD	        11645.114	   33.235	   33.033	  0.243%	 85.725%	     0.000	        1	[densenet201/pool4_relu/Relu;densenet201/pool4_bn/FusedBatchNormV3]:343
	                 CONV_2D	        11678.159	  657.715	  658.575	  4.836%	 90.560%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	         AVERAGE_POOL_2D	        12336.745	    4.483	    4.504	  0.033%	 90.594%	     0.000	        1	[densenet201/pool4_pool/AvgPool]:345
	                     MUL	        12341.257	    3.078	    3.104	  0.023%	 90.616%	     0.000	        1	[densenet201/conv5_block1_0_bn/FusedBatchNormV31]:346
	                     ADD	        12344.369	    4.092	    4.126	  0.030%	 90.647%	     0.000	        1	[densenet201/conv5_block1_0_relu/Relu;densenet201/conv5_block1_0_bn/FusedBatchNormV3]:347
	                 CONV_2D	        12348.504	   13.671	   13.732	  0.101%	 90.747%	     0.000	        1	[densenet201/conv5_block1_1_relu/Relu;densenet201/conv5_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block1_1_conv/Conv2D]:348
	                 CONV_2D	        12362.247	    5.245	    5.271	  0.039%	 90.786%	     0.000	        1	[densenet201/conv5_block1_2_conv/Conv2D1]:349
	           CONCATENATION	        12367.527	    0.048	    0.055	  0.000%	 90.787%	     0.000	        1	[densenet201/conv5_block1_concat/concat]:350
	                     MUL	        12367.589	    3.210	    3.206	  0.024%	 90.810%	     0.000	        1	[densenet201/conv5_block2_0_bn/FusedBatchNormV31]:351
	                     ADD	        12370.802	    4.250	    4.265	  0.031%	 90.841%	     0.000	        1	[densenet201/conv5_block2_0_relu/Relu;densenet201/conv5_block2_0_bn/FusedBatchNormV3]:352
	                 CONV_2D	        12375.075	   14.073	   14.268	  0.105%	 90.946%	     0.000	        1	[densenet201/conv5_block2_1_relu/Relu;densenet201/conv5_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block2_1_conv/Conv2D]:353
	                 CONV_2D	        12389.352	    5.243	    5.304	  0.039%	 90.985%	     0.000	        1	[densenet201/conv5_block2_2_conv/Conv2D1]:354
	           CONCATENATION	        12394.664	    0.038	    0.043	  0.000%	 90.985%	     0.000	        1	[densenet201/conv5_block2_concat/concat]:355
	                     MUL	        12394.713	    3.293	    3.334	  0.024%	 91.010%	     0.000	        1	[densenet201/conv5_block3_0_bn/FusedBatchNormV31]:356
	                     ADD	        12398.056	    4.381	    4.405	  0.032%	 91.042%	     0.000	        1	[densenet201/conv5_block3_0_relu/Relu;densenet201/conv5_block3_0_bn/FusedBatchNormV3]:357
	                 CONV_2D	        12402.468	   14.586	   14.652	  0.108%	 91.150%	     0.000	        1	[densenet201/conv5_block3_1_relu/Relu;densenet201/conv5_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block3_1_conv/Conv2D]:358
	                 CONV_2D	        12417.129	    5.390	    5.290	  0.039%	 91.189%	     0.000	        1	[densenet201/conv5_block3_2_conv/Conv2D1]:359
	           CONCATENATION	        12422.427	    0.036	    0.047	  0.000%	 91.189%	     0.000	        1	[densenet201/conv5_block3_concat/concat]:360
	                     MUL	        12422.480	    3.410	    3.429	  0.025%	 91.214%	     0.000	        1	[densenet201/conv5_block4_0_bn/FusedBatchNormV31]:361
	                     ADD	        12425.916	    4.592	    4.563	  0.034%	 91.248%	     0.000	        1	[densenet201/conv5_block4_0_relu/Relu;densenet201/conv5_block4_0_bn/FusedBatchNormV3]:362
	                 CONV_2D	        12430.487	   15.130	   15.187	  0.112%	 91.359%	     0.000	        1	[densenet201/conv5_block4_1_relu/Relu;densenet201/conv5_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block4_1_conv/Conv2D]:363
	                 CONV_2D	        12445.684	    5.298	    5.293	  0.039%	 91.398%	     0.000	        1	[densenet201/conv5_block4_2_conv/Conv2D1]:364
	           CONCATENATION	        12450.986	    0.034	    0.049	  0.000%	 91.398%	     0.000	        1	[densenet201/conv5_block4_concat/concat]:365
	                     MUL	        12451.042	    3.527	    3.530	  0.026%	 91.424%	     0.000	        1	[densenet201/conv5_block5_0_bn/FusedBatchNormV31]:366
	                     ADD	        12454.579	    4.683	    4.699	  0.035%	 91.459%	     0.000	        1	[densenet201/conv5_block5_0_relu/Relu;densenet201/conv5_block5_0_bn/FusedBatchNormV3]:367
	                 CONV_2D	        12459.285	   15.550	   15.601	  0.115%	 91.573%	     0.000	        1	[densenet201/conv5_block5_1_relu/Relu;densenet201/conv5_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block5_1_conv/Conv2D]:368
	                 CONV_2D	        12474.895	    5.256	    5.295	  0.039%	 91.612%	     0.000	        1	[densenet201/conv5_block5_2_conv/Conv2D1]:369
	           CONCATENATION	        12480.199	    0.050	    0.052	  0.000%	 91.613%	     0.000	        1	[densenet201/conv5_block5_concat/concat]:370
	                     MUL	        12480.257	    3.663	    3.644	  0.027%	 91.639%	     0.000	        1	[densenet201/conv5_block6_0_bn/FusedBatchNormV31]:371
	                     ADD	        12483.909	    4.822	    4.874	  0.036%	 91.675%	     0.000	        1	[densenet201/conv5_block6_0_relu/Relu;densenet201/conv5_block6_0_bn/FusedBatchNormV3]:372
	                 CONV_2D	        12488.792	   16.277	   16.320	  0.120%	 91.795%	     0.000	        1	[densenet201/conv5_block6_1_relu/Relu;densenet201/conv5_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block6_1_conv/Conv2D]:373
	                 CONV_2D	        12505.120	    5.254	    5.312	  0.039%	 91.834%	     0.000	        1	[densenet201/conv5_block6_2_conv/Conv2D1]:374
	           CONCATENATION	        12510.441	    0.055	    0.059	  0.000%	 91.835%	     0.000	        1	[densenet201/conv5_block6_concat/concat]:375
	                     MUL	        12510.507	    3.731	    3.761	  0.028%	 91.862%	     0.000	        1	[densenet201/conv5_block7_0_bn/FusedBatchNormV31]:376
	                     ADD	        12514.277	    4.953	    4.993	  0.037%	 91.899%	     0.000	        1	[densenet201/conv5_block7_0_relu/Relu;densenet201/conv5_block7_0_bn/FusedBatchNormV3]:377
	                 CONV_2D	        12519.278	   16.692	   16.778	  0.123%	 92.022%	     0.000	        1	[densenet201/conv5_block7_1_relu/Relu;densenet201/conv5_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block7_1_conv/Conv2D]:378
	                 CONV_2D	        12536.066	    5.247	    5.290	  0.039%	 92.061%	     0.000	        1	[densenet201/conv5_block7_2_conv/Conv2D1]:379
	           CONCATENATION	        12541.364	    0.034	    0.051	  0.000%	 92.061%	     0.000	        1	[densenet201/conv5_block7_concat/concat]:380
	                     MUL	        12541.422	    3.843	    3.865	  0.028%	 92.090%	     0.000	        1	[densenet201/conv5_block8_0_bn/FusedBatchNormV31]:381
	                     ADD	        12545.295	    5.120	    5.157	  0.038%	 92.127%	     0.000	        1	[densenet201/conv5_block8_0_relu/Relu;densenet201/conv5_block8_0_bn/FusedBatchNormV3]:382
	                 CONV_2D	        12550.462	   17.224	   17.299	  0.127%	 92.255%	     0.000	        1	[densenet201/conv5_block8_1_relu/Relu;densenet201/conv5_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block8_1_conv/Conv2D]:383
	                 CONV_2D	        12567.770	    5.283	    5.291	  0.039%	 92.293%	     0.000	        1	[densenet201/conv5_block8_2_conv/Conv2D1]:384
	           CONCATENATION	        12573.070	    0.058	    0.060	  0.000%	 92.294%	     0.000	        1	[densenet201/conv5_block8_concat/concat]:385
	                     MUL	        12573.136	    3.948	    3.979	  0.029%	 92.323%	     0.000	        1	[densenet201/conv5_block9_0_bn/FusedBatchNormV31]:386
	                     ADD	        12577.123	    5.283	    5.292	  0.039%	 92.362%	     0.000	        1	[densenet201/conv5_block9_0_relu/Relu;densenet201/conv5_block9_0_bn/FusedBatchNormV3]:387
	                 CONV_2D	        12582.423	   17.640	   17.744	  0.130%	 92.492%	     0.000	        1	[densenet201/conv5_block9_1_relu/Relu;densenet201/conv5_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block9_1_conv/Conv2D]:388
	                 CONV_2D	        12600.175	    5.230	    5.313	  0.039%	 92.531%	     0.000	        1	[densenet201/conv5_block9_2_conv/Conv2D1]:389
	           CONCATENATION	        12605.496	    0.036	    0.051	  0.000%	 92.532%	     0.000	        1	[densenet201/conv5_block9_concat/concat]:390
	                     MUL	        12605.554	    4.062	    4.091	  0.030%	 92.562%	     0.000	        1	[densenet201/conv5_block10_0_bn/FusedBatchNormV31]:391
	                     ADD	        12609.653	    5.402	    5.445	  0.040%	 92.602%	     0.000	        1	[densenet201/conv5_block10_0_relu/Relu;densenet201/conv5_block10_0_bn/FusedBatchNormV3]:392
	                 CONV_2D	        12615.106	   18.148	   18.273	  0.134%	 92.736%	     0.000	        1	[densenet201/conv5_block10_1_relu/Relu;densenet201/conv5_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block10_1_conv/Conv2D]:393
	                 CONV_2D	        12633.388	    5.293	    5.322	  0.039%	 92.775%	     0.000	        1	[densenet201/conv5_block10_2_conv/Conv2D1]:394
	           CONCATENATION	        12638.718	    0.035	    0.051	  0.000%	 92.775%	     0.000	        1	[densenet201/conv5_block10_concat/concat]:395
	                     MUL	        12638.775	    4.173	    4.206	  0.031%	 92.806%	     0.000	        1	[densenet201/conv5_block11_0_bn/FusedBatchNormV31]:396
	                     ADD	        12642.990	    5.539	    5.588	  0.041%	 92.847%	     0.000	        1	[densenet201/conv5_block11_0_relu/Relu;densenet201/conv5_block11_0_bn/FusedBatchNormV3]:397
	                 CONV_2D	        12648.589	   18.552	   18.737	  0.138%	 92.985%	     0.000	        1	[densenet201/conv5_block11_1_relu/Relu;densenet201/conv5_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block11_1_conv/Conv2D]:398
	                 CONV_2D	        12667.336	    5.259	    5.306	  0.039%	 93.024%	     0.000	        1	[densenet201/conv5_block11_2_conv/Conv2D1]:399
	           CONCATENATION	        12672.651	    0.055	    0.057	  0.000%	 93.024%	     0.000	        1	[densenet201/conv5_block11_concat/concat]:400
	                     MUL	        12672.715	    4.320	    4.325	  0.032%	 93.056%	     0.000	        1	[densenet201/conv5_block12_0_bn/FusedBatchNormV31]:401
	                     ADD	        12677.049	    5.700	    5.755	  0.042%	 93.098%	     0.000	        1	[densenet201/conv5_block12_0_relu/Relu;densenet201/conv5_block12_0_bn/FusedBatchNormV3]:402
	                 CONV_2D	        12682.812	   19.112	   19.259	  0.141%	 93.239%	     0.000	        1	[densenet201/conv5_block12_1_relu/Relu;densenet201/conv5_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block12_1_conv/Conv2D]:403
	                 CONV_2D	        12702.080	    5.374	    5.266	  0.039%	 93.278%	     0.000	        1	[densenet201/conv5_block12_2_conv/Conv2D1]:404
	           CONCATENATION	        12707.355	    0.038	    0.058	  0.000%	 93.279%	     0.000	        1	[densenet201/conv5_block12_concat/concat]:405
	                     MUL	        12707.420	    4.390	    4.420	  0.032%	 93.311%	     0.000	        1	[densenet201/conv5_block13_0_bn/FusedBatchNormV31]:406
	                     ADD	        12711.848	    5.838	    5.891	  0.043%	 93.354%	     0.000	        1	[densenet201/conv5_block13_0_relu/Relu;densenet201/conv5_block13_0_bn/FusedBatchNormV3]:407
	                 CONV_2D	        12717.747	   19.461	   19.669	  0.144%	 93.499%	     0.000	        1	[densenet201/conv5_block13_1_relu/Relu;densenet201/conv5_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block13_1_conv/Conv2D]:408
	                 CONV_2D	        12737.425	    5.245	    5.292	  0.039%	 93.538%	     0.000	        1	[densenet201/conv5_block13_2_conv/Conv2D1]:409
	           CONCATENATION	        12742.726	    0.034	    0.063	  0.000%	 93.538%	     0.000	        1	[densenet201/conv5_block13_concat/concat]:410
	                     MUL	        12742.795	    4.493	    4.528	  0.033%	 93.571%	     0.000	        1	[densenet201/conv5_block14_0_bn/FusedBatchNormV31]:411
	                     ADD	        12747.331	    6.022	    6.035	  0.044%	 93.616%	     0.000	        1	[densenet201/conv5_block14_0_relu/Relu;densenet201/conv5_block14_0_bn/FusedBatchNormV3]:412
	                 CONV_2D	        12753.374	   20.048	   20.191	  0.148%	 93.764%	     0.000	        1	[densenet201/conv5_block14_1_relu/Relu;densenet201/conv5_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block14_1_conv/Conv2D]:413
	                 CONV_2D	        12773.575	    5.362	    5.309	  0.039%	 93.803%	     0.000	        1	[densenet201/conv5_block14_2_conv/Conv2D1]:414
	           CONCATENATION	        12778.893	    0.047	    0.058	  0.000%	 93.803%	     0.000	        1	[densenet201/conv5_block14_concat/concat]:415
	                     MUL	        12778.958	    4.603	    4.648	  0.034%	 93.837%	     0.000	        1	[densenet201/conv5_block15_0_bn/FusedBatchNormV31]:416
	                     ADD	        12783.613	    6.131	    6.169	  0.045%	 93.883%	     0.000	        1	[densenet201/conv5_block15_0_relu/Relu;densenet201/conv5_block15_0_bn/FusedBatchNormV3]:417
	                 CONV_2D	        12789.791	   20.489	   20.657	  0.152%	 94.034%	     0.000	        1	[densenet201/conv5_block15_1_relu/Relu;densenet201/conv5_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block15_1_conv/Conv2D]:418
	                 CONV_2D	        12810.457	    5.287	    5.297	  0.039%	 94.073%	     0.000	        1	[densenet201/conv5_block15_2_conv/Conv2D1]:419
	           CONCATENATION	        12815.765	    0.052	    0.057	  0.000%	 94.074%	     0.000	        1	[densenet201/conv5_block15_concat/concat]:420
	                     MUL	        12815.829	    4.721	    4.745	  0.035%	 94.109%	     0.000	        1	[densenet201/conv5_block16_0_bn/FusedBatchNormV31]:421
	                     ADD	        12820.581	    6.267	    6.327	  0.046%	 94.155%	     0.000	        1	[densenet201/conv5_block16_0_relu/Relu;densenet201/conv5_block16_0_bn/FusedBatchNormV3]:422
	                 CONV_2D	        12826.916	   21.003	   21.139	  0.155%	 94.310%	     0.000	        1	[densenet201/conv5_block16_1_relu/Relu;densenet201/conv5_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block16_1_conv/Conv2D]:423
	                 CONV_2D	        12848.066	    5.244	    5.368	  0.039%	 94.350%	     0.000	        1	[densenet201/conv5_block16_2_conv/Conv2D1]:424
	           CONCATENATION	        12853.442	    0.042	    0.060	  0.000%	 94.350%	     0.000	        1	[densenet201/conv5_block16_concat/concat]:425
	                     MUL	        12853.509	    4.832	    4.858	  0.036%	 94.386%	     0.000	        1	[densenet201/conv5_block17_0_bn/FusedBatchNormV31]:426
	                     ADD	        12858.375	    6.450	    6.484	  0.048%	 94.433%	     0.000	        1	[densenet201/conv5_block17_0_relu/Relu;densenet201/conv5_block17_0_bn/FusedBatchNormV3]:427
	                 CONV_2D	        12864.868	   21.364	   21.623	  0.159%	 94.592%	     0.000	        1	[densenet201/conv5_block17_1_relu/Relu;densenet201/conv5_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block17_1_conv/Conv2D]:428
	                 CONV_2D	        12886.502	    5.254	    5.300	  0.039%	 94.631%	     0.000	        1	[densenet201/conv5_block17_2_conv/Conv2D1]:429
	           CONCATENATION	        12891.815	    0.043	    0.060	  0.000%	 94.631%	     0.000	        1	[densenet201/conv5_block17_concat/concat]:430
	                     MUL	        12891.882	    4.934	    4.977	  0.037%	 94.668%	     0.000	        1	[densenet201/conv5_block18_0_bn/FusedBatchNormV31]:431
	                     ADD	        12896.868	    6.556	    6.611	  0.049%	 94.717%	     0.000	        1	[densenet201/conv5_block18_0_relu/Relu;densenet201/conv5_block18_0_bn/FusedBatchNormV3]:432
	                 CONV_2D	        12903.490	   21.946	   22.083	  0.162%	 94.879%	     0.000	        1	[densenet201/conv5_block18_1_relu/Relu;densenet201/conv5_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block18_1_conv/Conv2D]:433
	                 CONV_2D	        12925.583	    5.208	    5.295	  0.039%	 94.918%	     0.000	        1	[densenet201/conv5_block18_2_conv/Conv2D1]:434
	           CONCATENATION	        12930.887	    0.040	    0.061	  0.000%	 94.918%	     0.000	        1	[densenet201/conv5_block18_concat/concat]:435
	                     MUL	        12930.955	    5.063	    5.090	  0.037%	 94.955%	     0.000	        1	[densenet201/conv5_block19_0_bn/FusedBatchNormV31]:436
	                     ADD	        12936.054	    6.700	    6.746	  0.050%	 95.005%	     0.000	        1	[densenet201/conv5_block19_0_relu/Relu;densenet201/conv5_block19_0_bn/FusedBatchNormV3]:437
	                 CONV_2D	        12942.807	   22.417	   22.543	  0.166%	 95.170%	     0.000	        1	[densenet201/conv5_block19_1_relu/Relu;densenet201/conv5_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block19_1_conv/Conv2D]:438
	                 CONV_2D	        12965.360	    5.273	    5.320	  0.039%	 95.210%	     0.000	        1	[densenet201/conv5_block19_2_conv/Conv2D1]:439
	           CONCATENATION	        12970.690	    0.048	    0.062	  0.000%	 95.210%	     0.000	        1	[densenet201/conv5_block19_concat/concat]:440
	                     MUL	        12970.758	    5.158	    5.194	  0.038%	 95.248%	     0.000	        1	[densenet201/conv5_block20_0_bn/FusedBatchNormV31]:441
	                     ADD	        12975.961	    6.876	    6.912	  0.051%	 95.299%	     0.000	        1	[densenet201/conv5_block20_0_relu/Relu;densenet201/conv5_block20_0_bn/FusedBatchNormV3]:442
	                 CONV_2D	        12982.881	   22.894	   23.143	  0.170%	 95.469%	     0.000	        1	[densenet201/conv5_block20_1_relu/Relu;densenet201/conv5_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block20_1_conv/Conv2D]:443
	                 CONV_2D	        13006.035	    5.269	    5.340	  0.039%	 95.508%	     0.000	        1	[densenet201/conv5_block20_2_conv/Conv2D1]:444
	           CONCATENATION	        13011.384	    0.043	    0.062	  0.000%	 95.508%	     0.000	        1	[densenet201/conv5_block20_concat/concat]:445
	                     MUL	        13011.453	    5.266	    5.300	  0.039%	 95.547%	     0.000	        1	[densenet201/conv5_block21_0_bn/FusedBatchNormV31]:446
	                     ADD	        13016.761	    7.011	    7.079	  0.052%	 95.599%	     0.000	        1	[densenet201/conv5_block21_0_relu/Relu;densenet201/conv5_block21_0_bn/FusedBatchNormV3]:447
	                 CONV_2D	        13023.849	   23.195	   23.559	  0.173%	 95.772%	     0.000	        1	[densenet201/conv5_block21_1_relu/Relu;densenet201/conv5_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block21_1_conv/Conv2D]:448
	                 CONV_2D	        13047.419	    5.194	    5.288	  0.039%	 95.811%	     0.000	        1	[densenet201/conv5_block21_2_conv/Conv2D1]:449
	           CONCATENATION	        13052.715	    0.040	    0.065	  0.000%	 95.812%	     0.000	        1	[densenet201/conv5_block21_concat/concat]:450
	                     MUL	        13052.786	    5.369	    5.419	  0.040%	 95.851%	     0.000	        1	[densenet201/conv5_block22_0_bn/FusedBatchNormV31]:451
	                     ADD	        13058.214	    7.155	    7.207	  0.053%	 95.904%	     0.000	        1	[densenet201/conv5_block22_0_relu/Relu;densenet201/conv5_block22_0_bn/FusedBatchNormV3]:452
	                 CONV_2D	        13065.429	   24.001	   24.349	  0.179%	 96.083%	     0.000	        1	[densenet201/conv5_block22_1_relu/Relu;densenet201/conv5_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block22_1_conv/Conv2D]:453
	                 CONV_2D	        13089.788	    5.269	    5.316	  0.039%	 96.122%	     0.000	        1	[densenet201/conv5_block22_2_conv/Conv2D1]:454
	           CONCATENATION	        13095.112	    0.044	    0.063	  0.000%	 96.123%	     0.000	        1	[densenet201/conv5_block22_concat/concat]:455
	                     MUL	        13095.182	    5.472	    5.525	  0.041%	 96.163%	     0.000	        1	[densenet201/conv5_block23_0_bn/FusedBatchNormV31]:456
	                     ADD	        13100.716	    7.280	    7.338	  0.054%	 96.217%	     0.000	        1	[densenet201/conv5_block23_0_relu/Relu;densenet201/conv5_block23_0_bn/FusedBatchNormV3]:457
	                 CONV_2D	        13108.061	   24.410	   24.794	  0.182%	 96.399%	     0.000	        1	[densenet201/conv5_block23_1_relu/Relu;densenet201/conv5_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block23_1_conv/Conv2D]:458
	                 CONV_2D	        13132.865	    5.256	    5.307	  0.039%	 96.438%	     0.000	        1	[densenet201/conv5_block23_2_conv/Conv2D1]:459
	           CONCATENATION	        13138.180	    0.046	    0.063	  0.000%	 96.439%	     0.000	        1	[densenet201/conv5_block23_concat/concat]:460
	                     MUL	        13138.250	    5.664	    5.633	  0.041%	 96.480%	     0.000	        1	[densenet201/conv5_block24_0_bn/FusedBatchNormV31]:461
	                     ADD	        13143.891	    7.451	    7.511	  0.055%	 96.535%	     0.000	        1	[densenet201/conv5_block24_0_relu/Relu;densenet201/conv5_block24_0_bn/FusedBatchNormV3]:462
	                 CONV_2D	        13151.411	   24.982	   25.253	  0.185%	 96.721%	     0.000	        1	[densenet201/conv5_block24_1_relu/Relu;densenet201/conv5_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block24_1_conv/Conv2D]:463
	                 CONV_2D	        13176.674	    5.206	    5.428	  0.040%	 96.760%	     0.000	        1	[densenet201/conv5_block24_2_conv/Conv2D1]:464
	           CONCATENATION	        13182.111	    0.101	    0.089	  0.001%	 96.761%	     0.000	        1	[densenet201/conv5_block24_concat/concat]:465
	                     MUL	        13182.207	    5.697	    5.878	  0.043%	 96.804%	     0.000	        1	[densenet201/conv5_block25_0_bn/FusedBatchNormV31]:466
	                     ADD	        13188.095	    7.611	    7.748	  0.057%	 96.861%	     0.000	        1	[densenet201/conv5_block25_0_relu/Relu;densenet201/conv5_block25_0_bn/FusedBatchNormV3]:467
	                 CONV_2D	        13195.852	   25.446	   25.782	  0.189%	 97.050%	     0.000	        1	[densenet201/conv5_block25_1_relu/Relu;densenet201/conv5_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block25_1_conv/Conv2D]:468
	                 CONV_2D	        13221.643	    5.224	    5.322	  0.039%	 97.089%	     0.000	        1	[densenet201/conv5_block25_2_conv/Conv2D1]:469
	           CONCATENATION	        13226.973	    0.061	    0.076	  0.001%	 97.090%	     0.000	        1	[densenet201/conv5_block25_concat/concat]:470
	                     MUL	        13227.056	    5.844	    5.894	  0.043%	 97.133%	     0.000	        1	[densenet201/conv5_block26_0_bn/FusedBatchNormV31]:471
	                     ADD	        13232.958	    7.800	    7.822	  0.057%	 97.191%	     0.000	        1	[densenet201/conv5_block26_0_relu/Relu;densenet201/conv5_block26_0_bn/FusedBatchNormV3]:472
	                 CONV_2D	        13240.789	   26.316	   26.228	  0.193%	 97.383%	     0.000	        1	[densenet201/conv5_block26_1_relu/Relu;densenet201/conv5_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block26_1_conv/Conv2D]:473
	                 CONV_2D	        13267.027	    5.316	    5.317	  0.039%	 97.422%	     0.000	        1	[densenet201/conv5_block26_2_conv/Conv2D1]:474
	           CONCATENATION	        13272.354	    0.095	    0.075	  0.001%	 97.423%	     0.000	        1	[densenet201/conv5_block26_concat/concat]:475
	                     MUL	        13272.436	    6.043	    6.021	  0.044%	 97.467%	     0.000	        1	[densenet201/conv5_block27_0_bn/FusedBatchNormV31]:476
	                     ADD	        13278.466	    7.879	    8.399	  0.062%	 97.529%	     0.000	        1	[densenet201/conv5_block27_0_relu/Relu;densenet201/conv5_block27_0_bn/FusedBatchNormV3]:477
	                 CONV_2D	        13286.873	   26.368	   26.665	  0.196%	 97.725%	     0.000	        1	[densenet201/conv5_block27_1_relu/Relu;densenet201/conv5_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block27_1_conv/Conv2D]:478
	                 CONV_2D	        13313.549	    5.274	    5.318	  0.039%	 97.764%	     0.000	        1	[densenet201/conv5_block27_2_conv/Conv2D1]:479
	           CONCATENATION	        13318.876	    0.079	    0.081	  0.001%	 97.764%	     0.000	        1	[densenet201/conv5_block27_concat/concat]:480
	                     MUL	        13318.964	    6.034	    6.079	  0.045%	 97.809%	     0.000	        1	[densenet201/conv5_block28_0_bn/FusedBatchNormV31]:481
	                     ADD	        13325.051	    8.028	    8.102	  0.059%	 97.868%	     0.000	        1	[densenet201/conv5_block28_0_relu/Relu;densenet201/conv5_block28_0_bn/FusedBatchNormV3]:482
	                 CONV_2D	        13333.162	   26.766	   27.124	  0.199%	 98.068%	     0.000	        1	[densenet201/conv5_block28_1_relu/Relu;densenet201/conv5_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block28_1_conv/Conv2D]:483
	                 CONV_2D	        13360.297	    5.216	    5.340	  0.039%	 98.107%	     0.000	        1	[densenet201/conv5_block28_2_conv/Conv2D1]:484
	           CONCATENATION	        13365.646	    0.051	    0.078	  0.001%	 98.107%	     0.000	        1	[densenet201/conv5_block28_concat/concat]:485
	                     MUL	        13365.731	    6.270	    6.192	  0.045%	 98.153%	     0.000	        1	[densenet201/conv5_block29_0_bn/FusedBatchNormV31]:486
	                     ADD	        13371.932	    8.340	    8.271	  0.061%	 98.214%	     0.000	        1	[densenet201/conv5_block29_0_relu/Relu;densenet201/conv5_block29_0_bn/FusedBatchNormV3]:487
	                 CONV_2D	        13380.212	   27.468	   27.611	  0.203%	 98.416%	     0.000	        1	[densenet201/conv5_block29_1_relu/Relu;densenet201/conv5_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block29_1_conv/Conv2D]:488
	                 CONV_2D	        13407.833	    5.370	    5.363	  0.039%	 98.456%	     0.000	        1	[densenet201/conv5_block29_2_conv/Conv2D1]:489
	           CONCATENATION	        13413.206	    0.090	    0.093	  0.001%	 98.456%	     0.000	        1	[densenet201/conv5_block29_concat/concat]:490
	                     MUL	        13413.306	    6.375	    6.341	  0.047%	 98.503%	     0.000	        1	[densenet201/conv5_block30_0_bn/FusedBatchNormV31]:491
	                     ADD	        13419.656	    8.441	    8.401	  0.062%	 98.565%	     0.000	        1	[densenet201/conv5_block30_0_relu/Relu;densenet201/conv5_block30_0_bn/FusedBatchNormV3]:492
	                 CONV_2D	        13428.066	   28.920	   28.208	  0.207%	 98.772%	     0.000	        1	[densenet201/conv5_block30_1_relu/Relu;densenet201/conv5_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block30_1_conv/Conv2D]:493
	                 CONV_2D	        13456.284	    5.430	    5.310	  0.039%	 98.811%	     0.000	        1	[densenet201/conv5_block30_2_conv/Conv2D1]:494
	           CONCATENATION	        13461.603	    0.103	    0.080	  0.001%	 98.811%	     0.000	        1	[densenet201/conv5_block30_concat/concat]:495
	                     MUL	        13461.690	    6.452	    6.393	  0.047%	 98.858%	     0.000	        1	[densenet201/conv5_block31_0_bn/FusedBatchNormV31]:496
	                     ADD	        13468.092	    8.656	    8.548	  0.063%	 98.921%	     0.000	        1	[densenet201/conv5_block31_0_relu/Relu;densenet201/conv5_block31_0_bn/FusedBatchNormV3]:497
	                 CONV_2D	        13476.649	   29.393	   28.577	  0.210%	 99.131%	     0.000	        1	[densenet201/conv5_block31_1_relu/Relu;densenet201/conv5_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block31_1_conv/Conv2D]:498
	                 CONV_2D	        13505.236	    5.479	    5.319	  0.039%	 99.170%	     0.000	        1	[densenet201/conv5_block31_2_conv/Conv2D1]:499
	           CONCATENATION	        13510.563	    0.110	    0.086	  0.001%	 99.171%	     0.000	        1	[densenet201/conv5_block31_concat/concat]:500
	                     MUL	        13510.656	    6.627	    6.514	  0.048%	 99.218%	     0.000	        1	[densenet201/conv5_block32_0_bn/FusedBatchNormV31]:501
	                     ADD	        13517.178	    8.807	    8.679	  0.064%	 99.282%	     0.000	        1	[densenet201/conv5_block32_0_relu/Relu;densenet201/conv5_block32_0_bn/FusedBatchNormV3]:502
	                 CONV_2D	        13525.866	   29.921	   29.098	  0.214%	 99.496%	     0.000	        1	[densenet201/conv5_block32_1_relu/Relu;densenet201/conv5_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block32_1_conv/Conv2D]:503
	                 CONV_2D	        13554.977	    5.608	    5.323	  0.039%	 99.535%	     0.000	        1	[densenet201/conv5_block32_2_conv/Conv2D210]:504
	           CONCATENATION	        13560.309	    0.078	    0.067	  0.000%	 99.535%	     0.000	        1	[densenet201/conv5_block32_concat/concat]:505
	                     MUL	        13560.382	    6.781	    6.621	  0.049%	 99.584%	     0.000	        1	[densenet201/bn/FusedBatchNormV31]:506
	                     ADD	        13567.011	    8.942	    8.796	  0.065%	 99.649%	     0.000	        1	[densenet201/relu/Relu;densenet201/bn/FusedBatchNormV3]:507
	                    MEAN	        13575.816	   16.374	   16.114	  0.118%	 99.767%	     0.000	        1	[densenet201/avg_pool/Mean]:508
	         FULLY_CONNECTED	        13591.938	   31.820	   31.661	  0.232%	 99.999%	     0.000	        1	[densenet201/predictions/MatMul;densenet201/predictions/BiasAdd]:509
	                 SOFTMAX	        13623.611	    0.086	    0.093	  0.001%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:510

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	        11678.159	  657.715	  658.575	  4.836%	  4.836%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	                 CONV_2D	            3.756	  328.475	  329.528	  2.420%	  7.255%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                 CONV_2D	          928.559	  318.623	  319.509	  2.346%	  9.601%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	                 CONV_2D	          461.545	  315.473	  318.480	  2.339%	 11.940%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	                 CONV_2D	         1439.994	  321.416	  317.869	  2.334%	 14.274%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	                 CONV_2D	         3229.968	  315.617	  317.523	  2.331%	 16.606%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	                 CONV_2D	         2589.203	  316.215	  317.281	  2.330%	 18.935%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	                 CONV_2D	         1993.329	  315.258	  317.204	  2.329%	 21.264%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	                 CONV_2D	         3680.636	  239.055	  234.027	  1.718%	 22.983%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	                 CONV_2D	         6233.219	  220.497	  216.607	  1.590%	 24.573%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100

Number of nodes executed: 511
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	      200	 10396.736	    76.342%	    76.342%	     0.000	      200
	                     ADD	      102	  1774.317	    13.029%	    89.371%	     0.000	      102
	                     MUL	      102	  1334.196	     9.797%	    99.167%	     0.000	      102
	         FULLY_CONNECTED	        1	    31.661	     0.232%	    99.400%	     0.000	        1
	                     PAD	        2	    21.928	     0.161%	    99.561%	     0.000	        2
	         AVERAGE_POOL_2D	        3	    20.018	     0.147%	    99.708%	     0.000	        3
	           CONCATENATION	       98	    18.321	     0.135%	    99.842%	     0.000	       98
	                    MEAN	        1	    16.114	     0.118%	    99.961%	     0.000	        1
	             MAX_POOL_2D	        1	     5.264	     0.039%	    99.999%	     0.000	        1
	                 SOFTMAX	        1	     0.092	     0.001%	   100.000%	     0.000	        1

Timings (microseconds): count=12 first=13617500 curr=13601876 min=13590923 max=13664407 avg=1.36189e+07 std=19768
Memory (bytes): count=0
511 nodes observed



[ perf record: Woken up 924 times to write data ]
[ perf record: Captured and wrote 231.036 MB /tmp/data.record (1199675 samples) ]

303.288

