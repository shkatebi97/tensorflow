STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/VGG19.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/VGG19.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 27, ), Input shape (50176, 3, ), and Output shape (50176, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (50176, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (50176, 64, ), and Output shape (50176, 64, ), and the ID is 1
	Allocating LowPrecision Activations Tensors with Shape of (50176, 576)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (12544, 64, ), and Output shape (12544, 128, ), and the ID is 2
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 576)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (12544, 128, ), and Output shape (12544, 128, ), and the ID is 3
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 1152)
Applying Conv Low-Precision for Kernel shape (256, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 256, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 6
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 7
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 2304)
Applying Conv Low-Precision for Kernel shape (512, 2304, ), Input shape (784, 256, ), and Output shape (784, 512, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (784, 2304)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (784, 4608)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (784, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 11
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (784, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 12
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 13
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 14
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 15
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
Applying Low-Precision for shape (4096, 25088, ) and Input shape (1, 25088, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (4096, 25088)
	Transformed Activation Shape From: (1, 25088) To: (8, 25088)
Applying Low-Precision for shape (4096, 4096, ) and Input shape (1, 4096, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (4096, 4096)
	Transformed Activation Shape From: (1, 4096) To: (8, 4096)
Applying Low-Precision for shape (1000, 4096, ) and Input shape (1, 4096, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 4096)
	Transformed Activation Shape From: (1, 4096) To: (8, 4096)
The input model file size (MB): 143.801
Initialized session in 3168.37ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=4 first=45485376 curr=45338558 min=45285800 max=45485376 avg=4.53938e+07 std=84053

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=4 first=45352604 curr=45368930 min=45337693 max=45368930 avg=4.53569e+07 std=12893

Inference timings in us: Init: 3168367, First inference: 45485376, Warmup (avg): 4.53938e+07, Inference (avg): 4.53569e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=277.555 overall=341.246
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	 3163.104	 3163.104	100.000%	100.000%	279640.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	 3163.104	 3163.104	100.000%	100.000%	279640.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	  3163.104	   100.000%	   100.000%	279640.000	        1

Timings (microseconds): count=1 curr=3163104
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.018	  375.588	  375.608	  0.828%	  0.828%	     0.000	        1	[vgg19/block1_conv1/Relu;vgg19/block1_conv1/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv1/Conv2D]:0
	                 CONV_2D	          375.637	 5038.242	 5017.753	 11.063%	 11.891%	     0.000	        1	[vgg19/block1_conv2/Relu;vgg19/block1_conv2/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv2/Conv2D]:1
	             MAX_POOL_2D	         5393.402	   12.448	   12.483	  0.028%	 11.919%	     0.000	        1	[vgg19/block1_pool/MaxPool]:2
	                 CONV_2D	         5405.896	 2090.729	 2093.794	  4.616%	 16.535%	     0.000	        1	[vgg19/block2_conv1/Relu;vgg19/block2_conv1/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv1/Conv2D]:3
	                 CONV_2D	         7499.703	 4164.777	 4178.413	  9.212%	 25.747%	     0.000	        1	[vgg19/block2_conv2/Relu;vgg19/block2_conv2/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv2/Conv2D]:4
	             MAX_POOL_2D	        11678.128	    5.755	    5.738	  0.013%	 25.760%	     0.000	        1	[vgg19/block2_pool/MaxPool]:5
	                 CONV_2D	        11683.877	 1991.062	 1987.774	  4.383%	 30.142%	     0.000	        1	[vgg19/block3_conv1/Relu;vgg19/block3_conv1/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv1/Conv2D]:6
	                 CONV_2D	        13671.674	 4010.866	 4005.086	  8.830%	 38.973%	     0.000	        1	[vgg19/block3_conv2/Relu;vgg19/block3_conv2/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv2/Conv2D]:7
	                 CONV_2D	        17676.772	 3985.466	 4003.887	  8.828%	 47.800%	     0.000	        1	[vgg19/block3_conv3/Relu;vgg19/block3_conv3/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv3/Conv2D]:8
	                 CONV_2D	        21680.670	 4006.224	 4000.969	  8.821%	 56.621%	     0.000	        1	[vgg19/block3_conv4/Relu;vgg19/block3_conv4/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv4/Conv2D]:9
	             MAX_POOL_2D	        25681.650	    2.740	    2.770	  0.006%	 56.627%	     0.000	        1	[vgg19/block3_pool/MaxPool]:10
	                 CONV_2D	        25684.430	 1945.924	 1947.961	  4.295%	 60.922%	     0.000	        1	[vgg19/block4_conv1/Relu;vgg19/block4_conv1/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv1/Conv2D]:11
	                 CONV_2D	        27632.403	 3906.842	 3923.252	  8.650%	 69.572%	     0.000	        1	[vgg19/block4_conv2/Relu;vgg19/block4_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv2/Conv2D]:12
	                 CONV_2D	        31555.667	 3931.836	 3917.317	  8.637%	 78.209%	     0.000	        1	[vgg19/block4_conv3/Relu;vgg19/block4_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv3/Conv2D]:13
	                 CONV_2D	        35472.995	 3917.850	 3920.287	  8.643%	 86.852%	     0.000	        1	[vgg19/block4_conv4/Relu;vgg19/block4_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv4/Conv2D]:14
	             MAX_POOL_2D	        39393.294	    1.471	    1.476	  0.003%	 86.855%	     0.000	        1	[vgg19/block4_pool/MaxPool]:15
	                 CONV_2D	        39394.778	  980.065	  985.042	  2.172%	 89.027%	     0.000	        1	[vgg19/block5_conv1/Relu;vgg19/block5_conv1/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv1/Conv2D]:16
	                 CONV_2D	        40379.835	  987.266	  989.113	  2.181%	 91.208%	     0.000	        1	[vgg19/block5_conv2/Relu;vgg19/block5_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv2/Conv2D]:17
	                 CONV_2D	        41368.960	  997.739	  988.523	  2.179%	 93.387%	     0.000	        1	[vgg19/block5_conv3/Relu;vgg19/block5_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv3/Conv2D]:18
	                 CONV_2D	        42357.495	  980.895	  987.220	  2.177%	 95.564%	     0.000	        1	[vgg19/block5_conv4/Relu;vgg19/block5_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv4/Conv2D]:19
	             MAX_POOL_2D	        43344.727	    0.390	    0.391	  0.001%	 95.565%	     0.000	        1	[vgg19/block5_pool/MaxPool]:20
	                 RESHAPE	        43345.125	    0.013	    0.015	  0.000%	 95.565%	     0.000	        1	[vgg19/flatten/Reshape]:21
	         FULLY_CONNECTED	        43345.147	 1679.484	 1671.631	  3.686%	 99.250%	     0.000	        1	[vgg19/fc1/MatMul;vgg19/fc1/Relu;vgg19/fc1/BiasAdd]:22
	         FULLY_CONNECTED	        45016.789	  271.420	  272.317	  0.600%	 99.851%	     0.000	        1	[vgg19/fc2/MatMul;vgg19/fc2/Relu;vgg19/fc2/BiasAdd]:23
	         FULLY_CONNECTED	        45289.117	   67.081	   67.652	  0.149%	100.000%	     0.000	        1	[vgg19/predictions/MatMul;vgg19/predictions/BiasAdd]:24
	                 SOFTMAX	        45356.780	    0.085	    0.089	  0.000%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:25

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          375.637	 5038.242	 5017.753	 11.063%	 11.063%	     0.000	        1	[vgg19/block1_conv2/Relu;vgg19/block1_conv2/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv2/Conv2D]:1
	                 CONV_2D	         7499.703	 4164.777	 4178.413	  9.212%	 20.275%	     0.000	        1	[vgg19/block2_conv2/Relu;vgg19/block2_conv2/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv2/Conv2D]:4
	                 CONV_2D	        13671.674	 4010.866	 4005.086	  8.830%	 29.105%	     0.000	        1	[vgg19/block3_conv2/Relu;vgg19/block3_conv2/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv2/Conv2D]:7
	                 CONV_2D	        17676.772	 3985.466	 4003.887	  8.828%	 37.933%	     0.000	        1	[vgg19/block3_conv3/Relu;vgg19/block3_conv3/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv3/Conv2D]:8
	                 CONV_2D	        21680.670	 4006.224	 4000.969	  8.821%	 46.754%	     0.000	        1	[vgg19/block3_conv4/Relu;vgg19/block3_conv4/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv4/Conv2D]:9
	                 CONV_2D	        27632.403	 3906.842	 3923.252	  8.650%	 55.404%	     0.000	        1	[vgg19/block4_conv2/Relu;vgg19/block4_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv2/Conv2D]:12
	                 CONV_2D	        35472.995	 3917.850	 3920.287	  8.643%	 64.047%	     0.000	        1	[vgg19/block4_conv4/Relu;vgg19/block4_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv4/Conv2D]:14
	                 CONV_2D	        31555.667	 3931.836	 3917.317	  8.637%	 72.684%	     0.000	        1	[vgg19/block4_conv3/Relu;vgg19/block4_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv3/Conv2D]:13
	                 CONV_2D	         5405.896	 2090.729	 2093.794	  4.616%	 77.300%	     0.000	        1	[vgg19/block2_conv1/Relu;vgg19/block2_conv1/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv1/Conv2D]:3
	                 CONV_2D	        11683.877	 1991.062	 1987.774	  4.383%	 81.683%	     0.000	        1	[vgg19/block3_conv1/Relu;vgg19/block3_conv1/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv1/Conv2D]:6

Number of nodes executed: 26
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       16	 43321.992	    95.514%	    95.514%	     0.000	       16
	         FULLY_CONNECTED	        3	  2011.599	     4.435%	    99.949%	     0.000	        3
	             MAX_POOL_2D	        5	    22.857	     0.050%	   100.000%	     0.000	        5
	                 SOFTMAX	        1	     0.088	     0.000%	   100.000%	     0.000	        1
	                 RESHAPE	        1	     0.015	     0.000%	   100.000%	     0.000	        1

Timings (microseconds): count=4 first=45352258 curr=45368535 min=45337364 max=45368535 avg=4.53566e+07 std=12875
Memory (bytes): count=0
26 nodes observed



free(): invalid size
[ perf record: Woken up 1157 times to write data ]
Warning:
Processed 1464647 events and lost 2 chunks!

Check IO/CPU overload!

[ perf record: Captured and wrote 289.394 MB /tmp/data.record (1462081 samples) ]

369.488

