STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/ResNet152.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/ResNet152.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 160)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 160)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 1
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (64, 64, ), Input shape (3136, 64, ), and Output shape (3136, 64, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
2
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3136, 64, ), and Output shape (3136, 64, ), and the ID is 3
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (64, 256, ), Input shape (3136, 256, ), and Output shape (3136, 64, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 256)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3136, 64, ), and Output shape (3136, 64, ), and the ID is 6
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (64, 256, ), Input shape (3136, 256, ), and Output shape (3136, 64, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 256)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3136, 64, ), and Output shape (3136, 64, ), and the ID is 9
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (512, 256, ), Input shape (3136, 256, ), and Output shape (784, 512, ), and the ID is 11
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 256)
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (3136, 256, ), and Output shape (784, 128, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 15
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 22
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 24
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 25
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 26
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 27
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 28
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 31
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 32
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 33
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (1024, 512, ), Input shape (784, 512, ), and Output shape (196, 1024, ), and the ID is 36
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 512)
	Allocating LowPrecision Activations Tensors with Shape of (200, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (784, 512, ), and Output shape (196, 256, ), and the ID is 37
	Allocating LowPrecision Weight Tensors with Shape of (256, 512)
	Allocating LowPrecision Activations Tensors with Shape of (200, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 38
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 39
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 40
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 41
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 42
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 43
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 44
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 45
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 46
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 48
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 50
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 51
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 52
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 53
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 54
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 55
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 56
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 57
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 58
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 59
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 60
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 61
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 62
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 63
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 64
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 65
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 66
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 67
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 68
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 69
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 70
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 71
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 72
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 73
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 74
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 75
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 76
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 77
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 78
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 79
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 80
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 81
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 82
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 83
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 84
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 85
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 86
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 87
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 88
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 89
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 90
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 92
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 93
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 95
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 96
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 97
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 98
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 99
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 100
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 101
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 102
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 103
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 104	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)

	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 105
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 106
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 107
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 108
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 109
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 110
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 111
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
, and the ID is 112
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 113
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 114
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 115
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 117
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 118
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 119
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 120
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 121
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 122
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 123
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 124
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 125
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
126
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 127
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 128
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 129
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 130
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 131
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 132	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 133
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 134
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 135	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 136	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 137
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 138
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 139
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
140
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 141
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 142
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (196, 256, ), and Output shape (196, 256, ), and the ID is 143
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 144
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (2048, 1024, ), Input shape (196, 1024, ), and Output shape (49, 2048, ), and the ID is 145
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1024)
Applying Conv Low-Precision for Kernel shape (512, 1024, ), Input shape (196, 1024, ), and Output shape (49, 512, ), and the ID is 146	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (512, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (49, 512, ), and Output shape (49, 512, ), and the ID is 147
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (56, 4608)
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 148
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 512)
	Allocating LowPrecision Activations Tensors with Shape of (56, 512)
Applying Conv Low-Precision for Kernel shape (512, 2048, ), Input shape (49, 2048, ), and Output shape (49, 512, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 2048)
	Allocating LowPrecision Activations Tensors with Shape of (56, 2048)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (49, 512, ), and Output shape (49, 512, ), and the ID is 150
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (56, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 151
	Allocating LowPrecision Weight Tensors with Shape of (2048, 512)
	Allocating LowPrecision Activations Tensors with Shape of (56, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 2048, ), Input shape (49, 2048, ), and Output shape (49, 512, ), and the ID is 152
	Allocating LowPrecision Weight Tensors with Shape of (512, 2048)
	Allocating LowPrecision Activations Tensors with Shape of (56, 2048)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (49, 512, ), and Output shape (49, 512, ), and the ID is 153	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (56, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 154
	Allocating LowPrecision Weight Tensors with Shape of (2048, 512)
	Allocating LowPrecision Activations Tensors with Shape of (56, 512)
Applying Low-Precision for shape (1000, 2048, ) and Input shape (1, 2048, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 2048)
	Transformed Activation Shape From: (1, 2048) To: (8, 2048)
The input model file size (MB): 61.9989
Initialized session in 935.309ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=6 first=26100340 curr=26118368 min=26091508 max=26138666 avg=2.61115e+07 std=14875

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=6 first=26089314 curr=26108446 min=26064677 max=26108446 avg=2.60855e+07 std=14608

Inference timings in us: Init: 935309, First inference: 26100340, Warmup (avg): 2.61115e+07, Inference (avg): 2.60855e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=122.293 overall=128.383
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  891.684	  891.684	100.000%	100.000%	109476.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  891.684	  891.684	100.000%	100.000%	109476.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   891.684	   100.000%	   100.000%	109476.000	        1

Timings (microseconds): count=1 curr=891684
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.023	    3.807	    3.733	  0.014%	  0.014%	     0.000	        1	[resnet152/conv1_pad/Pad]:0
	                 CONV_2D	            3.765	  327.597	  328.628	  1.260%	  1.274%	     0.000	        1	[resnet152/conv1_relu/Relu;resnet152/conv1_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv1_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv1_conv/Conv2D]:1
	                     PAD	          332.405	   18.144	   18.236	  0.070%	  1.344%	     0.000	        1	[resnet152/pool1_pad/Pad]:2
	             MAX_POOL_2D	          350.653	    5.192	    5.249	  0.020%	  1.364%	     0.000	        1	[resnet152/pool1_pool/MaxPool]:3
	                 CONV_2D	          355.914	  136.609	  138.775	  0.532%	  1.896%	     0.000	        1	[resnet152/conv2_block1_0_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block1_0_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_conv/Conv2D]:4
	                 CONV_2D	          494.702	   36.935	   37.662	  0.144%	  2.041%	     0.000	        1	[resnet152/conv2_block1_1_relu/Relu;resnet152/conv2_block1_1_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block1_1_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block1_1_conv/Conv2D]:5
	                 CONV_2D	          532.378	  278.635	  279.856	  1.073%	  3.114%	     0.000	        1	[resnet152/conv2_block1_2_relu/Relu;resnet152/conv2_block1_2_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block1_2_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block1_2_conv/Conv2D]:6
	                 CONV_2D	          812.247	  138.221	  138.151	  0.530%	  3.643%	     0.000	        1	[resnet152/conv2_block1_3_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block1_3_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv2_block1_3_conv/Conv2D]:7
	                     ADD	          950.412	   76.236	   76.389	  0.293%	  3.936%	     0.000	        1	[resnet152/conv2_block1_out/Relu;resnet152/conv2_block1_add/add]:8
	                 CONV_2D	         1026.814	  124.116	  124.634	  0.478%	  4.414%	     0.000	        1	[resnet152/conv2_block2_1_relu/Relu;resnet152/conv2_block2_1_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block2_1_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block2_1_conv/Conv2D]:9
	                 CONV_2D	         1151.460	  276.124	  277.409	  1.064%	  5.478%	     0.000	        1	[resnet152/conv2_block2_2_relu/Relu;resnet152/conv2_block2_2_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block2_2_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block2_2_conv/Conv2D]:10
	                 CONV_2D	         1428.881	  137.647	  137.545	  0.527%	  6.005%	     0.000	        1	[resnet152/conv2_block2_3_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block2_3_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv2_block2_3_conv/Conv2D]:11
	                     ADD	         1566.438	   78.007	   76.740	  0.294%	  6.299%	     0.000	        1	[resnet152/conv2_block2_out/Relu;resnet152/conv2_block2_add/add]:12
	                 CONV_2D	         1643.189	  126.946	  125.129	  0.480%	  6.779%	     0.000	        1	[resnet152/conv2_block3_1_relu/Relu;resnet152/conv2_block3_1_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block3_1_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block3_1_conv/Conv2D]:13
	                 CONV_2D	         1768.331	  283.199	  280.348	  1.075%	  7.854%	     0.000	        1	[resnet152/conv2_block3_2_relu/Relu;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block3_2_conv/BiasAdd;resnet152/conv2_block3_2_conv/Conv2D]:14
	                 CONV_2D	         2048.691	  137.973	  137.915	  0.529%	  8.382%	     0.000	        1	[resnet152/conv2_block3_3_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block3_3_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv2_block3_3_conv/Conv2D]:15
	                     ADD	         2186.619	   77.275	   76.649	  0.294%	  8.676%	     0.000	        1	[resnet152/conv2_block3_out/Relu;resnet152/conv2_block3_add/add]:16
	                 CONV_2D	         2263.282	  220.530	  219.236	  0.841%	  9.517%	     0.000	        1	[resnet152/conv3_block1_0_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block1_0_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_conv/Conv2D]:17
	                 CONV_2D	         2482.530	   57.202	   57.141	  0.219%	  9.736%	     0.000	        1	[resnet152/conv3_block1_1_relu/Relu;resnet152/conv3_block1_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block1_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_conv/Conv2D]:18
	                 CONV_2D	         2539.683	  255.129	  254.841	  0.977%	 10.713%	     0.000	        1	[resnet152/conv3_block1_2_relu/Relu;resnet152/conv3_block1_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block1_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block1_2_conv/Conv2D]:19
	                 CONV_2D	         2794.536	  116.705	  118.336	  0.454%	 11.167%	     0.000	        1	[resnet152/conv3_block1_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block1_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block1_3_conv/Conv2D]:20
	                     ADD	         2912.883	   38.276	   38.544	  0.148%	 11.314%	     0.000	        1	[resnet152/conv3_block1_out/Relu;resnet152/conv3_block1_add/add]:21
	                 CONV_2D	         2951.439	  110.345	  112.905	  0.433%	 11.747%	     0.000	        1	[resnet152/conv3_block2_1_relu/Relu;resnet152/conv3_block2_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block2_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block2_1_conv/Conv2D]:22
	                 CONV_2D	         3064.357	  254.164	  257.046	  0.985%	 12.733%	     0.000	        1	[resnet152/conv3_block2_2_relu/Relu;resnet152/conv3_block2_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block2_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block2_2_conv/Conv2D]:23
	                 CONV_2D	         3321.416	  117.338	  118.263	  0.453%	 13.186%	     0.000	        1	[resnet152/conv3_block2_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block2_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block2_3_conv/Conv2D]:24
	                     ADD	         3439.690	   38.212	   38.370	  0.147%	 13.333%	     0.000	        1	[resnet152/conv3_block2_out/Relu;resnet152/conv3_block2_add/add]:25
	                 CONV_2D	         3478.071	  110.758	  112.510	  0.431%	 13.765%	     0.000	        1	[resnet152/conv3_block3_1_relu/Relu;resnet152/conv3_block3_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block3_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block3_1_conv/Conv2D]:26
	                 CONV_2D	         3590.593	  254.241	  255.288	  0.979%	 14.743%	     0.000	        1	[resnet152/conv3_block3_2_relu/Relu;resnet152/conv3_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block3_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block3_2_conv/Conv2D]:27
	                 CONV_2D	         3845.893	  117.869	  117.597	  0.451%	 15.194%	     0.000	        1	[resnet152/conv3_block3_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block3_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block3_3_conv/Conv2D]:28
	                     ADD	         3963.501	   38.424	   38.265	  0.147%	 15.341%	     0.000	        1	[resnet152/conv3_block3_out/Relu;resnet152/conv3_block3_add/add]:29
	                 CONV_2D	         4001.777	  112.654	  112.279	  0.430%	 15.771%	     0.000	        1	[resnet152/conv3_block4_1_relu/Relu;resnet152/conv3_block4_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block4_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block4_1_conv/Conv2D]:30
	                 CONV_2D	         4114.068	  261.308	  257.656	  0.988%	 16.759%	     0.000	        1	[resnet152/conv3_block4_2_relu/Relu;resnet152/conv3_block4_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block4_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block4_2_conv/Conv2D]:31
	                 CONV_2D	         4371.736	  119.916	  118.826	  0.456%	 17.215%	     0.000	        1	[resnet152/conv3_block4_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block4_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block4_3_conv/Conv2D]:32
	                     ADD	         4490.574	   38.541	   38.344	  0.147%	 17.362%	     0.000	        1	[resnet152/conv3_block4_out/Relu;resnet152/conv3_block4_add/add]:33
	                 CONV_2D	         4528.929	  113.943	  112.485	  0.431%	 17.793%	     0.000	        1	[resnet152/conv3_block5_1_relu/Relu;resnet152/conv3_block5_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block5_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block5_1_conv/Conv2D]:34
	                 CONV_2D	         4641.432	  258.191	  256.134	  0.982%	 18.775%	     0.000	        1	[resnet152/conv3_block5_2_relu/Relu;resnet152/conv3_block5_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block5_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block5_2_conv/Conv2D]:35
	                 CONV_2D	         4897.580	  116.855	  117.256	  0.450%	 19.225%	     0.000	        1	[resnet152/conv3_block5_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block5_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block5_3_conv/Conv2D]:36
	                     ADD	         5014.848	   38.244	   38.182	  0.146%	 19.371%	     0.000	        1	[resnet152/conv3_block5_out/Relu;resnet152/conv3_block5_add/add]:37
	                 CONV_2D	         5053.044	  110.662	  111.036	  0.426%	 19.797%	     0.000	        1	[resnet152/conv3_block6_1_relu/Relu;resnet152/conv3_block6_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block6_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block6_1_conv/Conv2D]:38
	                 CONV_2D	         5164.092	  254.186	  255.831	  0.981%	 20.778%	     0.000	        1	[resnet152/conv3_block6_2_relu/Relu;resnet152/conv3_block6_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block6_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block6_2_conv/Conv2D]:39
	                 CONV_2D	         5419.934	  116.996	  118.846	  0.456%	 21.233%	     0.000	        1	[resnet152/conv3_block6_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block6_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block6_3_conv/Conv2D]:40
	                     ADD	         5538.792	   38.273	   38.441	  0.147%	 21.381%	     0.000	        1	[resnet152/conv3_block6_out/Relu;resnet152/conv3_block6_add/add]:41
	                 CONV_2D	         5577.244	  110.501	  112.868	  0.433%	 21.813%	     0.000	        1	[resnet152/conv3_block7_1_relu/Relu;resnet152/conv3_block7_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block7_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block7_1_conv/Conv2D]:42
	                 CONV_2D	         5690.123	  254.280	  256.319	  0.983%	 22.796%	     0.000	        1	[resnet152/conv3_block7_2_relu/Relu;resnet152/conv3_block7_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block7_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block7_2_conv/Conv2D]:43
	                 CONV_2D	         5946.454	  117.219	  118.285	  0.453%	 23.250%	     0.000	        1	[resnet152/conv3_block7_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block7_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block7_3_conv/Conv2D]:44
	                     ADD	         6064.752	   38.450	   38.349	  0.147%	 23.397%	     0.000	        1	[resnet152/conv3_block7_out/Relu;resnet152/conv3_block7_add/add]:45
	                 CONV_2D	         6103.112	  110.562	  111.146	  0.426%	 23.823%	     0.000	        1	[resnet152/conv3_block8_1_relu/Relu;resnet152/conv3_block8_1_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block8_1_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block8_1_conv/Conv2D]:46
	                 CONV_2D	         6214.276	  256.326	  255.103	  0.978%	 24.801%	     0.000	        1	[resnet152/conv3_block8_2_relu/Relu;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block8_2_conv/BiasAdd;resnet152/conv3_block8_2_conv/Conv2D]:47
	                 CONV_2D	         6469.391	  118.625	  118.006	  0.452%	 25.253%	     0.000	        1	[resnet152/conv3_block8_3_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block8_3_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block8_3_conv/Conv2D]:48
	                     ADD	         6587.409	   39.308	   38.545	  0.148%	 25.401%	     0.000	        1	[resnet152/conv3_block8_out/Relu;resnet152/conv3_block8_add/add]:49
	                 CONV_2D	         6625.965	  221.934	  215.331	  0.826%	 26.226%	     0.000	        1	[resnet152/conv4_block1_0_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block1_0_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block1_0_conv/Conv2D]:50
	                 CONV_2D	         6841.308	   55.992	   55.131	  0.211%	 26.438%	     0.000	        1	[resnet152/conv4_block1_1_relu/Relu;resnet152/conv4_block1_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block1_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block1_1_conv/Conv2D]:51
	                 CONV_2D	         6896.450	  252.223	  248.040	  0.951%	 27.389%	     0.000	        1	[resnet152/conv4_block1_2_relu/Relu;resnet152/conv4_block1_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block1_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block1_2_conv/Conv2D]:52
	                 CONV_2D	         7144.501	  110.803	  110.371	  0.423%	 27.812%	     0.000	        1	[resnet152/conv4_block1_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block1_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block1_3_conv/Conv2D]:53
	                     ADD	         7254.883	   19.251	   19.112	  0.073%	 27.885%	     0.000	        1	[resnet152/conv4_block1_out/Relu;resnet152/conv4_block1_add/add]:54
	                 CONV_2D	         7274.006	  109.928	  108.412	  0.416%	 28.301%	     0.000	        1	[resnet152/conv4_block2_1_relu/Relu;resnet152/conv4_block2_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block2_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block2_1_conv/Conv2D]:55
	                 CONV_2D	         7382.428	  244.656	  245.533	  0.941%	 29.242%	     0.000	        1	[resnet152/conv4_block2_2_relu/Relu;resnet152/conv4_block2_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block2_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block2_2_conv/Conv2D]:56
	                 CONV_2D	         7627.972	  109.722	  110.165	  0.422%	 29.665%	     0.000	        1	[resnet152/conv4_block2_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block2_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block2_3_conv/Conv2D]:57
	                     ADD	         7738.147	   19.053	   19.109	  0.073%	 29.738%	     0.000	        1	[resnet152/conv4_block2_out/Relu;resnet152/conv4_block2_add/add]:58
	                 CONV_2D	         7757.267	  107.523	  109.666	  0.420%	 30.158%	     0.000	        1	[resnet152/conv4_block3_1_relu/Relu;resnet152/conv4_block3_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block3_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block3_1_conv/Conv2D]:59
	                 CONV_2D	         7866.944	  244.901	  249.087	  0.955%	 31.113%	     0.000	        1	[resnet152/conv4_block3_2_relu/Relu;resnet152/conv4_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block3_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block3_2_conv/Conv2D]:60
	                 CONV_2D	         8116.043	  109.643	  110.566	  0.424%	 31.537%	     0.000	        1	[resnet152/conv4_block3_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block3_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block3_3_conv/Conv2D]:61
	                     ADD	         8226.621	   19.097	   19.175	  0.074%	 31.611%	     0.000	        1	[resnet152/conv4_block3_out/Relu;resnet152/conv4_block3_add/add]:62
	                 CONV_2D	         8245.807	  107.815	  108.955	  0.418%	 32.028%	     0.000	        1	[resnet152/conv4_block4_1_relu/Relu;resnet152/conv4_block4_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block4_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block4_1_conv/Conv2D]:63
	                 CONV_2D	         8354.772	  244.067	  247.935	  0.951%	 32.979%	     0.000	        1	[resnet152/conv4_block4_2_relu/Relu;resnet152/conv4_block4_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block4_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block4_2_conv/Conv2D]:64
	                 CONV_2D	         8602.719	  109.890	  110.304	  0.423%	 33.402%	     0.000	        1	[resnet152/conv4_block4_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block4_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block4_3_conv/Conv2D]:65
	                     ADD	         8713.034	   19.128	   19.124	  0.073%	 33.475%	     0.000	        1	[resnet152/conv4_block4_out/Relu;resnet152/conv4_block4_add/add]:66
	                 CONV_2D	         8732.168	  108.136	  108.779	  0.417%	 33.892%	     0.000	        1	[resnet152/conv4_block5_1_relu/Relu;resnet152/conv4_block5_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block5_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block5_1_conv/Conv2D]:67
	                 CONV_2D	         8840.958	  251.368	  246.923	  0.947%	 34.839%	     0.000	        1	[resnet152/conv4_block5_2_relu/Relu;resnet152/conv4_block5_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block5_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block5_2_conv/Conv2D]:68
	                 CONV_2D	         9087.894	  112.763	  111.225	  0.426%	 35.265%	     0.000	        1	[resnet152/conv4_block5_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block5_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block5_3_conv/Conv2D]:69
	                     ADD	         9199.131	   19.318	   19.183	  0.074%	 35.339%	     0.000	        1	[resnet152/conv4_block5_out/Relu;resnet152/conv4_block5_add/add]:70
	                 CONV_2D	         9218.325	  112.788	  109.876	  0.421%	 35.760%	     0.000	        1	[resnet152/conv4_block6_1_relu/Relu;resnet152/conv4_block6_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block6_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block6_1_conv/Conv2D]:71
	                 CONV_2D	         9328.213	  253.467	  248.924	  0.954%	 36.715%	     0.000	        1	[resnet152/conv4_block6_2_relu/Relu;resnet152/conv4_block6_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block6_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block6_2_conv/Conv2D]:72
	                 CONV_2D	         9577.149	  110.957	  110.404	  0.423%	 37.138%	     0.000	        1	[resnet152/conv4_block6_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block6_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block6_3_conv/Conv2D]:73
	                     ADD	         9687.565	   19.286	   19.163	  0.073%	 37.211%	     0.000	        1	[resnet152/conv4_block6_out/Relu;resnet152/conv4_block6_add/add]:74
	                 CONV_2D	         9706.738	  110.358	  109.007	  0.418%	 37.629%	     0.000	        1	[resnet152/conv4_block7_1_relu/Relu;resnet152/conv4_block7_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block7_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block7_1_conv/Conv2D]:75
	                 CONV_2D	         9815.756	  253.718	  247.090	  0.947%	 38.576%	     0.000	        1	[resnet152/conv4_block7_2_relu/Relu;resnet152/conv4_block7_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block7_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block7_2_conv/Conv2D]:76
	                 CONV_2D	        10062.858	  110.929	  110.388	  0.423%	 39.000%	     0.000	        1	[resnet152/conv4_block7_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block7_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block7_3_conv/Conv2D]:77
	                     ADD	        10173.257	   19.199	   19.101	  0.073%	 39.073%	     0.000	        1	[resnet152/conv4_block7_out/Relu;resnet152/conv4_block7_add/add]:78
	                 CONV_2D	        10192.368	  110.593	  108.945	  0.418%	 39.491%	     0.000	        1	[resnet152/conv4_block8_1_relu/Relu;resnet152/conv4_block8_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block8_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block8_1_conv/Conv2D]:79
	                 CONV_2D	        10301.324	  245.343	  249.225	  0.956%	 40.446%	     0.000	        1	[resnet152/conv4_block8_2_relu/Relu;resnet152/conv4_block8_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block8_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block8_2_conv/Conv2D]:80
	                 CONV_2D	        10550.561	  109.898	  110.881	  0.425%	 40.871%	     0.000	        1	[resnet152/conv4_block8_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block8_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block8_3_conv/Conv2D]:81
	                     ADD	        10661.453	   19.140	   19.171	  0.073%	 40.945%	     0.000	        1	[resnet152/conv4_block8_out/Relu;resnet152/conv4_block8_add/add]:82
	                 CONV_2D	        10680.633	  109.227	  109.439	  0.420%	 41.364%	     0.000	        1	[resnet152/conv4_block9_1_relu/Relu;resnet152/conv4_block9_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block9_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block9_1_conv/Conv2D]:83
	                 CONV_2D	        10790.084	  245.067	  248.318	  0.952%	 42.316%	     0.000	        1	[resnet152/conv4_block9_2_relu/Relu;resnet152/conv4_block9_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block9_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block9_2_conv/Conv2D]:84
	                 CONV_2D	        11038.413	  109.725	  110.487	  0.424%	 42.740%	     0.000	        1	[resnet152/conv4_block9_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block9_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block9_3_conv/Conv2D]:85
	                     ADD	        11148.912	   19.042	   19.167	  0.073%	 42.813%	     0.000	        1	[resnet152/conv4_block9_out/Relu;resnet152/conv4_block9_add/add]:86
	                 CONV_2D	        11168.089	  107.775	  108.826	  0.417%	 43.231%	     0.000	        1	[resnet152/conv4_block10_1_relu/Relu;resnet152/conv4_block10_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block10_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block10_1_conv/Conv2D]:87
	                 CONV_2D	        11276.927	  248.966	  246.835	  0.946%	 44.177%	     0.000	        1	[resnet152/conv4_block10_2_relu/Relu;resnet152/conv4_block10_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block10_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block10_2_conv/Conv2D]:88
	                 CONV_2D	        11523.773	  114.302	  111.354	  0.427%	 44.604%	     0.000	        1	[resnet152/conv4_block10_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block10_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_conv/Conv2D]:89
	                     ADD	        11635.140	   19.325	   19.207	  0.074%	 44.678%	     0.000	        1	[resnet152/conv4_block10_out/Relu;resnet152/conv4_block10_add/add]:90
	                 CONV_2D	        11654.359	  111.204	  109.485	  0.420%	 45.097%	     0.000	        1	[resnet152/conv4_block11_1_relu/Relu;resnet152/conv4_block11_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block11_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block11_1_conv/Conv2D]:91
	                 CONV_2D	        11763.855	  258.650	  249.624	  0.957%	 46.054%	     0.000	        1	[resnet152/conv4_block11_2_relu/Relu;resnet152/conv4_block11_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block11_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block11_2_conv/Conv2D]:92
	                 CONV_2D	        12013.491	  110.863	  110.504	  0.424%	 46.478%	     0.000	        1	[resnet152/conv4_block11_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block11_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block11_3_conv/Conv2D]:93
	                     ADD	        12124.006	   19.256	   19.157	  0.073%	 46.551%	     0.000	        1	[resnet152/conv4_block11_out/Relu;resnet152/conv4_block11_add/add]:94
	                 CONV_2D	        12143.174	  110.889	  109.090	  0.418%	 46.970%	     0.000	        1	[resnet152/conv4_block12_1_relu/Relu;resnet152/conv4_block12_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block12_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block12_1_conv/Conv2D]:95
	                 CONV_2D	        12252.275	  246.030	  245.469	  0.941%	 47.911%	     0.000	        1	[resnet152/conv4_block12_2_relu/Relu;resnet152/conv4_block12_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block12_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block12_2_conv/Conv2D]:96
	                 CONV_2D	        12497.756	  109.900	  109.948	  0.422%	 48.332%	     0.000	        1	[resnet152/conv4_block12_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block12_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block12_3_conv/Conv2D]:97
	                     ADD	        12607.715	   19.057	   19.178	  0.074%	 48.406%	     0.000	        1	[resnet152/conv4_block12_out/Relu;resnet152/conv4_block12_add/add]:98
	                 CONV_2D	        12626.904	  107.925	  108.107	  0.414%	 48.820%	     0.000	        1	[resnet152/conv4_block13_1_relu/Relu;resnet152/conv4_block13_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block13_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block13_1_conv/Conv2D]:99
	                 CONV_2D	        12735.021	  244.017	  249.112	  0.955%	 49.775%	     0.000	        1	[resnet152/conv4_block13_2_relu/Relu;resnet152/conv4_block13_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block13_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block13_2_conv/Conv2D]:100
	                 CONV_2D	        12984.144	  109.917	  110.860	  0.425%	 50.200%	     0.000	        1	[resnet152/conv4_block13_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block13_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block13_3_conv/Conv2D]:101
	                     ADD	        13095.015	   19.061	   19.170	  0.073%	 50.274%	     0.000	        1	[resnet152/conv4_block13_out/Relu;resnet152/conv4_block13_add/add]:102
	                 CONV_2D	        13114.195	  107.532	  109.153	  0.418%	 50.692%	     0.000	        1	[resnet152/conv4_block14_1_relu/Relu;resnet152/conv4_block14_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block14_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block14_1_conv/Conv2D]:103
	                 CONV_2D	        13223.359	  244.324	  248.050	  0.951%	 51.643%	     0.000	        1	[resnet152/conv4_block14_2_relu/Relu;resnet152/conv4_block14_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block14_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block14_2_conv/Conv2D]:104
	                 CONV_2D	        13471.426	  109.923	  110.397	  0.423%	 52.067%	     0.000	        1	[resnet152/conv4_block14_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block14_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block14_3_conv/Conv2D]:105
	                     ADD	        13581.834	   19.085	   19.097	  0.073%	 52.140%	     0.000	        1	[resnet152/conv4_block14_out/Relu;resnet152/conv4_block14_add/add]:106
	                 CONV_2D	        13600.940	  108.133	  107.795	  0.413%	 52.553%	     0.000	        1	[resnet152/conv4_block15_1_relu/Relu;resnet152/conv4_block15_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block15_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block15_1_conv/Conv2D]:107
	                 CONV_2D	        13708.746	  246.925	  245.297	  0.940%	 53.494%	     0.000	        1	[resnet152/conv4_block15_2_relu/Relu;resnet152/conv4_block15_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block15_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block15_2_conv/Conv2D]:108
	                 CONV_2D	        13954.054	  112.495	  110.914	  0.425%	 53.919%	     0.000	        1	[resnet152/conv4_block15_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block15_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block15_3_conv/Conv2D]:109
	                     ADD	        14064.980	   19.317	   19.179	  0.074%	 53.992%	     0.000	        1	[resnet152/conv4_block15_out/Relu;resnet152/conv4_block15_add/add]:110
	                 CONV_2D	        14084.169	  111.631	  109.795	  0.421%	 54.413%	     0.000	        1	[resnet152/conv4_block16_1_relu/Relu;resnet152/conv4_block16_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block16_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block16_1_conv/Conv2D]:111
	                 CONV_2D	        14193.975	  252.459	  248.534	  0.953%	 55.366%	     0.000	        1	[resnet152/conv4_block16_2_relu/Relu;resnet152/conv4_block16_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block16_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block16_2_conv/Conv2D]:112
	                 CONV_2D	        14442.520	  110.910	  110.555	  0.424%	 55.790%	     0.000	        1	[resnet152/conv4_block16_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block16_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block16_3_conv/Conv2D]:113
	                     ADD	        14553.091	   19.259	   19.153	  0.073%	 55.863%	     0.000	        1	[resnet152/conv4_block16_out/Relu;resnet152/conv4_block16_add/add]:114
	                 CONV_2D	        14572.254	  110.264	  109.254	  0.419%	 56.282%	     0.000	        1	[resnet152/conv4_block17_1_relu/Relu;resnet152/conv4_block17_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block17_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block17_1_conv/Conv2D]:115
	                 CONV_2D	        14681.519	  247.738	  246.494	  0.945%	 57.227%	     0.000	        1	[resnet152/conv4_block17_2_relu/Relu;resnet152/conv4_block17_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block17_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block17_2_conv/Conv2D]:116
	                 CONV_2D	        14928.024	  109.785	  109.937	  0.421%	 57.649%	     0.000	        1	[resnet152/conv4_block17_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block17_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block17_3_conv/Conv2D]:117
	                     ADD	        15037.971	   19.061	   19.086	  0.073%	 57.722%	     0.000	        1	[resnet152/conv4_block17_out/Relu;resnet152/conv4_block17_add/add]:118
	                 CONV_2D	        15057.067	  107.480	  108.004	  0.414%	 58.136%	     0.000	        1	[resnet152/conv4_block18_1_relu/Relu;resnet152/conv4_block18_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block18_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block18_1_conv/Conv2D]:119
	                 CONV_2D	        15165.083	  244.216	  247.859	  0.950%	 59.086%	     0.000	        1	[resnet152/conv4_block18_2_relu/Relu;resnet152/conv4_block18_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block18_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block18_2_conv/Conv2D]:120
	                 CONV_2D	        15412.953	  110.117	  111.387	  0.427%	 59.513%	     0.000	        1	[resnet152/conv4_block18_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block18_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block18_3_conv/Conv2D]:121
	                     ADD	        15524.351	   19.112	   19.326	  0.074%	 59.588%	     0.000	        1	[resnet152/conv4_block18_out/Relu;resnet152/conv4_block18_add/add]:122
	                 CONV_2D	        15543.688	  107.479	  110.175	  0.422%	 60.010%	     0.000	        1	[resnet152/conv4_block19_1_relu/Relu;resnet152/conv4_block19_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block19_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block19_1_conv/Conv2D]:123
	                 CONV_2D	        15653.874	  243.967	  248.251	  0.952%	 60.962%	     0.000	        1	[resnet152/conv4_block19_2_relu/Relu;resnet152/conv4_block19_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block19_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block19_2_conv/Conv2D]:124
	                 CONV_2D	        15902.136	  109.979	  110.481	  0.424%	 61.385%	     0.000	        1	[resnet152/conv4_block19_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block19_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block19_3_conv/Conv2D]:125
	                     ADD	        16012.630	   19.032	   19.115	  0.073%	 61.459%	     0.000	        1	[resnet152/conv4_block19_out/Relu;resnet152/conv4_block19_add/add]:126
	                 CONV_2D	        16031.755	  107.823	  108.748	  0.417%	 61.875%	     0.000	        1	[resnet152/conv4_block20_1_relu/Relu;resnet152/conv4_block20_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block20_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block20_1_conv/Conv2D]:127
	                 CONV_2D	        16140.514	  245.880	  247.178	  0.948%	 62.823%	     0.000	        1	[resnet152/conv4_block20_2_relu/Relu;resnet152/conv4_block20_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block20_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block20_2_conv/Conv2D]:128
	                 CONV_2D	        16387.704	  111.303	  110.419	  0.423%	 63.246%	     0.000	        1	[resnet152/conv4_block20_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block20_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block20_3_conv/Conv2D]:129
	                     ADD	        16498.135	   19.299	   19.166	  0.073%	 63.320%	     0.000	        1	[resnet152/conv4_block20_out/Relu;resnet152/conv4_block20_add/add]:130
	                 CONV_2D	        16517.311	  111.622	  109.647	  0.420%	 63.740%	     0.000	        1	[resnet152/conv4_block21_1_relu/Relu;resnet152/conv4_block21_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block21_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block21_1_conv/Conv2D]:131
	                 CONV_2D	        16626.976	  253.490	  249.246	  0.956%	 64.696%	     0.000	        1	[resnet152/conv4_block21_2_relu/Relu;resnet152/conv4_block21_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block21_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block21_2_conv/Conv2D]:132
	                 CONV_2D	        16876.234	  111.029	  110.417	  0.423%	 65.119%	     0.000	        1	[resnet152/conv4_block21_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block21_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block21_3_conv/Conv2D]:133
	                     ADD	        16986.666	   19.167	   19.127	  0.073%	 65.193%	     0.000	        1	[resnet152/conv4_block21_out/Relu;resnet152/conv4_block21_add/add]:134
	                 CONV_2D	        17005.804	  107.634	  108.575	  0.416%	 65.609%	     0.000	        1	[resnet152/conv4_block22_1_relu/Relu;resnet152/conv4_block22_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block22_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block22_1_conv/Conv2D]:135
	                 CONV_2D	        17114.391	  244.241	  246.895	  0.947%	 66.555%	     0.000	        1	[resnet152/conv4_block22_2_relu/Relu;resnet152/conv4_block22_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block22_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block22_2_conv/Conv2D]:136
	                 CONV_2D	        17361.298	  109.895	  110.347	  0.423%	 66.978%	     0.000	        1	[resnet152/conv4_block22_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block22_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block22_3_conv/Conv2D]:137
	                     ADD	        17471.655	   19.094	   19.130	  0.073%	 67.052%	     0.000	        1	[resnet152/conv4_block22_out/Relu;resnet152/conv4_block22_add/add]:138
	                 CONV_2D	        17490.795	  107.974	  108.274	  0.415%	 67.467%	     0.000	        1	[resnet152/conv4_block23_1_relu/Relu;resnet152/conv4_block23_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block23_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block23_1_conv/Conv2D]:139
	                 CONV_2D	        17599.085	  244.609	  247.094	  0.947%	 68.414%	     0.000	        1	[resnet152/conv4_block23_2_relu/Relu;resnet152/conv4_block23_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block23_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block23_2_conv/Conv2D]:140
	                 CONV_2D	        17846.191	  109.959	  111.011	  0.426%	 68.840%	     0.000	        1	[resnet152/conv4_block23_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block23_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block23_3_conv/Conv2D]:141
	                     ADD	        17957.217	   19.052	   19.177	  0.074%	 68.913%	     0.000	        1	[resnet152/conv4_block23_out/Relu;resnet152/conv4_block23_add/add]:142
	                 CONV_2D	        17976.405	  107.903	  110.082	  0.422%	 69.335%	     0.000	        1	[resnet152/conv4_block24_1_relu/Relu;resnet152/conv4_block24_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block24_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block24_1_conv/Conv2D]:143
	                 CONV_2D	        18086.498	  243.894	  248.109	  0.951%	 70.287%	     0.000	        1	[resnet152/conv4_block24_2_relu/Relu;resnet152/conv4_block24_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block24_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block24_2_conv/Conv2D]:144
	                 CONV_2D	        18334.620	  109.946	  110.522	  0.424%	 70.710%	     0.000	        1	[resnet152/conv4_block24_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block24_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block24_3_conv/Conv2D]:145
	                     ADD	        18445.154	   19.064	   19.148	  0.073%	 70.784%	     0.000	        1	[resnet152/conv4_block24_out/Relu;resnet152/conv4_block24_add/add]:146
	                 CONV_2D	        18464.311	  108.219	  108.934	  0.418%	 71.201%	     0.000	        1	[resnet152/conv4_block25_1_relu/Relu;resnet152/conv4_block25_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block25_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block25_1_conv/Conv2D]:147
	                 CONV_2D	        18573.258	  246.532	  245.084	  0.940%	 72.141%	     0.000	        1	[resnet152/conv4_block25_2_relu/Relu;resnet152/conv4_block25_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block25_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block25_2_conv/Conv2D]:148
	                 CONV_2D	        18818.353	  110.410	  110.119	  0.422%	 72.563%	     0.000	        1	[resnet152/conv4_block25_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block25_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block25_3_conv/Conv2D]:149
	                     ADD	        18928.483	   19.354	   19.165	  0.073%	 72.637%	     0.000	        1	[resnet152/conv4_block25_out/Relu;resnet152/conv4_block25_add/add]:150
	                 CONV_2D	        18947.659	  112.378	  109.467	  0.420%	 73.056%	     0.000	        1	[resnet152/conv4_block26_1_relu/Relu;resnet152/conv4_block26_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block26_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block26_1_conv/Conv2D]:151
	                 CONV_2D	        19057.138	  253.889	  249.940	  0.958%	 74.015%	     0.000	        1	[resnet152/conv4_block26_2_relu/Relu;resnet152/conv4_block26_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block26_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block26_2_conv/Conv2D]:152
	                 CONV_2D	        19307.089	  111.186	  110.605	  0.424%	 74.439%	     0.000	        1	[resnet152/conv4_block26_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block26_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block26_3_conv/Conv2D]:153
	                     ADD	        19417.706	   19.232	   19.151	  0.073%	 74.512%	     0.000	        1	[resnet152/conv4_block26_out/Relu;resnet152/conv4_block26_add/add]:154
	                 CONV_2D	        19436.867	  110.314	  108.963	  0.418%	 74.930%	     0.000	        1	[resnet152/conv4_block27_1_relu/Relu;resnet152/conv4_block27_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block27_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block27_1_conv/Conv2D]:155
	                 CONV_2D	        19545.841	  249.511	  248.075	  0.951%	 75.881%	     0.000	        1	[resnet152/conv4_block27_2_relu/Relu;resnet152/conv4_block27_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block27_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block27_2_conv/Conv2D]:156
	                 CONV_2D	        19793.929	  110.054	  110.256	  0.423%	 76.304%	     0.000	        1	[resnet152/conv4_block27_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block27_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block27_3_conv/Conv2D]:157
	                     ADD	        19904.196	   19.117	   19.127	  0.073%	 76.377%	     0.000	        1	[resnet152/conv4_block27_out/Relu;resnet152/conv4_block27_add/add]:158
	                 CONV_2D	        19923.333	  107.944	  108.283	  0.415%	 76.792%	     0.000	        1	[resnet152/conv4_block28_1_relu/Relu;resnet152/conv4_block28_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block28_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block28_1_conv/Conv2D]:159
	                 CONV_2D	        20031.628	  244.341	  246.894	  0.947%	 77.739%	     0.000	        1	[resnet152/conv4_block28_2_relu/Relu;resnet152/conv4_block28_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block28_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block28_2_conv/Conv2D]:160
	                 CONV_2D	        20278.534	  110.170	  111.456	  0.427%	 78.166%	     0.000	        1	[resnet152/conv4_block28_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block28_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block28_3_conv/Conv2D]:161
	                     ADD	        20390.001	   19.084	   19.208	  0.074%	 78.240%	     0.000	        1	[resnet152/conv4_block28_out/Relu;resnet152/conv4_block28_add/add]:162
	                 CONV_2D	        20409.219	  107.458	  109.531	  0.420%	 78.660%	     0.000	        1	[resnet152/conv4_block29_1_relu/Relu;resnet152/conv4_block29_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block29_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block29_1_conv/Conv2D]:163
	                 CONV_2D	        20518.762	  244.901	  248.630	  0.953%	 79.613%	     0.000	        1	[resnet152/conv4_block29_2_relu/Relu;resnet152/conv4_block29_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block29_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block29_2_conv/Conv2D]:164
	                 CONV_2D	        20767.402	  110.154	  110.657	  0.424%	 80.037%	     0.000	        1	[resnet152/conv4_block29_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block29_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block29_3_conv/Conv2D]:165
	                     ADD	        20878.071	   19.097	   19.162	  0.073%	 80.111%	     0.000	        1	[resnet152/conv4_block29_out/Relu;resnet152/conv4_block29_add/add]:166
	                 CONV_2D	        20897.244	  108.186	  109.065	  0.418%	 80.529%	     0.000	        1	[resnet152/conv4_block30_1_relu/Relu;resnet152/conv4_block30_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block30_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block30_1_conv/Conv2D]:167
	                 CONV_2D	        21006.320	  244.239	  245.673	  0.942%	 81.471%	     0.000	        1	[resnet152/conv4_block30_2_relu/Relu;resnet152/conv4_block30_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block30_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block30_2_conv/Conv2D]:168
	                 CONV_2D	        21252.003	  110.520	  110.265	  0.423%	 81.893%	     0.000	        1	[resnet152/conv4_block30_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block30_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block30_3_conv/Conv2D]:169
	                     ADD	        21362.280	   19.243	   19.113	  0.073%	 81.967%	     0.000	        1	[resnet152/conv4_block30_out/Relu;resnet152/conv4_block30_add/add]:170
	                 CONV_2D	        21381.402	  110.550	  108.612	  0.416%	 82.383%	     0.000	        1	[resnet152/conv4_block31_1_relu/Relu;resnet152/conv4_block31_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block31_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block31_1_conv/Conv2D]:171
	                 CONV_2D	        21490.026	  255.540	  249.508	  0.957%	 83.340%	     0.000	        1	[resnet152/conv4_block31_2_relu/Relu;resnet152/conv4_block31_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block31_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block31_2_conv/Conv2D]:172
	                 CONV_2D	        21739.546	  111.376	  110.974	  0.425%	 83.765%	     0.000	        1	[resnet152/conv4_block31_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block31_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block31_3_conv/Conv2D]:173
	                     ADD	        21850.531	   19.211	   19.173	  0.074%	 83.839%	     0.000	        1	[resnet152/conv4_block31_out/Relu;resnet152/conv4_block31_add/add]:174
	                 CONV_2D	        21869.713	  110.640	  109.157	  0.418%	 84.257%	     0.000	        1	[resnet152/conv4_block32_1_relu/Relu;resnet152/conv4_block32_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block32_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block32_1_conv/Conv2D]:175
	                 CONV_2D	        21978.881	  253.079	  248.240	  0.952%	 85.209%	     0.000	        1	[resnet152/conv4_block32_2_relu/Relu;resnet152/conv4_block32_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block32_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block32_2_conv/Conv2D]:176
	                 CONV_2D	        22227.134	  111.158	  110.272	  0.423%	 85.632%	     0.000	        1	[resnet152/conv4_block32_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block32_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block32_3_conv/Conv2D]:177
	                     ADD	        22337.417	   19.230	   19.110	  0.073%	 85.705%	     0.000	        1	[resnet152/conv4_block32_out/Relu;resnet152/conv4_block32_add/add]:178
	                 CONV_2D	        22356.537	  109.889	  108.094	  0.414%	 86.119%	     0.000	        1	[resnet152/conv4_block33_1_relu/Relu;resnet152/conv4_block33_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block33_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block33_1_conv/Conv2D]:179
	                 CONV_2D	        22464.643	  244.580	  245.724	  0.942%	 87.061%	     0.000	        1	[resnet152/conv4_block33_2_relu/Relu;resnet152/conv4_block33_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block33_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block33_2_conv/Conv2D]:180
	                 CONV_2D	        22710.379	  110.145	  110.852	  0.425%	 87.486%	     0.000	        1	[resnet152/conv4_block33_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block33_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block33_3_conv/Conv2D]:181
	                     ADD	        22821.242	   19.234	   19.561	  0.075%	 87.561%	     0.000	        1	[resnet152/conv4_block33_out/Relu;resnet152/conv4_block33_add/add]:182
	                 CONV_2D	        22840.816	  107.964	  109.885	  0.421%	 87.983%	     0.000	        1	[resnet152/conv4_block34_1_relu/Relu;resnet152/conv4_block34_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block34_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block34_1_conv/Conv2D]:183
	                 CONV_2D	        22950.714	  244.371	  248.707	  0.954%	 88.936%	     0.000	        1	[resnet152/conv4_block34_2_relu/Relu;resnet152/conv4_block34_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block34_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block34_2_conv/Conv2D]:184
	                 CONV_2D	        23199.439	  110.046	  110.473	  0.424%	 89.360%	     0.000	        1	[resnet152/conv4_block34_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block34_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block34_3_conv/Conv2D]:185
	                     ADD	        23309.924	   19.076	   19.138	  0.073%	 89.433%	     0.000	        1	[resnet152/conv4_block34_out/Relu;resnet152/conv4_block34_add/add]:186
	                 CONV_2D	        23329.071	  107.541	  109.069	  0.418%	 89.851%	     0.000	        1	[resnet152/conv4_block35_1_relu/Relu;resnet152/conv4_block35_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block35_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block35_1_conv/Conv2D]:187
	                 CONV_2D	        23438.151	  245.072	  246.739	  0.946%	 90.797%	     0.000	        1	[resnet152/conv4_block35_2_relu/Relu;resnet152/conv4_block35_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block35_2_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block35_2_conv/Conv2D]:188
	                 CONV_2D	        23684.904	  110.446	  110.225	  0.423%	 91.220%	     0.000	        1	[resnet152/conv4_block35_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block35_3_conv/BiasAdd;resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block35_3_conv/Conv2D]:189
	                     ADD	        23795.140	   19.124	   19.147	  0.073%	 91.293%	     0.000	        1	[resnet152/conv4_block35_out/Relu;resnet152/conv4_block35_add/add]:190
	                 CONV_2D	        23814.297	  109.162	  108.159	  0.415%	 91.708%	     0.000	        1	[resnet152/conv4_block36_1_relu/Relu;resnet152/conv4_block36_1_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block36_1_conv/BiasAdd;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv4_block36_1_conv/Conv2D]:191
	                 CONV_2D	        23922.467	  254.738	  249.468	  0.956%	 92.664%	     0.000	        1	[resnet152/conv4_block36_2_relu/Relu;resnet152/conv4_block36_2_bn/FusedBatchNormV3;resnet152/conv2_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block36_2_conv/BiasAdd;resnet152/conv4_block36_2_conv/Conv2D]:192
	                 CONV_2D	        24171.948	  112.748	  111.041	  0.426%	 93.090%	     0.000	        1	[resnet152/conv4_block36_3_bn/FusedBatchNormV3;resnet152/conv4_block10_3_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv4_block36_3_conv/BiasAdd;resnet152/conv4_block36_3_conv/Conv2D]:193
	                     ADD	        24283.000	   19.230	   19.174	  0.074%	 93.164%	     0.000	        1	[resnet152/conv4_block36_out/Relu;resnet152/conv4_block36_add/add]:194
	                 CONV_2D	        24302.185	  237.054	  235.746	  0.904%	 94.067%	     0.000	        1	[resnet152/conv5_block1_0_bn/FusedBatchNormV3;resnet152/conv5_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block1_0_conv/BiasAdd;resnet152/conv5_block3_3_bn/FusedBatchNormV3;resnet152/conv5_block1_0_conv/Conv2D]:195
	                 CONV_2D	        24537.945	   59.954	   59.627	  0.229%	 94.296%	     0.000	        1	[resnet152/conv5_block1_1_relu/Relu;resnet152/conv5_block1_1_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block1_1_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block1_1_conv/Conv2D]:196
	                 CONV_2D	        24597.582	  270.838	  271.122	  1.039%	 95.336%	     0.000	        1	[resnet152/conv5_block1_2_relu/Relu;resnet152/conv5_block1_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block1_2_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block1_2_conv/Conv2D]:197
	                 CONV_2D	        24868.715	  117.579	  117.800	  0.452%	 95.787%	     0.000	        1	[resnet152/conv5_block1_3_bn/FusedBatchNormV3;resnet152/conv5_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block1_3_conv/BiasAdd;resnet152/conv5_block3_3_bn/FusedBatchNormV3;resnet152/conv5_block1_3_conv/Conv2D]:198
	                     ADD	        24986.527	    9.572	    9.598	  0.037%	 95.824%	     0.000	        1	[resnet152/conv5_block1_out/Relu;resnet152/conv5_block1_add/add]:199
	                 CONV_2D	        24996.135	  118.753	  119.095	  0.457%	 96.281%	     0.000	        1	[resnet152/conv5_block2_1_relu/Relu;resnet152/conv5_block2_1_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block2_1_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block2_1_conv/Conv2D]:200
	                 CONV_2D	        25115.242	  267.776	  272.296	  1.044%	 97.325%	     0.000	        1	[resnet152/conv5_block2_2_relu/Relu;resnet152/conv5_block2_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block2_2_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block2_2_conv/Conv2D]:201
	                 CONV_2D	        25387.553	  117.621	  118.356	  0.454%	 97.778%	     0.000	        1	[resnet152/conv5_block2_3_bn/FusedBatchNormV3;resnet152/conv5_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block2_3_conv/BiasAdd;resnet152/conv5_block3_3_bn/FusedBatchNormV3;resnet152/conv5_block2_3_conv/Conv2D]:202
	                     ADD	        25505.919	    9.561	    9.608	  0.037%	 97.815%	     0.000	        1	[resnet152/conv5_block2_out/Relu;resnet152/conv5_block2_add/add]:203
	                 CONV_2D	        25515.536	  118.501	  120.078	  0.460%	 98.275%	     0.000	        1	[resnet152/conv5_block3_1_relu/Relu;resnet152/conv5_block3_1_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block3_1_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block3_1_conv/Conv2D]:204
	                 CONV_2D	        25635.626	  268.647	  271.536	  1.041%	 99.317%	     0.000	        1	[resnet152/conv5_block3_2_relu/Relu;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block3_2_conv/BiasAdd;resnet152/conv5_block3_2_conv/Conv2D]:205
	                 CONV_2D	        25907.176	  117.472	  117.678	  0.451%	 99.768%	     0.000	        1	[resnet152/conv5_block3_3_bn/FusedBatchNormV3;resnet152/conv5_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block3_3_conv/BiasAdd;resnet152/conv5_block3_3_conv/Conv2D]:206
	                     ADD	        26024.865	    9.562	    9.576	  0.037%	 99.804%	     0.000	        1	[resnet152/conv5_block3_out/Relu;resnet152/conv5_block3_add/add]:207
	                    MEAN	        26034.448	   17.267	   17.156	  0.066%	 99.870%	     0.000	        1	[resnet152/avg_pool/Mean]:208
	         FULLY_CONNECTED	        26051.614	   33.745	   33.770	  0.129%	100.000%	     0.000	        1	[resnet152/predictions/MatMul;resnet152/predictions/BiasAdd]:209
	                 SOFTMAX	        26085.396	    0.099	    0.095	  0.000%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:210

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            3.765	  327.597	  328.628	  1.260%	  1.260%	     0.000	        1	[resnet152/conv1_relu/Relu;resnet152/conv1_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv1_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv1_conv/Conv2D]:1
	                 CONV_2D	         1768.331	  283.199	  280.348	  1.075%	  2.335%	     0.000	        1	[resnet152/conv2_block3_2_relu/Relu;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block3_2_conv/BiasAdd;resnet152/conv2_block3_2_conv/Conv2D]:14
	                 CONV_2D	          532.378	  278.635	  279.856	  1.073%	  3.408%	     0.000	        1	[resnet152/conv2_block1_2_relu/Relu;resnet152/conv2_block1_2_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block1_2_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block1_2_conv/Conv2D]:6
	                 CONV_2D	         1151.460	  276.124	  277.409	  1.064%	  4.471%	     0.000	        1	[resnet152/conv2_block2_2_relu/Relu;resnet152/conv2_block2_2_bn/FusedBatchNormV3;resnet152/conv1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv2_block2_2_conv/BiasAdd;resnet152/conv2_block3_2_bn/FusedBatchNormV3;resnet152/conv2_block2_2_conv/Conv2D]:10
	                 CONV_2D	        25115.242	  267.776	  272.296	  1.044%	  5.515%	     0.000	        1	[resnet152/conv5_block2_2_relu/Relu;resnet152/conv5_block2_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block2_2_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block2_2_conv/Conv2D]:201
	                 CONV_2D	        25635.626	  268.647	  271.536	  1.041%	  6.556%	     0.000	        1	[resnet152/conv5_block3_2_relu/Relu;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block3_2_conv/BiasAdd;resnet152/conv5_block3_2_conv/Conv2D]:205
	                 CONV_2D	        24597.582	  270.838	  271.122	  1.039%	  7.596%	     0.000	        1	[resnet152/conv5_block1_2_relu/Relu;resnet152/conv5_block1_2_bn/FusedBatchNormV3;resnet152/conv3_block1_0_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv5_block1_2_conv/BiasAdd;resnet152/conv5_block3_2_bn/FusedBatchNormV3;resnet152/conv5_block1_2_conv/Conv2D]:197
	                 CONV_2D	         4114.068	  261.308	  257.656	  0.988%	  8.584%	     0.000	        1	[resnet152/conv3_block4_2_relu/Relu;resnet152/conv3_block4_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block4_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block4_2_conv/Conv2D]:31
	                 CONV_2D	         3064.357	  254.164	  257.046	  0.985%	  9.569%	     0.000	        1	[resnet152/conv3_block2_2_relu/Relu;resnet152/conv3_block2_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block2_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block2_2_conv/Conv2D]:23
	                 CONV_2D	         5690.123	  254.280	  256.319	  0.983%	 10.552%	     0.000	        1	[resnet152/conv3_block7_2_relu/Relu;resnet152/conv3_block7_2_bn/FusedBatchNormV3;resnet152/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152/conv3_block7_2_conv/BiasAdd;resnet152/conv3_block8_2_bn/FusedBatchNormV3;resnet152/conv3_block7_2_conv/Conv2D]:43

Number of nodes executed: 211
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	      155	 24749.158	    94.886%	    94.886%	     0.000	      155
	                     ADD	       50	  1255.536	     4.814%	    99.700%	     0.000	       50
	         FULLY_CONNECTED	        1	    33.770	     0.129%	    99.830%	     0.000	        1
	                     PAD	        2	    21.969	     0.084%	    99.914%	     0.000	        2
	                    MEAN	        1	    17.155	     0.066%	    99.980%	     0.000	        1
	             MAX_POOL_2D	        1	     5.249	     0.020%	   100.000%	     0.000	        1
	                 SOFTMAX	        1	     0.095	     0.000%	   100.000%	     0.000	        1

Timings (microseconds): count=6 first=26086766 curr=26105953 min=26062198 max=26105953 avg=2.6083e+07 std=14602
Memory (bytes): count=0
211 nodes observed



munmap_chunk(): invalid pointer
[ perf record: Woken up 986 times to write data ]
[ perf record: Captured and wrote 246.993 MB /tmp/data.record (1255501 samples) ]

317.347

