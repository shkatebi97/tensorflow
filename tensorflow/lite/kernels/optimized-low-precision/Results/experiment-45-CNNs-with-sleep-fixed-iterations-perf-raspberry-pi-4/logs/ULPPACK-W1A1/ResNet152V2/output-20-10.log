STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/ResNet152V2.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/ResNet152V2.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 160)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 160)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
1
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (64, 64, ), Input shape (3136, 64, ), and Output shape (3136, 64, ), and the ID is 2
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3364, 64, ), and Output shape (3136, 64, ), and the ID is 3
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (64, 256, ), Input shape (3136, 256, ), and Output shape (3136, 64, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 256)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 256)
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3364, 64, ), and Output shape (3136, 64, ), and the ID is 6
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (64, 256, ), Input shape (3136, 256, ), and Output shape (3136, 64, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 256)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3364, 64, ), and Output shape (784, 64, ), and the ID is 9
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (784, 64, ), and Output shape (784, 256, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (512, 256, ), Input shape (784, 256, ), and Output shape (784, 512, ), and the ID is 11	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 256)

	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 14	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 15
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 20
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 22
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 24	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 25
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 26
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
27
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 28	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 31
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 32
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 33
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (196, 128, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (196, 128, ), and Output shape (196, 512, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
	Allocating LowPrecision Activations Tensors with Shape of (200, 128)
Applying Conv Low-Precision for Kernel shape (1024, 512, ), Input shape (196, 512, ), and Output shape (196, 1024, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
36
	Allocating LowPrecision Weight Tensors with Shape of (1024, 512)
	Allocating LowPrecision Activations Tensors with Shape of (200, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (196, 512, ), and Output shape (196, 256, ), and the ID is 37
	Allocating LowPrecision Weight Tensors with Shape of (256, 512)
	Allocating LowPrecision Activations Tensors with Shape of (200, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 38
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 39
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 40
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 41
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 42
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 43
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 44
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 45
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
46
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 48
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 50
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 51
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 52
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 53
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 54
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 55	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 56
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 57
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 58
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 59
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 60	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 61	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 62
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 63
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 64	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 65
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 66
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
, and the ID is 67
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 68
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 69
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 70
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 71
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 72
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 73
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 74
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 75
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 76
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
77
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 78
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 79
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 80
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 81
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 82
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 83
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 84
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 85
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 86
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 87
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 88
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 89
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 90
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 92
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 93
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 95
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 96
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 97
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 98
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 99
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 100
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 101
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 102
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 103	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 104
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 105
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 106
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 107
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 108
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 109
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 110
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 111
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
112
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 113
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 114
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 115
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 117
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 118
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 119
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 120
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 121
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 122
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 123
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 124	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 125
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 126
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 127
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 128
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 129
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 130
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 131
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 132
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 133
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 134
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 135
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 136
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 137	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 138
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 140
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (200, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 141
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 142
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (49, 256, ), and the ID is 143
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (56, 2304)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (49, 256, ), and Output shape (49, 1024, ), and the ID is 144
	Allocating LowPrecision Weight Tensors with Shape of (1024, 256)
	Allocating LowPrecision Activations Tensors with Shape of (56, 256)
Applying Conv Low-Precision for Kernel shape (2048, 1024, ), Input shape (49, 1024, ), and Output shape (49, 2048, ), and the ID is 145
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 1024, ), Input shape (49, 1024, ), and Output shape (49, 512, ), and the ID is 146
	Allocating LowPrecision Weight Tensors with Shape of (512, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1024)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (81, 512, ), and Output shape (49, 512, ), and the ID is 147
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (56, 4608)
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 148
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 512)
	Allocating LowPrecision Activations Tensors with Shape of (56, 512)
Applying Conv Low-Precision for Kernel shape (512, 2048, ), Input shape (49, 2048, ), and Output shape (49, 512, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 2048)
	Allocating LowPrecision Activations Tensors with Shape of (56, 2048)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (81, 512, ), and Output shape (49, 512, ), and the ID is 150
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (56, 4608)
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 151
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 512)
	Allocating LowPrecision Activations Tensors with Shape of (56, 512)
Applying Conv Low-Precision for Kernel shape (512, 2048, ), Input shape (49, 2048, ), and Output shape (49, 512, ), and the ID is 152	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (512, 2048)
	Allocating LowPrecision Activations Tensors with Shape of (56, 2048)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (81, 512, ), and Output shape (49, 512, ), and the ID is 153
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (56, 4608)
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 154
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 512)
	Allocating LowPrecision Activations Tensors with Shape of (56, 512)
Applying Low-Precision for shape (1000, 2048, ) and Input shape (1, 2048, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 2048)
	Transformed Activation Shape From: (1, 2048) To: (8, 2048)
The input model file size (MB): 62.0293
Initialized session in 994.101ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=6 first=27251116 curr=27228915 min=27228915 max=27261674 avg=2.72433e+07 std=10898

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=6 first=27232937 curr=27211308 min=27209329 max=27240979 avg=2.72218e+07 std=11816

Inference timings in us: Init: 994101, First inference: 27251116, Warmup (avg): 2.72433e+07, Inference (avg): 2.72218e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=122.262 overall=128.668
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  946.741	  946.741	100.000%	100.000%	108972.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  946.741	  946.741	100.000%	100.000%	108972.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   946.741	   100.000%	   100.000%	108972.000	        1

Timings (microseconds): count=1 curr=946741
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.026	    3.738	    3.733	  0.014%	  0.014%	     0.000	        1	[resnet152v2/conv1_pad/Pad]:0
	                 CONV_2D	            3.768	  333.146	  333.209	  1.224%	  1.238%	     0.000	        1	[resnet152v2/conv1_conv/BiasAdd;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv1_conv/Conv2D]:1
	                     PAD	          336.988	   18.337	   18.310	  0.067%	  1.305%	     0.000	        1	[resnet152v2/pool1_pad/Pad]:2
	             MAX_POOL_2D	          355.311	    5.295	    5.316	  0.020%	  1.325%	     0.000	        1	[resnet152v2/pool1_pool/MaxPool]:3
	                     MUL	          360.638	   14.879	   14.763	  0.054%	  1.379%	     0.000	        1	[resnet152v2/conv2_block1_preact_bn/FusedBatchNormV3]:4
	                     ADD	          375.411	   19.105	   19.173	  0.070%	  1.449%	     0.000	        1	[resnet152v2/conv2_block1_preact_relu/Relu;resnet152v2/conv2_block1_preact_bn/FusedBatchNormV3]:5
	                 CONV_2D	          394.595	  139.131	  140.423	  0.516%	  1.965%	     0.000	        1	[resnet152v2/conv2_block1_0_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_0_conv/Conv2D]:6
	                 CONV_2D	          535.030	   37.803	   38.086	  0.140%	  2.105%	     0.000	        1	[resnet152v2/conv2_block1_1_relu/Relu;resnet152v2/conv2_block1_1_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_1_conv/Conv2D]:7
	                     PAD	          573.128	    4.692	    4.728	  0.017%	  2.123%	     0.000	        1	[resnet152v2/conv2_block1_2_pad/Pad]:8
	                 CONV_2D	          577.866	  281.363	  281.458	  1.034%	  3.157%	     0.000	        1	[resnet152v2/conv2_block1_2_relu/Relu;resnet152v2/conv2_block1_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_2_conv/Conv2D]:9
	                 CONV_2D	          859.336	  135.732	  137.830	  0.506%	  3.663%	     0.000	        1	[resnet152v2/conv2_block1_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_3_conv/Conv2D]:10
	                     ADD	          997.178	   72.873	   73.146	  0.269%	  3.932%	     0.000	        1	[resnet152v2/conv2_block1_out/add]:11
	                     MUL	         1070.335	   56.703	   56.833	  0.209%	  4.141%	     0.000	        1	[resnet152v2/conv2_block2_preact_bn/FusedBatchNormV3]:12
	                     ADD	         1127.180	   74.845	   75.028	  0.276%	  4.416%	     0.000	        1	[resnet152v2/conv2_block2_preact_relu/Relu;resnet152v2/conv2_block2_preact_bn/FusedBatchNormV3]:13
	                 CONV_2D	         1202.221	  125.274	  125.163	  0.460%	  4.876%	     0.000	        1	[resnet152v2/conv2_block2_1_relu/Relu;resnet152v2/conv2_block2_1_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_1_conv/Conv2D]:14
	                     PAD	         1327.395	    4.662	    4.688	  0.017%	  4.893%	     0.000	        1	[resnet152v2/conv2_block2_2_pad/Pad]:15
	                 CONV_2D	         1332.092	  280.737	  280.950	  1.032%	  5.926%	     0.000	        1	[resnet152v2/conv2_block2_2_relu/Relu;resnet152v2/conv2_block2_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_2_conv/Conv2D]:16
	                 CONV_2D	         1613.055	  137.068	  136.803	  0.503%	  6.428%	     0.000	        1	[resnet152v2/conv2_block2_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_3_conv/Conv2D]:17
	                     ADD	         1749.871	   72.988	   72.947	  0.268%	  6.696%	     0.000	        1	[resnet152v2/conv2_block2_out/add]:18
	             MAX_POOL_2D	         1822.832	    1.610	    1.618	  0.006%	  6.702%	     0.000	        1	[resnet152v2/max_pooling2d_8/MaxPool]:19
	                     MUL	         1824.458	   56.616	   56.604	  0.208%	  6.910%	     0.000	        1	[resnet152v2/conv2_block3_preact_bn/FusedBatchNormV3]:20
	                     ADD	         1881.075	   74.931	   74.913	  0.275%	  7.185%	     0.000	        1	[resnet152v2/conv2_block3_preact_relu/Relu;resnet152v2/conv2_block3_preact_bn/FusedBatchNormV3]:21
	                 CONV_2D	         1956.001	  125.915	  124.600	  0.458%	  7.643%	     0.000	        1	[resnet152v2/conv2_block3_1_relu/Relu;resnet152v2/conv2_block3_1_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block3_1_conv/Conv2D]:22
	                     PAD	         2080.613	    4.677	    4.694	  0.017%	  7.660%	     0.000	        1	[resnet152v2/conv2_block3_2_pad/Pad]:23
	                 CONV_2D	         2085.316	   70.078	   68.522	  0.252%	  7.912%	     0.000	        1	[resnet152v2/conv2_block3_2_relu/Relu;resnet152v2/conv2_block3_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block3_2_conv/Conv2D]:24
	                 CONV_2D	         2153.850	   34.711	   34.195	  0.126%	  8.038%	     0.000	        1	[resnet152v2/conv2_block3_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block3_3_conv/Conv2D]:25
	                     ADD	         2188.068	   18.468	   18.361	  0.067%	  8.105%	     0.000	        1	[resnet152v2/conv2_block3_out/add]:26
	                     MUL	         2206.439	   14.459	   14.225	  0.052%	  8.158%	     0.000	        1	[resnet152v2/conv3_block1_preact_bn/FusedBatchNormV3]:27
	                     ADD	         2220.673	   18.992	   18.804	  0.069%	  8.227%	     0.000	        1	[resnet152v2/conv3_block1_preact_relu/Relu;resnet152v2/conv3_block1_preact_bn/FusedBatchNormV3]:28
	                 CONV_2D	         2239.487	  226.417	  221.609	  0.814%	  9.041%	     0.000	        1	[resnet152v2/conv3_block1_0_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block1_0_conv/Conv2D]:29
	                 CONV_2D	         2461.109	   59.319	   58.685	  0.216%	  9.256%	     0.000	        1	[resnet152v2/conv3_block1_1_relu/Relu;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block1_1_conv/Conv2D]:30
	                     PAD	         2519.805	    2.481	    2.474	  0.009%	  9.266%	     0.000	        1	[resnet152v2/conv3_block1_2_pad/Pad]:31
	                 CONV_2D	         2522.290	  258.835	  258.558	  0.950%	 10.216%	     0.000	        1	[resnet152v2/conv3_block1_2_relu/Relu;resnet152v2/conv3_block1_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block1_2_conv/Conv2D]:32
	                 CONV_2D	         2780.860	  119.329	  119.528	  0.439%	 10.655%	     0.000	        1	[resnet152v2/conv3_block1_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block1_3_conv/Conv2D]:33
	                     ADD	         2900.399	   36.784	   36.805	  0.135%	 10.790%	     0.000	        1	[resnet152v2/conv3_block1_out/add]:34
	                     MUL	         2937.215	   28.268	   28.500	  0.105%	 10.895%	     0.000	        1	[resnet152v2/conv3_block2_preact_bn/FusedBatchNormV3]:35
	                     ADD	         2965.727	   37.137	   37.580	  0.138%	 11.033%	     0.000	        1	[resnet152v2/conv3_block2_preact_relu/Relu;resnet152v2/conv3_block2_preact_bn/FusedBatchNormV3]:36
	                 CONV_2D	         3003.320	  110.913	  113.906	  0.418%	 11.451%	     0.000	        1	[resnet152v2/conv3_block2_1_relu/Relu;resnet152v2/conv3_block2_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block2_1_conv/Conv2D]:37
	                     PAD	         3117.241	    2.453	    2.462	  0.009%	 11.460%	     0.000	        1	[resnet152v2/conv3_block2_2_pad/Pad]:38
	                 CONV_2D	         3119.711	  254.909	  257.635	  0.947%	 12.407%	     0.000	        1	[resnet152v2/conv3_block2_2_relu/Relu;resnet152v2/conv3_block2_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block2_2_conv/Conv2D]:39
	                 CONV_2D	         3377.357	  117.785	  118.394	  0.435%	 12.842%	     0.000	        1	[resnet152v2/conv3_block2_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block2_3_conv/Conv2D]:40
	                     ADD	         3495.762	   36.482	   36.633	  0.135%	 12.976%	     0.000	        1	[resnet152v2/conv3_block2_out/add]:41
	                     MUL	         3532.407	   28.224	   28.303	  0.104%	 13.080%	     0.000	        1	[resnet152v2/conv3_block3_preact_bn/FusedBatchNormV3]:42
	                     ADD	         3560.720	   37.163	   37.271	  0.137%	 13.217%	     0.000	        1	[resnet152v2/conv3_block3_preact_relu/Relu;resnet152v2/conv3_block3_preact_bn/FusedBatchNormV3]:43
	                 CONV_2D	         3598.001	  110.802	  112.060	  0.412%	 13.629%	     0.000	        1	[resnet152v2/conv3_block3_1_relu/Relu;resnet152v2/conv3_block3_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block3_1_conv/Conv2D]:44
	                     PAD	         3710.072	    2.458	    2.461	  0.009%	 13.638%	     0.000	        1	[resnet152v2/conv3_block3_2_pad/Pad]:45
	                 CONV_2D	         3712.542	  255.501	  256.433	  0.942%	 14.580%	     0.000	        1	[resnet152v2/conv3_block3_2_relu/Relu;resnet152v2/conv3_block3_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block3_2_conv/Conv2D]:46
	                 CONV_2D	         3968.988	  117.400	  117.365	  0.431%	 15.011%	     0.000	        1	[resnet152v2/conv3_block3_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block3_3_conv/Conv2D]:47
	                     ADD	         4086.363	   36.443	   36.468	  0.134%	 15.145%	     0.000	        1	[resnet152v2/conv3_block3_out/add]:48
	                     MUL	         4122.842	   28.193	   28.223	  0.104%	 15.249%	     0.000	        1	[resnet152v2/conv3_block4_preact_bn/FusedBatchNormV3]:49
	                     ADD	         4151.075	   37.157	   37.186	  0.137%	 15.386%	     0.000	        1	[resnet152v2/conv3_block4_preact_relu/Relu;resnet152v2/conv3_block4_preact_bn/FusedBatchNormV3]:50
	                 CONV_2D	         4188.272	  111.310	  111.370	  0.409%	 15.795%	     0.000	        1	[resnet152v2/conv3_block4_1_relu/Relu;resnet152v2/conv3_block4_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block4_1_conv/Conv2D]:51
	                     PAD	         4299.653	    2.464	    2.450	  0.009%	 15.804%	     0.000	        1	[resnet152v2/conv3_block4_2_pad/Pad]:52
	                 CONV_2D	         4302.109	  255.627	  255.197	  0.938%	 16.741%	     0.000	        1	[resnet152v2/conv3_block4_2_relu/Relu;resnet152v2/conv3_block4_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block4_2_conv/Conv2D]:53
	                 CONV_2D	         4557.318	  119.220	  118.218	  0.434%	 17.176%	     0.000	        1	[resnet152v2/conv3_block4_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block4_3_conv/Conv2D]:54
	                     ADD	         4675.547	   37.552	   36.725	  0.135%	 17.311%	     0.000	        1	[resnet152v2/conv3_block4_out/add]:55
	                     MUL	         4712.283	   28.568	   28.325	  0.104%	 17.415%	     0.000	        1	[resnet152v2/conv3_block5_preact_bn/FusedBatchNormV3]:56
	                     ADD	         4740.619	   37.576	   37.309	  0.137%	 17.552%	     0.000	        1	[resnet152v2/conv3_block5_preact_relu/Relu;resnet152v2/conv3_block5_preact_bn/FusedBatchNormV3]:57
	                 CONV_2D	         4777.939	  115.526	  113.216	  0.416%	 17.968%	     0.000	        1	[resnet152v2/conv3_block5_1_relu/Relu;resnet152v2/conv3_block5_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block5_1_conv/Conv2D]:58
	                     PAD	         4891.166	    2.497	    2.461	  0.009%	 17.977%	     0.000	        1	[resnet152v2/conv3_block5_2_pad/Pad]:59
	                 CONV_2D	         4893.635	  261.210	  258.514	  0.950%	 18.927%	     0.000	        1	[resnet152v2/conv3_block5_2_relu/Relu;resnet152v2/conv3_block5_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block5_2_conv/Conv2D]:60
	                 CONV_2D	         5152.160	  119.299	  118.896	  0.437%	 19.364%	     0.000	        1	[resnet152v2/conv3_block5_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block5_3_conv/Conv2D]:61
	                     ADD	         5271.068	   36.931	   36.806	  0.135%	 19.499%	     0.000	        1	[resnet152v2/conv3_block5_out/add]:62
	                     MUL	         5307.885	   28.587	   28.521	  0.105%	 19.604%	     0.000	        1	[resnet152v2/conv3_block6_preact_bn/FusedBatchNormV3]:63
	                     ADD	         5336.417	   37.440	   37.484	  0.138%	 19.741%	     0.000	        1	[resnet152v2/conv3_block6_preact_relu/Relu;resnet152v2/conv3_block6_preact_bn/FusedBatchNormV3]:64
	                 CONV_2D	         5373.914	  113.545	  114.585	  0.421%	 20.162%	     0.000	        1	[resnet152v2/conv3_block6_1_relu/Relu;resnet152v2/conv3_block6_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block6_1_conv/Conv2D]:65
	                     PAD	         5488.510	    2.441	    2.473	  0.009%	 20.171%	     0.000	        1	[resnet152v2/conv3_block6_2_pad/Pad]:66
	                 CONV_2D	         5490.992	  254.795	  258.504	  0.950%	 21.121%	     0.000	        1	[resnet152v2/conv3_block6_2_relu/Relu;resnet152v2/conv3_block6_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block6_2_conv/Conv2D]:67
	                 CONV_2D	         5749.508	  117.621	  118.462	  0.435%	 21.556%	     0.000	        1	[resnet152v2/conv3_block6_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block6_3_conv/Conv2D]:68
	                     ADD	         5867.981	   36.484	   36.677	  0.135%	 21.691%	     0.000	        1	[resnet152v2/conv3_block6_out/add]:69
	                     MUL	         5904.669	   28.266	   28.339	  0.104%	 21.795%	     0.000	        1	[resnet152v2/conv3_block7_preact_bn/FusedBatchNormV3]:70
	                     ADD	         5933.024	   37.129	   37.365	  0.137%	 21.933%	     0.000	        1	[resnet152v2/conv3_block7_preact_relu/Relu;resnet152v2/conv3_block7_preact_bn/FusedBatchNormV3]:71
	                 CONV_2D	         5970.400	  111.330	  113.026	  0.415%	 22.348%	     0.000	        1	[resnet152v2/conv3_block7_1_relu/Relu;resnet152v2/conv3_block7_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block7_1_conv/Conv2D]:72
	                     PAD	         6083.437	    2.437	    2.446	  0.009%	 22.357%	     0.000	        1	[resnet152v2/conv3_block7_2_pad/Pad]:73
	                 CONV_2D	         6085.891	  256.032	  257.006	  0.944%	 23.301%	     0.000	        1	[resnet152v2/conv3_block7_2_relu/Relu;resnet152v2/conv3_block7_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block7_2_conv/Conv2D]:74
	                 CONV_2D	         6342.909	  117.568	  117.461	  0.432%	 23.733%	     0.000	        1	[resnet152v2/conv3_block7_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block7_3_conv/Conv2D]:75
	                     ADD	         6460.384	   36.526	   36.467	  0.134%	 23.867%	     0.000	        1	[resnet152v2/conv3_block7_out/add]:76
	             MAX_POOL_2D	         6496.864	    0.820	    0.826	  0.003%	 23.870%	     0.000	        1	[resnet152v2/max_pooling2d_9/MaxPool]:77
	                     MUL	         6497.697	   28.135	   28.203	  0.104%	 23.973%	     0.000	        1	[resnet152v2/conv3_block8_preact_bn/FusedBatchNormV3]:78
	                     ADD	         6525.910	   37.129	   37.153	  0.137%	 24.110%	     0.000	        1	[resnet152v2/conv3_block8_preact_relu/Relu;resnet152v2/conv3_block8_preact_bn/FusedBatchNormV3]:79
	                 CONV_2D	         6563.082	  110.727	  110.869	  0.407%	 24.517%	     0.000	        1	[resnet152v2/conv3_block8_1_relu/Relu;resnet152v2/conv3_block8_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block8_1_conv/Conv2D]:80
	                     PAD	         6673.962	    2.447	    2.452	  0.009%	 24.526%	     0.000	        1	[resnet152v2/conv3_block8_2_pad/Pad]:81
	                 CONV_2D	         6676.422	   63.167	   63.270	  0.232%	 24.759%	     0.000	        1	[resnet152v2/conv3_block8_2_relu/Relu;resnet152v2/conv3_block8_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block8_2_conv/Conv2D]:82
	                 CONV_2D	         6739.702	   29.401	   29.541	  0.109%	 24.867%	     0.000	        1	[resnet152v2/conv3_block8_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block8_3_conv/Conv2D]:83
	                     ADD	         6769.252	    9.143	    9.164	  0.034%	 24.901%	     0.000	        1	[resnet152v2/conv3_block8_out/add]:84
	                     MUL	         6778.424	    7.121	    7.059	  0.026%	 24.927%	     0.000	        1	[resnet152v2/conv4_block1_preact_bn/FusedBatchNormV3]:85
	                     ADD	         6785.490	    9.277	    9.262	  0.034%	 24.961%	     0.000	        1	[resnet152v2/conv4_block1_preact_relu/Relu;resnet152v2/conv4_block1_preact_bn/FusedBatchNormV3]:86
	                 CONV_2D	         6794.760	  213.132	  212.903	  0.782%	 25.743%	     0.000	        1	[resnet152v2/conv4_block1_0_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_0_conv/Conv2D]:87
	                 CONV_2D	         7007.675	   54.332	   54.458	  0.200%	 25.943%	     0.000	        1	[resnet152v2/conv4_block1_1_relu/Relu;resnet152v2/conv4_block1_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_1_conv/Conv2D]:88
	                     PAD	         7062.143	    1.340	    1.364	  0.005%	 25.948%	     0.000	        1	[resnet152v2/conv4_block1_2_pad/Pad]:89
	                 CONV_2D	         7063.514	  252.858	  247.292	  0.909%	 26.857%	     0.000	        1	[resnet152v2/conv4_block1_2_relu/Relu;resnet152v2/conv4_block1_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_2_conv/Conv2D]:90
	                 CONV_2D	         7310.818	  112.322	  111.625	  0.410%	 27.267%	     0.000	        1	[resnet152v2/conv4_block1_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_3_conv/Conv2D]:91
	                     ADD	         7422.453	   18.584	   18.375	  0.068%	 27.334%	     0.000	        1	[resnet152v2/conv4_block1_out/add]:92
	                     MUL	         7440.839	   14.231	   14.176	  0.052%	 27.386%	     0.000	        1	[resnet152v2/conv4_block2_preact_bn/FusedBatchNormV3]:93
	                     ADD	         7455.025	   18.772	   18.661	  0.069%	 27.455%	     0.000	        1	[resnet152v2/conv4_block2_preact_relu/Relu;resnet152v2/conv4_block2_preact_bn/FusedBatchNormV3]:94
	                 CONV_2D	         7473.696	  110.836	  109.967	  0.404%	 27.859%	     0.000	        1	[resnet152v2/conv4_block2_1_relu/Relu;resnet152v2/conv4_block2_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block2_1_conv/Conv2D]:95
	                     PAD	         7583.674	    1.400	    1.374	  0.005%	 27.864%	     0.000	        1	[resnet152v2/conv4_block2_2_pad/Pad]:96
	                 CONV_2D	         7585.057	  251.467	  250.474	  0.920%	 28.784%	     0.000	        1	[resnet152v2/conv4_block2_2_relu/Relu;resnet152v2/conv4_block2_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block2_2_conv/Conv2D]:97
	                 CONV_2D	         7835.542	  110.839	  111.509	  0.410%	 29.194%	     0.000	        1	[resnet152v2/conv4_block2_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block2_3_conv/Conv2D]:98
	                     ADD	         7947.065	   18.196	   18.470	  0.068%	 29.262%	     0.000	        1	[resnet152v2/conv4_block2_out/add]:99
	                     MUL	         7965.546	   14.037	   14.213	  0.052%	 29.314%	     0.000	        1	[resnet152v2/conv4_block3_preact_bn/FusedBatchNormV3]:100
	                     ADD	         7979.772	   18.649	   18.768	  0.069%	 29.383%	     0.000	        1	[resnet152v2/conv4_block3_preact_relu/Relu;resnet152v2/conv4_block3_preact_bn/FusedBatchNormV3]:101
	                 CONV_2D	         7998.551	  108.385	  110.352	  0.405%	 29.788%	     0.000	        1	[resnet152v2/conv4_block3_1_relu/Relu;resnet152v2/conv4_block3_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block3_1_conv/Conv2D]:102
	                     PAD	         8108.914	    1.343	    1.373	  0.005%	 29.793%	     0.000	        1	[resnet152v2/conv4_block3_2_pad/Pad]:103
	                 CONV_2D	         8110.294	  244.239	  248.441	  0.913%	 30.706%	     0.000	        1	[resnet152v2/conv4_block3_2_relu/Relu;resnet152v2/conv4_block3_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block3_2_conv/Conv2D]:104
	                 CONV_2D	         8358.747	  110.809	  111.010	  0.408%	 31.114%	     0.000	        1	[resnet152v2/conv4_block3_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block3_3_conv/Conv2D]:105
	                     ADD	         8469.768	   18.298	   18.331	  0.067%	 31.181%	     0.000	        1	[resnet152v2/conv4_block3_out/add]:106
	                     MUL	         8488.110	   14.046	   14.130	  0.052%	 31.233%	     0.000	        1	[resnet152v2/conv4_block4_preact_bn/FusedBatchNormV3]:107
	                     ADD	         8502.249	   18.471	   18.611	  0.068%	 31.302%	     0.000	        1	[resnet152v2/conv4_block4_preact_relu/Relu;resnet152v2/conv4_block4_preact_bn/FusedBatchNormV3]:108
	                 CONV_2D	         8520.869	  107.974	  109.104	  0.401%	 31.703%	     0.000	        1	[resnet152v2/conv4_block4_1_relu/Relu;resnet152v2/conv4_block4_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block4_1_conv/Conv2D]:109
	                     PAD	         8629.984	    1.349	    1.353	  0.005%	 31.707%	     0.000	        1	[resnet152v2/conv4_block4_2_pad/Pad]:110
	                 CONV_2D	         8631.345	  244.238	  245.512	  0.902%	 32.610%	     0.000	        1	[resnet152v2/conv4_block4_2_relu/Relu;resnet152v2/conv4_block4_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block4_2_conv/Conv2D]:111
	                 CONV_2D	         8876.868	  110.608	  110.622	  0.406%	 33.016%	     0.000	        1	[resnet152v2/conv4_block4_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block4_3_conv/Conv2D]:112
	                     ADD	         8987.501	   18.219	   18.245	  0.067%	 33.083%	     0.000	        1	[resnet152v2/conv4_block4_out/add]:113
	                     MUL	         9005.756	   14.088	   14.054	  0.052%	 33.135%	     0.000	        1	[resnet152v2/conv4_block5_preact_bn/FusedBatchNormV3]:114
	                     ADD	         9019.818	   18.634	   18.515	  0.068%	 33.203%	     0.000	        1	[resnet152v2/conv4_block5_preact_relu/Relu;resnet152v2/conv4_block5_preact_bn/FusedBatchNormV3]:115
	                 CONV_2D	         9038.342	  107.937	  108.064	  0.397%	 33.600%	     0.000	        1	[resnet152v2/conv4_block5_1_relu/Relu;resnet152v2/conv4_block5_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block5_1_conv/Conv2D]:116
	                     PAD	         9146.418	    1.343	    1.350	  0.005%	 33.605%	     0.000	        1	[resnet152v2/conv4_block5_2_pad/Pad]:117
	                 CONV_2D	         9147.776	  244.431	  244.435	  0.898%	 34.503%	     0.000	        1	[resnet152v2/conv4_block5_2_relu/Relu;resnet152v2/conv4_block5_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block5_2_conv/Conv2D]:118
	                 CONV_2D	         9392.225	  110.972	  110.590	  0.406%	 34.909%	     0.000	        1	[resnet152v2/conv4_block5_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block5_3_conv/Conv2D]:119
	                     ADD	         9502.825	   18.240	   18.231	  0.067%	 34.976%	     0.000	        1	[resnet152v2/conv4_block5_out/add]:120
	                     MUL	         9521.066	   14.102	   14.085	  0.052%	 35.028%	     0.000	        1	[resnet152v2/conv4_block6_preact_bn/FusedBatchNormV3]:121
	                     ADD	         9535.159	   18.644	   18.554	  0.068%	 35.096%	     0.000	        1	[resnet152v2/conv4_block6_preact_relu/Relu;resnet152v2/conv4_block6_preact_bn/FusedBatchNormV3]:122
	                 CONV_2D	         9553.724	  110.684	  108.739	  0.400%	 35.495%	     0.000	        1	[resnet152v2/conv4_block6_1_relu/Relu;resnet152v2/conv4_block6_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block6_1_conv/Conv2D]:123
	                     PAD	         9662.474	    1.474	    1.381	  0.005%	 35.500%	     0.000	        1	[resnet152v2/conv4_block6_2_pad/Pad]:124
	                 CONV_2D	         9663.863	  254.027	  248.234	  0.912%	 36.413%	     0.000	        1	[resnet152v2/conv4_block6_2_relu/Relu;resnet152v2/conv4_block6_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block6_2_conv/Conv2D]:125
	                 CONV_2D	         9912.111	  111.896	  111.726	  0.410%	 36.823%	     0.000	        1	[resnet152v2/conv4_block6_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block6_3_conv/Conv2D]:126
	                     ADD	        10023.848	   18.439	   18.348	  0.067%	 36.890%	     0.000	        1	[resnet152v2/conv4_block6_out/add]:127
	                     MUL	        10042.206	   14.261	   14.178	  0.052%	 36.943%	     0.000	        1	[resnet152v2/conv4_block7_preact_bn/FusedBatchNormV3]:128
	                     ADD	        10056.394	   18.757	   18.623	  0.068%	 37.011%	     0.000	        1	[resnet152v2/conv4_block7_preact_relu/Relu;resnet152v2/conv4_block7_preact_bn/FusedBatchNormV3]:129
	                 CONV_2D	        10075.027	  110.378	  109.757	  0.403%	 37.414%	     0.000	        1	[resnet152v2/conv4_block7_1_relu/Relu;resnet152v2/conv4_block7_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block7_1_conv/Conv2D]:130
	                     PAD	        10184.794	    1.361	    1.363	  0.005%	 37.419%	     0.000	        1	[resnet152v2/conv4_block7_2_pad/Pad]:131
	                 CONV_2D	        10186.165	  250.488	  251.238	  0.923%	 38.342%	     0.000	        1	[resnet152v2/conv4_block7_2_relu/Relu;resnet152v2/conv4_block7_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block7_2_conv/Conv2D]:132
	                 CONV_2D	        10437.416	  110.722	  111.800	  0.411%	 38.753%	     0.000	        1	[resnet152v2/conv4_block7_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block7_3_conv/Conv2D]:133
	                     ADD	        10549.230	   18.281	   18.440	  0.068%	 38.821%	     0.000	        1	[resnet152v2/conv4_block7_out/add]:134
	                     MUL	        10567.680	   14.053	   14.210	  0.052%	 38.873%	     0.000	        1	[resnet152v2/conv4_block8_preact_bn/FusedBatchNormV3]:135
	                     ADD	        10581.902	   18.503	   18.712	  0.069%	 38.942%	     0.000	        1	[resnet152v2/conv4_block8_preact_relu/Relu;resnet152v2/conv4_block8_preact_bn/FusedBatchNormV3]:136
	                 CONV_2D	        10600.624	  108.138	  110.134	  0.405%	 39.346%	     0.000	        1	[resnet152v2/conv4_block8_1_relu/Relu;resnet152v2/conv4_block8_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block8_1_conv/Conv2D]:137
	                     PAD	        10710.770	    1.347	    1.390	  0.005%	 39.351%	     0.000	        1	[resnet152v2/conv4_block8_2_pad/Pad]:138
	                 CONV_2D	        10712.168	  244.489	  249.115	  0.915%	 40.267%	     0.000	        1	[resnet152v2/conv4_block8_2_relu/Relu;resnet152v2/conv4_block8_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block8_2_conv/Conv2D]:139
	                 CONV_2D	        10961.295	  110.439	  110.843	  0.407%	 40.674%	     0.000	        1	[resnet152v2/conv4_block8_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block8_3_conv/Conv2D]:140
	                     ADD	        11072.149	   18.290	   18.273	  0.067%	 40.741%	     0.000	        1	[resnet152v2/conv4_block8_out/add]:141
	                     MUL	        11090.432	   14.061	   14.082	  0.052%	 40.793%	     0.000	        1	[resnet152v2/conv4_block9_preact_bn/FusedBatchNormV3]:142
	                     ADD	        11104.522	   18.513	   18.551	  0.068%	 40.861%	     0.000	        1	[resnet152v2/conv4_block9_preact_relu/Relu;resnet152v2/conv4_block9_preact_bn/FusedBatchNormV3]:143
	                 CONV_2D	        11123.083	  108.010	  108.450	  0.398%	 41.259%	     0.000	        1	[resnet152v2/conv4_block9_1_relu/Relu;resnet152v2/conv4_block9_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block9_1_conv/Conv2D]:144
	                     PAD	        11231.544	    1.348	    1.355	  0.005%	 41.264%	     0.000	        1	[resnet152v2/conv4_block9_2_pad/Pad]:145
	                 CONV_2D	        11232.906	  244.385	  244.707	  0.899%	 42.163%	     0.000	        1	[resnet152v2/conv4_block9_2_relu/Relu;resnet152v2/conv4_block9_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block9_2_conv/Conv2D]:146
	                 CONV_2D	        11477.624	  110.956	  110.578	  0.406%	 42.570%	     0.000	        1	[resnet152v2/conv4_block9_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block9_3_conv/Conv2D]:147
	                     ADD	        11588.214	   18.289	   18.242	  0.067%	 42.637%	     0.000	        1	[resnet152v2/conv4_block9_out/add]:148
	                     MUL	        11606.466	   14.032	   14.036	  0.052%	 42.688%	     0.000	        1	[resnet152v2/conv4_block10_preact_bn/FusedBatchNormV3]:149
	                     ADD	        11620.513	   18.467	   18.494	  0.068%	 42.756%	     0.000	        1	[resnet152v2/conv4_block10_preact_relu/Relu;resnet152v2/conv4_block10_preact_bn/FusedBatchNormV3]:150
	                 CONV_2D	        11639.015	  108.022	  108.042	  0.397%	 43.153%	     0.000	        1	[resnet152v2/conv4_block10_1_relu/Relu;resnet152v2/conv4_block10_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block10_1_conv/Conv2D]:151
	                     PAD	        11747.068	    1.348	    1.349	  0.005%	 43.158%	     0.000	        1	[resnet152v2/conv4_block10_2_pad/Pad]:152
	                 CONV_2D	        11748.424	  246.342	  244.852	  0.900%	 44.058%	     0.000	        1	[resnet152v2/conv4_block10_2_relu/Relu;resnet152v2/conv4_block10_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block10_2_conv/Conv2D]:153
	                 CONV_2D	        11993.287	  111.534	  110.722	  0.407%	 44.465%	     0.000	        1	[resnet152v2/conv4_block10_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block10_3_conv/Conv2D]:154
	                     ADD	        12104.021	   18.442	   18.268	  0.067%	 44.532%	     0.000	        1	[resnet152v2/conv4_block10_out/add]:155
	                     MUL	        12122.298	   14.343	   14.120	  0.052%	 44.584%	     0.000	        1	[resnet152v2/conv4_block11_preact_bn/FusedBatchNormV3]:156
	                     ADD	        12136.427	   18.811	   18.643	  0.068%	 44.652%	     0.000	        1	[resnet152v2/conv4_block11_preact_relu/Relu;resnet152v2/conv4_block11_preact_bn/FusedBatchNormV3]:157
	                 CONV_2D	        12155.080	  111.973	  109.418	  0.402%	 45.054%	     0.000	        1	[resnet152v2/conv4_block11_1_relu/Relu;resnet152v2/conv4_block11_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block11_1_conv/Conv2D]:158
	                     PAD	        12264.512	    1.458	    1.367	  0.005%	 45.059%	     0.000	        1	[resnet152v2/conv4_block11_2_pad/Pad]:159
	                 CONV_2D	        12265.886	  252.834	  249.543	  0.917%	 45.976%	     0.000	        1	[resnet152v2/conv4_block11_2_relu/Relu;resnet152v2/conv4_block11_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block11_2_conv/Conv2D]:160
	                 CONV_2D	        12515.441	  111.575	  111.228	  0.409%	 46.385%	     0.000	        1	[resnet152v2/conv4_block11_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block11_3_conv/Conv2D]:161
	                     ADD	        12626.684	   18.455	   18.356	  0.067%	 46.452%	     0.000	        1	[resnet152v2/conv4_block11_out/add]:162
	                     MUL	        12645.050	   14.250	   14.177	  0.052%	 46.504%	     0.000	        1	[resnet152v2/conv4_block12_preact_bn/FusedBatchNormV3]:163
	                     ADD	        12659.236	   18.753	   18.664	  0.069%	 46.573%	     0.000	        1	[resnet152v2/conv4_block12_preact_relu/Relu;resnet152v2/conv4_block12_preact_bn/FusedBatchNormV3]:164
	                 CONV_2D	        12677.909	  110.318	  110.475	  0.406%	 46.979%	     0.000	        1	[resnet152v2/conv4_block12_1_relu/Relu;resnet152v2/conv4_block12_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block12_1_conv/Conv2D]:165
	                     PAD	        12788.405	    1.397	    1.366	  0.005%	 46.984%	     0.000	        1	[resnet152v2/conv4_block12_2_pad/Pad]:166
	                 CONV_2D	        12789.779	  251.072	  251.905	  0.926%	 47.909%	     0.000	        1	[resnet152v2/conv4_block12_2_relu/Relu;resnet152v2/conv4_block12_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block12_2_conv/Conv2D]:167
	                 CONV_2D	        13041.695	  111.196	  111.525	  0.410%	 48.319%	     0.000	        1	[resnet152v2/conv4_block12_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block12_3_conv/Conv2D]:168
	                     ADD	        13153.234	   18.284	   18.404	  0.068%	 48.387%	     0.000	        1	[resnet152v2/conv4_block12_out/add]:169
	                     MUL	        13171.649	   14.056	   14.203	  0.052%	 48.439%	     0.000	        1	[resnet152v2/conv4_block13_preact_bn/FusedBatchNormV3]:170
	                     ADD	        13185.862	   18.502	   18.685	  0.069%	 48.507%	     0.000	        1	[resnet152v2/conv4_block13_preact_relu/Relu;resnet152v2/conv4_block13_preact_bn/FusedBatchNormV3]:171
	                 CONV_2D	        13204.557	  108.056	  109.754	  0.403%	 48.911%	     0.000	        1	[resnet152v2/conv4_block13_1_relu/Relu;resnet152v2/conv4_block13_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block13_1_conv/Conv2D]:172
	                     PAD	        13314.322	    1.351	    1.366	  0.005%	 48.916%	     0.000	        1	[resnet152v2/conv4_block13_2_pad/Pad]:173
	                 CONV_2D	        13315.695	  245.033	  247.175	  0.908%	 49.824%	     0.000	        1	[resnet152v2/conv4_block13_2_relu/Relu;resnet152v2/conv4_block13_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block13_2_conv/Conv2D]:174
	                 CONV_2D	        13562.881	  110.687	  110.632	  0.406%	 50.230%	     0.000	        1	[resnet152v2/conv4_block13_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block13_3_conv/Conv2D]:175
	                     ADD	        13673.526	   18.255	   18.237	  0.067%	 50.297%	     0.000	        1	[resnet152v2/conv4_block13_out/add]:176
	                     MUL	        13691.773	   14.062	   14.053	  0.052%	 50.349%	     0.000	        1	[resnet152v2/conv4_block14_preact_bn/FusedBatchNormV3]:177
	                     ADD	        13705.833	   18.504	   18.514	  0.068%	 50.417%	     0.000	        1	[resnet152v2/conv4_block14_preact_relu/Relu;resnet152v2/conv4_block14_preact_bn/FusedBatchNormV3]:178
	                 CONV_2D	        13724.356	  108.077	  108.110	  0.397%	 50.814%	     0.000	        1	[resnet152v2/conv4_block14_1_relu/Relu;resnet152v2/conv4_block14_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block14_1_conv/Conv2D]:179
	                     PAD	        13832.477	    1.342	    1.354	  0.005%	 50.819%	     0.000	        1	[resnet152v2/conv4_block14_2_pad/Pad]:180
	                 CONV_2D	        13833.838	  244.064	  244.273	  0.897%	 51.717%	     0.000	        1	[resnet152v2/conv4_block14_2_relu/Relu;resnet152v2/conv4_block14_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block14_2_conv/Conv2D]:181
	                 CONV_2D	        14078.124	  110.528	  110.541	  0.406%	 52.123%	     0.000	        1	[resnet152v2/conv4_block14_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block14_3_conv/Conv2D]:182
	                     ADD	        14188.677	   18.233	   18.228	  0.067%	 52.190%	     0.000	        1	[resnet152v2/conv4_block14_out/add]:183
	                     MUL	        14206.914	   14.029	   14.045	  0.052%	 52.241%	     0.000	        1	[resnet152v2/conv4_block15_preact_bn/FusedBatchNormV3]:184
	                     ADD	        14220.966	   18.497	   18.503	  0.068%	 52.309%	     0.000	        1	[resnet152v2/conv4_block15_preact_relu/Relu;resnet152v2/conv4_block15_preact_bn/FusedBatchNormV3]:185
	                 CONV_2D	        14239.478	  107.883	  108.094	  0.397%	 52.706%	     0.000	        1	[resnet152v2/conv4_block15_1_relu/Relu;resnet152v2/conv4_block15_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block15_1_conv/Conv2D]:186
	                     PAD	        14347.582	    1.350	    1.351	  0.005%	 52.711%	     0.000	        1	[resnet152v2/conv4_block15_2_pad/Pad]:187
	                 CONV_2D	        14348.940	  248.057	  245.094	  0.900%	 53.612%	     0.000	        1	[resnet152v2/conv4_block15_2_relu/Relu;resnet152v2/conv4_block15_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block15_2_conv/Conv2D]:188
	                 CONV_2D	        14594.046	  114.840	  111.471	  0.410%	 54.021%	     0.000	        1	[resnet152v2/conv4_block15_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block15_3_conv/Conv2D]:189
	                     ADD	        14705.528	   18.518	   18.329	  0.067%	 54.089%	     0.000	        1	[resnet152v2/conv4_block15_out/add]:190
	                     MUL	        14723.869	   14.313	   14.192	  0.052%	 54.141%	     0.000	        1	[resnet152v2/conv4_block16_preact_bn/FusedBatchNormV3]:191
	                     ADD	        14738.070	   18.807	   18.635	  0.068%	 54.209%	     0.000	        1	[resnet152v2/conv4_block16_preact_relu/Relu;resnet152v2/conv4_block16_preact_bn/FusedBatchNormV3]:192
	                 CONV_2D	        14756.714	  111.757	  109.827	  0.404%	 54.613%	     0.000	        1	[resnet152v2/conv4_block16_1_relu/Relu;resnet152v2/conv4_block16_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block16_1_conv/Conv2D]:193
	                     PAD	        14866.552	    1.355	    1.370	  0.005%	 54.618%	     0.000	        1	[resnet152v2/conv4_block16_2_pad/Pad]:194
	                 CONV_2D	        14867.931	  252.593	  249.439	  0.916%	 55.534%	     0.000	        1	[resnet152v2/conv4_block16_2_relu/Relu;resnet152v2/conv4_block16_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block16_2_conv/Conv2D]:195
	                 CONV_2D	        15117.380	  111.350	  111.331	  0.409%	 55.943%	     0.000	        1	[resnet152v2/conv4_block16_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block16_3_conv/Conv2D]:196
	                     ADD	        15228.722	   18.468	   18.408	  0.068%	 56.011%	     0.000	        1	[resnet152v2/conv4_block16_out/add]:197
	                     MUL	        15247.141	   14.197	   14.216	  0.052%	 56.063%	     0.000	        1	[resnet152v2/conv4_block17_preact_bn/FusedBatchNormV3]:198
	                     ADD	        15261.367	   18.788	   18.728	  0.069%	 56.132%	     0.000	        1	[resnet152v2/conv4_block17_preact_relu/Relu;resnet152v2/conv4_block17_preact_bn/FusedBatchNormV3]:199
	                 CONV_2D	        15280.105	  109.864	  111.222	  0.409%	 56.541%	     0.000	        1	[resnet152v2/conv4_block17_1_relu/Relu;resnet152v2/conv4_block17_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block17_1_conv/Conv2D]:200
	                     PAD	        15391.339	    1.397	    1.389	  0.005%	 56.546%	     0.000	        1	[resnet152v2/conv4_block17_2_pad/Pad]:201
	                 CONV_2D	        15392.737	  244.846	  250.688	  0.921%	 57.467%	     0.000	        1	[resnet152v2/conv4_block17_2_relu/Relu;resnet152v2/conv4_block17_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block17_2_conv/Conv2D]:202
	                 CONV_2D	        15643.442	  110.732	  111.081	  0.408%	 57.875%	     0.000	        1	[resnet152v2/conv4_block17_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block17_3_conv/Conv2D]:203
	                     ADD	        15754.534	   18.288	   18.359	  0.067%	 57.942%	     0.000	        1	[resnet152v2/conv4_block17_out/add]:204
	                     MUL	        15772.903	   14.076	   14.135	  0.052%	 57.994%	     0.000	        1	[resnet152v2/conv4_block18_preact_bn/FusedBatchNormV3]:205
	                     ADD	        15787.047	   18.534	   18.632	  0.068%	 58.063%	     0.000	        1	[resnet152v2/conv4_block18_preact_relu/Relu;resnet152v2/conv4_block18_preact_bn/FusedBatchNormV3]:206
	                 CONV_2D	        15805.689	  108.011	  109.253	  0.401%	 58.464%	     0.000	        1	[resnet152v2/conv4_block18_1_relu/Relu;resnet152v2/conv4_block18_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block18_1_conv/Conv2D]:207
	                     PAD	        15914.953	    1.356	    1.355	  0.005%	 58.469%	     0.000	        1	[resnet152v2/conv4_block18_2_pad/Pad]:208
	                 CONV_2D	        15916.316	  244.922	  246.590	  0.906%	 59.375%	     0.000	        1	[resnet152v2/conv4_block18_2_relu/Relu;resnet152v2/conv4_block18_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block18_2_conv/Conv2D]:209
	                 CONV_2D	        16162.917	  110.624	  110.547	  0.406%	 59.781%	     0.000	        1	[resnet152v2/conv4_block18_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block18_3_conv/Conv2D]:210
	                     ADD	        16273.475	   18.236	   18.220	  0.067%	 59.848%	     0.000	        1	[resnet152v2/conv4_block18_out/add]:211
	                     MUL	        16291.704	   14.076	   14.041	  0.052%	 59.900%	     0.000	        1	[resnet152v2/conv4_block19_preact_bn/FusedBatchNormV3]:212
	                     ADD	        16305.753	   18.502	   18.535	  0.068%	 59.968%	     0.000	        1	[resnet152v2/conv4_block19_preact_relu/Relu;resnet152v2/conv4_block19_preact_bn/FusedBatchNormV3]:213
	                 CONV_2D	        16324.297	  108.078	  108.256	  0.398%	 60.366%	     0.000	        1	[resnet152v2/conv4_block19_1_relu/Relu;resnet152v2/conv4_block19_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block19_1_conv/Conv2D]:214
	                     PAD	        16432.564	    1.357	    1.362	  0.005%	 60.371%	     0.000	        1	[resnet152v2/conv4_block19_2_pad/Pad]:215
	                 CONV_2D	        16433.933	  245.108	  244.709	  0.899%	 61.270%	     0.000	        1	[resnet152v2/conv4_block19_2_relu/Relu;resnet152v2/conv4_block19_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block19_2_conv/Conv2D]:216
	                 CONV_2D	        16678.654	  110.816	  110.600	  0.406%	 61.676%	     0.000	        1	[resnet152v2/conv4_block19_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block19_3_conv/Conv2D]:217
	                     ADD	        16789.266	   18.243	   18.212	  0.067%	 61.743%	     0.000	        1	[resnet152v2/conv4_block19_out/add]:218
	                     MUL	        16807.486	   14.076	   14.053	  0.052%	 61.795%	     0.000	        1	[resnet152v2/conv4_block20_preact_bn/FusedBatchNormV3]:219
	                     ADD	        16821.547	   18.607	   18.517	  0.068%	 61.863%	     0.000	        1	[resnet152v2/conv4_block20_preact_relu/Relu;resnet152v2/conv4_block20_preact_bn/FusedBatchNormV3]:220
	                 CONV_2D	        16840.075	  109.210	  108.270	  0.398%	 62.260%	     0.000	        1	[resnet152v2/conv4_block20_1_relu/Relu;resnet152v2/conv4_block20_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block20_1_conv/Conv2D]:221
	                     PAD	        16948.356	    1.360	    1.356	  0.005%	 62.265%	     0.000	        1	[resnet152v2/conv4_block20_2_pad/Pad]:222
	                 CONV_2D	        16949.719	  253.098	  246.835	  0.907%	 63.172%	     0.000	        1	[resnet152v2/conv4_block20_2_relu/Relu;resnet152v2/conv4_block20_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block20_2_conv/Conv2D]:223
	                 CONV_2D	        17196.565	  112.278	  111.463	  0.410%	 63.582%	     0.000	        1	[resnet152v2/conv4_block20_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block20_3_conv/Conv2D]:224
	                     ADD	        17308.039	   18.567	   18.345	  0.067%	 63.649%	     0.000	        1	[resnet152v2/conv4_block20_out/add]:225
	                     MUL	        17326.394	   14.514	   14.237	  0.052%	 63.702%	     0.000	        1	[resnet152v2/conv4_block21_preact_bn/FusedBatchNormV3]:226
	                     ADD	        17340.641	   18.851	   18.678	  0.069%	 63.770%	     0.000	        1	[resnet152v2/conv4_block21_preact_relu/Relu;resnet152v2/conv4_block21_preact_bn/FusedBatchNormV3]:227
	                 CONV_2D	        17359.329	  111.074	  109.883	  0.404%	 64.174%	     0.000	        1	[resnet152v2/conv4_block21_1_relu/Relu;resnet152v2/conv4_block21_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block21_1_conv/Conv2D]:228
	                     PAD	        17469.223	    1.358	    1.364	  0.005%	 64.179%	     0.000	        1	[resnet152v2/conv4_block21_2_pad/Pad]:229
	                 CONV_2D	        17470.595	  252.388	  250.311	  0.920%	 65.099%	     0.000	        1	[resnet152v2/conv4_block21_2_relu/Relu;resnet152v2/conv4_block21_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block21_2_conv/Conv2D]:230
	                 CONV_2D	        17720.920	  111.346	  111.601	  0.410%	 65.509%	     0.000	        1	[resnet152v2/conv4_block21_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block21_3_conv/Conv2D]:231
	                     ADD	        17832.532	   18.433	   18.474	  0.068%	 65.576%	     0.000	        1	[resnet152v2/conv4_block21_out/add]:232
	                     MUL	        17851.016	   14.099	   14.207	  0.052%	 65.629%	     0.000	        1	[resnet152v2/conv4_block22_preact_bn/FusedBatchNormV3]:233
	                     ADD	        17865.233	   18.644	   18.800	  0.069%	 65.698%	     0.000	        1	[resnet152v2/conv4_block22_preact_relu/Relu;resnet152v2/conv4_block22_preact_bn/FusedBatchNormV3]:234
	                 CONV_2D	        17884.044	  108.373	  110.725	  0.407%	 66.105%	     0.000	        1	[resnet152v2/conv4_block22_1_relu/Relu;resnet152v2/conv4_block22_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block22_1_conv/Conv2D]:235
	                     PAD	        17994.781	    1.396	    1.387	  0.005%	 66.110%	     0.000	        1	[resnet152v2/conv4_block22_2_pad/Pad]:236
	                 CONV_2D	        17996.176	  246.148	  249.560	  0.917%	 67.027%	     0.000	        1	[resnet152v2/conv4_block22_2_relu/Relu;resnet152v2/conv4_block22_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block22_2_conv/Conv2D]:237
	                 CONV_2D	        18245.748	  110.986	  111.106	  0.408%	 67.435%	     0.000	        1	[resnet152v2/conv4_block22_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block22_3_conv/Conv2D]:238
	                     ADD	        18356.865	   18.216	   18.344	  0.067%	 67.502%	     0.000	        1	[resnet152v2/conv4_block22_out/add]:239
	                     MUL	        18375.219	   14.036	   14.145	  0.052%	 67.554%	     0.000	        1	[resnet152v2/conv4_block23_preact_bn/FusedBatchNormV3]:240
	                     ADD	        18389.373	   18.479	   18.612	  0.068%	 67.622%	     0.000	        1	[resnet152v2/conv4_block23_preact_relu/Relu;resnet152v2/conv4_block23_preact_bn/FusedBatchNormV3]:241
	                 CONV_2D	        18407.994	  108.032	  108.849	  0.400%	 68.022%	     0.000	        1	[resnet152v2/conv4_block23_1_relu/Relu;resnet152v2/conv4_block23_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block23_1_conv/Conv2D]:242
	                     PAD	        18516.853	    1.347	    1.363	  0.005%	 68.027%	     0.000	        1	[resnet152v2/conv4_block23_2_pad/Pad]:243
	                 CONV_2D	        18518.223	  244.978	  245.941	  0.904%	 68.931%	     0.000	        1	[resnet152v2/conv4_block23_2_relu/Relu;resnet152v2/conv4_block23_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block23_2_conv/Conv2D]:244
	                 CONV_2D	        18764.176	  110.695	  110.744	  0.407%	 69.338%	     0.000	        1	[resnet152v2/conv4_block23_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block23_3_conv/Conv2D]:245
	                     ADD	        18874.932	   18.193	   18.244	  0.067%	 69.405%	     0.000	        1	[resnet152v2/conv4_block23_out/add]:246
	                     MUL	        18893.187	   14.032	   14.074	  0.052%	 69.457%	     0.000	        1	[resnet152v2/conv4_block24_preact_bn/FusedBatchNormV3]:247
	                     ADD	        18907.269	   18.570	   18.506	  0.068%	 69.525%	     0.000	        1	[resnet152v2/conv4_block24_preact_relu/Relu;resnet152v2/conv4_block24_preact_bn/FusedBatchNormV3]:248
	                 CONV_2D	        18925.784	  108.280	  108.079	  0.397%	 69.922%	     0.000	        1	[resnet152v2/conv4_block24_1_relu/Relu;resnet152v2/conv4_block24_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block24_1_conv/Conv2D]:249
	                     PAD	        19033.873	    1.341	    1.351	  0.005%	 69.927%	     0.000	        1	[resnet152v2/conv4_block24_2_pad/Pad]:250
	                 CONV_2D	        19035.232	  244.636	  244.794	  0.899%	 70.826%	     0.000	        1	[resnet152v2/conv4_block24_2_relu/Relu;resnet152v2/conv4_block24_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block24_2_conv/Conv2D]:251
	                 CONV_2D	        19280.037	  110.951	  110.599	  0.406%	 71.232%	     0.000	        1	[resnet152v2/conv4_block24_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block24_3_conv/Conv2D]:252
	                     ADD	        19390.648	   18.343	   18.253	  0.067%	 71.299%	     0.000	        1	[resnet152v2/conv4_block24_out/add]:253
	                     MUL	        19408.911	   14.118	   14.052	  0.052%	 71.351%	     0.000	        1	[resnet152v2/conv4_block25_preact_bn/FusedBatchNormV3]:254
	                     ADD	        19422.973	   18.603	   18.508	  0.068%	 71.419%	     0.000	        1	[resnet152v2/conv4_block25_preact_relu/Relu;resnet152v2/conv4_block25_preact_bn/FusedBatchNormV3]:255
	                 CONV_2D	        19441.489	  108.998	  108.395	  0.398%	 71.817%	     0.000	        1	[resnet152v2/conv4_block25_1_relu/Relu;resnet152v2/conv4_block25_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block25_1_conv/Conv2D]:256
	                     PAD	        19549.896	    1.355	    1.352	  0.005%	 71.822%	     0.000	        1	[resnet152v2/conv4_block25_2_pad/Pad]:257
	                 CONV_2D	        19551.255	  254.296	  248.009	  0.911%	 72.734%	     0.000	        1	[resnet152v2/conv4_block25_2_relu/Relu;resnet152v2/conv4_block25_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block25_2_conv/Conv2D]:258
	                 CONV_2D	        19799.276	  112.735	  111.510	  0.410%	 73.143%	     0.000	        1	[resnet152v2/conv4_block25_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block25_3_conv/Conv2D]:259
	                     ADD	        19910.798	   18.537	   18.424	  0.068%	 73.211%	     0.000	        1	[resnet152v2/conv4_block25_out/add]:260
	                     MUL	        19929.232	   14.184	   14.158	  0.052%	 73.263%	     0.000	        1	[resnet152v2/conv4_block26_preact_bn/FusedBatchNormV3]:261
	                     ADD	        19943.400	   18.749	   18.668	  0.069%	 73.332%	     0.000	        1	[resnet152v2/conv4_block26_preact_relu/Relu;resnet152v2/conv4_block26_preact_bn/FusedBatchNormV3]:262
	                 CONV_2D	        19962.078	  110.779	  109.898	  0.404%	 73.735%	     0.000	        1	[resnet152v2/conv4_block26_1_relu/Relu;resnet152v2/conv4_block26_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block26_1_conv/Conv2D]:263
	                     PAD	        20071.987	    1.363	    1.369	  0.005%	 73.740%	     0.000	        1	[resnet152v2/conv4_block26_2_pad/Pad]:264
	                 CONV_2D	        20073.364	  252.116	  250.550	  0.921%	 74.661%	     0.000	        1	[resnet152v2/conv4_block26_2_relu/Relu;resnet152v2/conv4_block26_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block26_2_conv/Conv2D]:265
	                 CONV_2D	        20323.925	  110.956	  111.781	  0.411%	 75.072%	     0.000	        1	[resnet152v2/conv4_block26_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block26_3_conv/Conv2D]:266
	                     ADD	        20435.718	   18.233	   18.386	  0.068%	 75.139%	     0.000	        1	[resnet152v2/conv4_block26_out/add]:267
	                     MUL	        20454.114	   14.065	   14.185	  0.052%	 75.191%	     0.000	        1	[resnet152v2/conv4_block27_preact_bn/FusedBatchNormV3]:268
	                     ADD	        20468.310	   18.536	   18.727	  0.069%	 75.260%	     0.000	        1	[resnet152v2/conv4_block27_preact_relu/Relu;resnet152v2/conv4_block27_preact_bn/FusedBatchNormV3]:269
	                 CONV_2D	        20487.048	  108.297	  110.401	  0.406%	 75.666%	     0.000	        1	[resnet152v2/conv4_block27_1_relu/Relu;resnet152v2/conv4_block27_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block27_1_conv/Conv2D]:270
	                     PAD	        20597.461	    1.360	    1.391	  0.005%	 75.671%	     0.000	        1	[resnet152v2/conv4_block27_2_pad/Pad]:271
	                 CONV_2D	        20598.860	  244.189	  249.251	  0.916%	 76.587%	     0.000	        1	[resnet152v2/conv4_block27_2_relu/Relu;resnet152v2/conv4_block27_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block27_2_conv/Conv2D]:272
	                 CONV_2D	        20848.122	  110.618	  111.088	  0.408%	 76.995%	     0.000	        1	[resnet152v2/conv4_block27_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block27_3_conv/Conv2D]:273
	                     ADD	        20959.222	   18.228	   18.322	  0.067%	 77.062%	     0.000	        1	[resnet152v2/conv4_block27_out/add]:274
	                     MUL	        20977.555	   14.057	   14.142	  0.052%	 77.114%	     0.000	        1	[resnet152v2/conv4_block28_preact_bn/FusedBatchNormV3]:275
	                     ADD	        20991.706	   18.474	   18.630	  0.068%	 77.182%	     0.000	        1	[resnet152v2/conv4_block28_preact_relu/Relu;resnet152v2/conv4_block28_preact_bn/FusedBatchNormV3]:276
	                 CONV_2D	        21010.346	  108.162	  108.915	  0.400%	 77.583%	     0.000	        1	[resnet152v2/conv4_block28_1_relu/Relu;resnet152v2/conv4_block28_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block28_1_conv/Conv2D]:277
	                     PAD	        21119.273	    1.368	    1.354	  0.005%	 77.588%	     0.000	        1	[resnet152v2/conv4_block28_2_pad/Pad]:278
	                 CONV_2D	        21120.634	  245.100	  244.833	  0.900%	 78.487%	     0.000	        1	[resnet152v2/conv4_block28_2_relu/Relu;resnet152v2/conv4_block28_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block28_2_conv/Conv2D]:279
	                 CONV_2D	        21365.478	  110.655	  110.749	  0.407%	 78.894%	     0.000	        1	[resnet152v2/conv4_block28_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block28_3_conv/Conv2D]:280
	                     ADD	        21476.238	   18.230	   18.230	  0.067%	 78.961%	     0.000	        1	[resnet152v2/conv4_block28_out/add]:281
	                     MUL	        21494.478	   14.051	   14.035	  0.052%	 79.012%	     0.000	        1	[resnet152v2/conv4_block29_preact_bn/FusedBatchNormV3]:282
	                     ADD	        21508.520	   18.535	   18.488	  0.068%	 79.080%	     0.000	        1	[resnet152v2/conv4_block29_preact_relu/Relu;resnet152v2/conv4_block29_preact_bn/FusedBatchNormV3]:283
	                 CONV_2D	        21527.017	  108.357	  108.181	  0.397%	 79.478%	     0.000	        1	[resnet152v2/conv4_block29_1_relu/Relu;resnet152v2/conv4_block29_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block29_1_conv/Conv2D]:284
	                     PAD	        21635.209	    1.377	    1.355	  0.005%	 79.483%	     0.000	        1	[resnet152v2/conv4_block29_2_pad/Pad]:285
	                 CONV_2D	        21636.570	  245.704	  244.833	  0.900%	 80.382%	     0.000	        1	[resnet152v2/conv4_block29_2_relu/Relu;resnet152v2/conv4_block29_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block29_2_conv/Conv2D]:286
	                 CONV_2D	        21881.416	  111.490	  110.750	  0.407%	 80.789%	     0.000	        1	[resnet152v2/conv4_block29_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block29_3_conv/Conv2D]:287
	                     ADD	        21992.178	   18.287	   18.229	  0.067%	 80.856%	     0.000	        1	[resnet152v2/conv4_block29_out/add]:288
	                     MUL	        22010.415	   14.207	   14.064	  0.052%	 80.908%	     0.000	        1	[resnet152v2/conv4_block30_preact_bn/FusedBatchNormV3]:289
	                     ADD	        22024.487	   18.791	   18.559	  0.068%	 80.976%	     0.000	        1	[resnet152v2/conv4_block30_preact_relu/Relu;resnet152v2/conv4_block30_preact_bn/FusedBatchNormV3]:290
	                 CONV_2D	        22043.057	  111.843	  109.364	  0.402%	 81.378%	     0.000	        1	[resnet152v2/conv4_block30_1_relu/Relu;resnet152v2/conv4_block30_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block30_1_conv/Conv2D]:291
	                     PAD	        22152.431	    1.366	    1.355	  0.005%	 81.383%	     0.000	        1	[resnet152v2/conv4_block30_2_pad/Pad]:292
	                 CONV_2D	        22153.794	  253.858	  248.733	  0.914%	 82.297%	     0.000	        1	[resnet152v2/conv4_block30_2_relu/Relu;resnet152v2/conv4_block30_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block30_2_conv/Conv2D]:293
	                 CONV_2D	        22402.541	  111.256	  111.184	  0.408%	 82.705%	     0.000	        1	[resnet152v2/conv4_block30_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block30_3_conv/Conv2D]:294
	                     ADD	        22513.736	   18.432	   18.373	  0.068%	 82.773%	     0.000	        1	[resnet152v2/conv4_block30_out/add]:295
	                     MUL	        22532.119	   14.233	   14.181	  0.052%	 82.825%	     0.000	        1	[resnet152v2/conv4_block31_preact_bn/FusedBatchNormV3]:296
	                     ADD	        22546.310	   18.727	   18.735	  0.069%	 82.894%	     0.000	        1	[resnet152v2/conv4_block31_preact_relu/Relu;resnet152v2/conv4_block31_preact_bn/FusedBatchNormV3]:297
	                 CONV_2D	        22565.056	  110.369	  109.944	  0.404%	 83.298%	     0.000	        1	[resnet152v2/conv4_block31_1_relu/Relu;resnet152v2/conv4_block31_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block31_1_conv/Conv2D]:298
	                     PAD	        22675.013	    1.386	    1.383	  0.005%	 83.303%	     0.000	        1	[resnet152v2/conv4_block31_2_pad/Pad]:299
	                 CONV_2D	        22676.404	  248.626	  251.383	  0.924%	 84.226%	     0.000	        1	[resnet152v2/conv4_block31_2_relu/Relu;resnet152v2/conv4_block31_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block31_2_conv/Conv2D]:300
	                 CONV_2D	        22927.799	  110.726	  111.525	  0.410%	 84.636%	     0.000	        1	[resnet152v2/conv4_block31_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block31_3_conv/Conv2D]:301
	                     ADD	        23039.335	   18.314	   18.368	  0.067%	 84.704%	     0.000	        1	[resnet152v2/conv4_block31_out/add]:302
	                     MUL	        23057.714	   14.058	   14.198	  0.052%	 84.756%	     0.000	        1	[resnet152v2/conv4_block32_preact_bn/FusedBatchNormV3]:303
	                     ADD	        23071.924	   18.558	   18.650	  0.069%	 84.824%	     0.000	        1	[resnet152v2/conv4_block32_preact_relu/Relu;resnet152v2/conv4_block32_preact_bn/FusedBatchNormV3]:304
	                 CONV_2D	        23090.584	  108.188	  109.466	  0.402%	 85.226%	     0.000	        1	[resnet152v2/conv4_block32_1_relu/Relu;resnet152v2/conv4_block32_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block32_1_conv/Conv2D]:305
	                     PAD	        23200.061	    1.347	    1.362	  0.005%	 85.231%	     0.000	        1	[resnet152v2/conv4_block32_2_pad/Pad]:306
	                 CONV_2D	        23201.431	  245.162	  248.204	  0.912%	 86.143%	     0.000	        1	[resnet152v2/conv4_block32_2_relu/Relu;resnet152v2/conv4_block32_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block32_2_conv/Conv2D]:307
	                 CONV_2D	        23449.646	  110.674	  110.869	  0.407%	 86.551%	     0.000	        1	[resnet152v2/conv4_block32_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block32_3_conv/Conv2D]:308
	                     ADD	        23560.528	   18.213	   18.306	  0.067%	 86.618%	     0.000	        1	[resnet152v2/conv4_block32_out/add]:309
	                     MUL	        23578.844	   14.066	   14.118	  0.052%	 86.670%	     0.000	        1	[resnet152v2/conv4_block33_preact_bn/FusedBatchNormV3]:310
	                     ADD	        23592.971	   18.524	   18.565	  0.068%	 86.738%	     0.000	        1	[resnet152v2/conv4_block33_preact_relu/Relu;resnet152v2/conv4_block33_preact_bn/FusedBatchNormV3]:311
	                 CONV_2D	        23611.545	  108.031	  108.222	  0.398%	 87.136%	     0.000	        1	[resnet152v2/conv4_block33_1_relu/Relu;resnet152v2/conv4_block33_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block33_1_conv/Conv2D]:312
	                     PAD	        23719.778	    1.344	    1.358	  0.005%	 87.141%	     0.000	        1	[resnet152v2/conv4_block33_2_pad/Pad]:313
	                 CONV_2D	        23721.143	  244.244	  244.659	  0.899%	 88.040%	     0.000	        1	[resnet152v2/conv4_block33_2_relu/Relu;resnet152v2/conv4_block33_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block33_2_conv/Conv2D]:314
	                 CONV_2D	        23965.813	  110.613	  110.695	  0.407%	 88.446%	     0.000	        1	[resnet152v2/conv4_block33_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block33_3_conv/Conv2D]:315
	                     ADD	        24076.520	   18.241	   18.231	  0.067%	 88.513%	     0.000	        1	[resnet152v2/conv4_block33_out/add]:316
	                     MUL	        24094.760	   14.106	   14.072	  0.052%	 88.565%	     0.000	        1	[resnet152v2/conv4_block34_preact_bn/FusedBatchNormV3]:317
	                     ADD	        24108.840	   18.501	   18.511	  0.068%	 88.633%	     0.000	        1	[resnet152v2/conv4_block34_preact_relu/Relu;resnet152v2/conv4_block34_preact_bn/FusedBatchNormV3]:318
	                 CONV_2D	        24127.359	  107.864	  108.293	  0.398%	 89.031%	     0.000	        1	[resnet152v2/conv4_block34_1_relu/Relu;resnet152v2/conv4_block34_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block34_1_conv/Conv2D]:319
	                     PAD	        24235.663	    1.348	    1.356	  0.005%	 89.036%	     0.000	        1	[resnet152v2/conv4_block34_2_pad/Pad]:320
	                 CONV_2D	        24237.026	  246.798	  245.457	  0.902%	 89.938%	     0.000	        1	[resnet152v2/conv4_block34_2_relu/Relu;resnet152v2/conv4_block34_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block34_2_conv/Conv2D]:321
	                 CONV_2D	        24482.495	  111.837	  110.897	  0.407%	 90.345%	     0.000	        1	[resnet152v2/conv4_block34_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block34_3_conv/Conv2D]:322
	                     ADD	        24593.405	   18.443	   18.329	  0.067%	 90.412%	     0.000	        1	[resnet152v2/conv4_block34_out/add]:323
	                     MUL	        24611.746	   14.415	   14.173	  0.052%	 90.464%	     0.000	        1	[resnet152v2/conv4_block35_preact_bn/FusedBatchNormV3]:324
	                     ADD	        24625.928	   18.801	   18.672	  0.069%	 90.533%	     0.000	        1	[resnet152v2/conv4_block35_preact_relu/Relu;resnet152v2/conv4_block35_preact_bn/FusedBatchNormV3]:325
	                 CONV_2D	        24644.610	  113.157	  110.469	  0.406%	 90.939%	     0.000	        1	[resnet152v2/conv4_block35_1_relu/Relu;resnet152v2/conv4_block35_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block35_1_conv/Conv2D]:326
	                     PAD	        24755.091	    1.381	    1.365	  0.005%	 90.944%	     0.000	        1	[resnet152v2/conv4_block35_2_pad/Pad]:327
	                 CONV_2D	        24756.464	  252.139	  249.715	  0.917%	 91.861%	     0.000	        1	[resnet152v2/conv4_block35_2_relu/Relu;resnet152v2/conv4_block35_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block35_2_conv/Conv2D]:328
	                 CONV_2D	        25006.192	  111.396	  111.092	  0.408%	 92.270%	     0.000	        1	[resnet152v2/conv4_block35_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block35_3_conv/Conv2D]:329
	                     ADD	        25117.296	   18.585	   18.447	  0.068%	 92.337%	     0.000	        1	[resnet152v2/conv4_block35_out/add]:330
	                     MUL	        25135.753	   14.196	   14.210	  0.052%	 92.390%	     0.000	        1	[resnet152v2/conv4_block36_preact_bn/FusedBatchNormV3]:331
	                     ADD	        25149.974	   18.739	   18.718	  0.069%	 92.458%	     0.000	        1	[resnet152v2/conv4_block36_preact_relu/Relu;resnet152v2/conv4_block36_preact_bn/FusedBatchNormV3]:332
	                 CONV_2D	        25168.705	  110.439	  110.769	  0.407%	 92.865%	     0.000	        1	[resnet152v2/conv4_block36_1_relu/Relu;resnet152v2/conv4_block36_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block36_1_conv/Conv2D]:333
	                     PAD	        25279.486	    1.344	    1.370	  0.005%	 92.870%	     0.000	        1	[resnet152v2/conv4_block36_2_pad/Pad]:334
	                 CONV_2D	        25280.865	   69.736	   70.019	  0.257%	 93.128%	     0.000	        1	[resnet152v2/conv4_block36_2_relu/Relu;resnet152v2/conv4_block36_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block36_2_conv/Conv2D]:335
	                 CONV_2D	        25350.897	   30.863	   31.046	  0.114%	 93.242%	     0.000	        1	[resnet152v2/conv4_block36_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block36_3_conv/Conv2D]:336
	             MAX_POOL_2D	        25381.954	    0.437	    0.442	  0.002%	 93.243%	     0.000	        1	[resnet152v2/max_pooling2d_10/MaxPool]:337
	                     ADD	        25382.404	    4.633	    4.645	  0.017%	 93.260%	     0.000	        1	[resnet152v2/conv4_block36_out/add]:338
	                     MUL	        25387.061	    3.597	    3.602	  0.013%	 93.274%	     0.000	        1	[resnet152v2/conv5_block1_preact_bn/FusedBatchNormV31]:339
	                     ADD	        25390.673	    4.725	    4.719	  0.017%	 93.291%	     0.000	        1	[resnet152v2/conv5_block1_preact_relu/Relu;resnet152v2/conv5_block1_preact_bn/FusedBatchNormV3]:340
	                 CONV_2D	        25395.405	  236.387	  237.104	  0.871%	 94.162%	     0.000	        1	[resnet152v2/conv5_block1_0_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_0_conv/Conv2D]:341
	                 CONV_2D	        25632.520	   59.554	   59.791	  0.220%	 94.382%	     0.000	        1	[resnet152v2/conv5_block1_1_relu/Relu;resnet152v2/conv5_block1_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_1_conv/Conv2D]:342
	                     PAD	        25692.322	    0.828	    0.826	  0.003%	 94.385%	     0.000	        1	[resnet152v2/conv5_block1_2_pad/Pad]:343
	                 CONV_2D	        25693.155	  267.880	  271.683	  0.998%	 95.383%	     0.000	        1	[resnet152v2/conv5_block1_2_relu/Relu;resnet152v2/conv5_block1_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_2_conv/Conv2D]:344
	                 CONV_2D	        25964.852	  117.565	  117.801	  0.433%	 95.816%	     0.000	        1	[resnet152v2/conv5_block1_3_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_3_conv/Conv2D]:345
	                     ADD	        26082.664	    9.165	    9.139	  0.034%	 95.849%	     0.000	        1	[resnet152v2/conv5_block1_out/add]:346
	                     MUL	        26091.811	    7.011	    7.008	  0.026%	 95.875%	     0.000	        1	[resnet152v2/conv5_block2_preact_bn/FusedBatchNormV3]:347
	                     ADD	        26098.826	    9.238	    9.249	  0.034%	 95.909%	     0.000	        1	[resnet152v2/conv5_block2_preact_relu/Relu;resnet152v2/conv5_block2_preact_bn/FusedBatchNormV3]:348
	                 CONV_2D	        26108.082	  119.074	  118.961	  0.437%	 96.346%	     0.000	        1	[resnet152v2/conv5_block2_1_relu/Relu;resnet152v2/conv5_block2_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_1_conv/Conv2D]:349
	                     PAD	        26227.054	    0.835	    0.829	  0.003%	 96.349%	     0.000	        1	[resnet152v2/conv5_block2_2_pad/Pad]:350
	                 CONV_2D	        26227.889	  267.923	  267.734	  0.984%	 97.333%	     0.000	        1	[resnet152v2/conv5_block2_2_relu/Relu;resnet152v2/conv5_block2_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_2_conv/Conv2D]:351
	                 CONV_2D	        26495.634	  117.667	  117.651	  0.432%	 97.765%	     0.000	        1	[resnet152v2/conv5_block2_3_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_3_conv/Conv2D]:352
	                     ADD	        26613.297	    9.191	    9.154	  0.034%	 97.799%	     0.000	        1	[resnet152v2/conv5_block2_out/add]:353
	                     MUL	        26622.459	    7.016	    7.009	  0.026%	 97.824%	     0.000	        1	[resnet152v2/conv5_block3_preact_bn/FusedBatchNormV3]:354
	                     ADD	        26629.475	    9.248	    9.249	  0.034%	 97.858%	     0.000	        1	[resnet152v2/conv5_block3_preact_relu/Relu;resnet152v2/conv5_block3_preact_bn/FusedBatchNormV3]:355
	                 CONV_2D	        26638.732	  118.629	  118.593	  0.436%	 98.294%	     0.000	        1	[resnet152v2/conv5_block3_1_relu/Relu;resnet152v2/conv5_block3_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_1_conv/Conv2D]:356
	                     PAD	        26757.336	    0.821	    0.815	  0.003%	 98.297%	     0.000	        1	[resnet152v2/conv5_block3_2_pad/Pad]:357
	                 CONV_2D	        26758.157	  270.197	  268.149	  0.985%	 99.282%	     0.000	        1	[resnet152v2/conv5_block3_2_relu/Relu;resnet152v2/conv5_block3_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_2_conv/Conv2D]:358
	                 CONV_2D	        27026.318	  120.136	  118.186	  0.434%	 99.717%	     0.000	        1	[resnet152v2/conv5_block3_3_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_3_conv/Conv2D]:359
	                     ADD	        27144.515	    9.397	    9.211	  0.034%	 99.750%	     0.000	        1	[resnet152v2/conv5_block3_out/add]:360
	                     MUL	        27153.736	    7.252	    7.084	  0.026%	 99.776%	     0.000	        1	[resnet152v2/post_bn/FusedBatchNormV31]:361
	                     ADD	        27160.828	    9.484	    9.334	  0.034%	 99.811%	     0.000	        1	[resnet152v2/post_relu/Relu;resnet152v2/post_bn/FusedBatchNormV3]:362
	                    MEAN	        27170.171	   17.396	   17.363	  0.064%	 99.875%	     0.000	        1	[resnet152v2/avg_pool/Mean]:363
	         FULLY_CONNECTED	        27187.544	   33.982	   34.060	  0.125%	100.000%	     0.000	        1	[resnet152v2/predictions/MatMul;resnet152v2/predictions/BiasAdd]:364
	                 SOFTMAX	        27221.620	    0.087	    0.093	  0.000%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:365

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            3.768	  333.146	  333.209	  1.224%	  1.224%	     0.000	        1	[resnet152v2/conv1_conv/BiasAdd;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv1_conv/Conv2D]:1
	                 CONV_2D	          577.866	  281.363	  281.458	  1.034%	  2.258%	     0.000	        1	[resnet152v2/conv2_block1_2_relu/Relu;resnet152v2/conv2_block1_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_2_conv/Conv2D]:9
	                 CONV_2D	         1332.092	  280.737	  280.950	  1.032%	  3.291%	     0.000	        1	[resnet152v2/conv2_block2_2_relu/Relu;resnet152v2/conv2_block2_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_2_conv/Conv2D]:16
	                 CONV_2D	        25693.155	  267.880	  271.683	  0.998%	  4.289%	     0.000	        1	[resnet152v2/conv5_block1_2_relu/Relu;resnet152v2/conv5_block1_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_2_conv/Conv2D]:344
	                 CONV_2D	        26758.157	  270.197	  268.149	  0.985%	  5.274%	     0.000	        1	[resnet152v2/conv5_block3_2_relu/Relu;resnet152v2/conv5_block3_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_2_conv/Conv2D]:358
	                 CONV_2D	        26227.889	  267.923	  267.734	  0.984%	  6.258%	     0.000	        1	[resnet152v2/conv5_block2_2_relu/Relu;resnet152v2/conv5_block2_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_2_conv/Conv2D]:351
	                 CONV_2D	         2522.290	  258.835	  258.558	  0.950%	  7.208%	     0.000	        1	[resnet152v2/conv3_block1_2_relu/Relu;resnet152v2/conv3_block1_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block1_2_conv/Conv2D]:32
	                 CONV_2D	         4893.635	  261.210	  258.514	  0.950%	  8.157%	     0.000	        1	[resnet152v2/conv3_block5_2_relu/Relu;resnet152v2/conv3_block5_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block5_2_conv/Conv2D]:60
	                 CONV_2D	         5490.992	  254.795	  258.504	  0.950%	  9.107%	     0.000	        1	[resnet152v2/conv3_block6_2_relu/Relu;resnet152v2/conv3_block6_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block6_2_conv/Conv2D]:67
	                 CONV_2D	         3119.711	  254.909	  257.635	  0.947%	 10.054%	     0.000	        1	[resnet152v2/conv3_block2_2_relu/Relu;resnet152v2/conv3_block2_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block2_2_conv/Conv2D]:39

Number of nodes executed: 366
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	      155	 23937.129	    87.947%	    87.947%	     0.000	      155
	                     ADD	      101	  2246.237	     8.253%	    96.200%	     0.000	      101
	                     MUL	       51	   867.233	     3.186%	    99.386%	     0.000	       51
	                     PAD	       52	   107.402	     0.395%	    99.781%	     0.000	       52
	         FULLY_CONNECTED	        1	    34.060	     0.125%	    99.906%	     0.000	        1
	                    MEAN	        1	    17.362	     0.064%	    99.970%	     0.000	        1
	             MAX_POOL_2D	        4	     8.201	     0.030%	   100.000%	     0.000	        4
	                 SOFTMAX	        1	     0.092	     0.000%	   100.000%	     0.000	        1

Timings (microseconds): count=6 first=27229044 curr=27207489 min=27205469 max=27237032 avg=2.72179e+07 std=11774
Memory (bytes): count=0
366 nodes observed



munmap_chunk(): invalid pointer
[ perf record: Woken up 1024 times to write data ]
[ perf record: Captured and wrote 255.827 MB /tmp/data.record (1310380 samples) ]

330.362

