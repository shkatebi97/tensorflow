STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 80)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (3136, 64, ), and Output shape (3136, 128, ), and the ID is 1
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 2
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 96, ), Input shape (3136, 96, ), and Output shape (3136, 128, ), and the ID is 3
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 4
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (3136, 128, ), and Output shape (3136, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
5
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
(3136, 128, ), and Output shape (3136, 32, ), and the ID is 6
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (3136, 160, ), and Output shape (3136, 128, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 80)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (3136, 192, ), and Output shape (3136, 128, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 10
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (3136, 224, ), and Output shape (3136, 128, ), and the ID is 11
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (3136, 256, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
(3136, 128, ), and the ID is 13
	Allocating LowPrecision Activations Tensors with Shape of (3136, 128)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 15
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (784, 160, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
	Allocating LowPrecision Activations Tensors with Shape of (784, 80)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (784, 192, ), and Output shape (784, 128, ), and the ID is 18
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
	Allocating LowPrecision Activations Tensors with Shape of (784, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (784, 224, ), and Output shape (784, 128, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
	Allocating LowPrecision Activations Tensors with Shape of (784, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 22
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (784, 288, ), and Output shape (784, 128, ), and the ID is 24
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 25
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (784, 320, ), and Output shape (784, 128, ), and the ID is 26	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)

	Allocating LowPrecision Activations Tensors with Shape of (784, 160)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 27
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (784, 352, ), and Output shape (784, 128, ), and the ID is 28
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (784, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 29	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (784, 384, ), and Output shape (784, 128, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (784, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 31
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (784, 416, ), and Output shape (784, 128, ), and the ID is 32
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (784, 208)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 33
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (784, 448, ), and Output shape (784, 128, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (784, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
35
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (784, 480, ), and Output shape (784, 128, ), and the ID is 36
	Allocating LowPrecision Activations Tensors with Shape of (784, 240)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
(784, 32, ), and the ID is 37
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (784, 512, ), and Output shape (784, 256, ), and the ID is 38
	Allocating LowPrecision Weight Tensors with Shape of (256, 256)
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (196, 256, ), and Output shape (196, 128, ), and the ID is 39
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 40
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (196, 288, ), and Output shape (196, 128, ), and the ID is 41	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)

	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 42
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (196, 320, ), and Output shape (196, 128, ), and the ID is 43
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (196, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
, and the ID is 44
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (196, 352, ), and Output shape (196, 128, ), and the ID is 45
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (196, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 46
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (196, 384, ), and Output shape (196, 128, ), and the ID is 47
	Allocating LowPrecision Activations Tensors with Shape of (196, 192)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 48
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (196, 416, ), and Output shape (196, 128, ), and the ID is 49
	Allocating LowPrecision Activations Tensors with Shape of (196, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
50
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (196, 448, ), and Output shape (196, 128, ), and the ID is 51
	Allocating LowPrecision Activations Tensors with Shape of (196, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 52
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (196, 480, ), and Output shape (196, 128, ), and the ID is 53
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
	Allocating LowPrecision Activations Tensors with Shape of (196, 240)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 54
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (196, 512, ), and Output shape (196, 128, ), and the ID is 55
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (196, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 56
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 272)
	Allocating LowPrecision Activations Tensors with Shape of (196, 272)
Applying Conv Low-Precision for Kernel shape (128, 544, ), Input shape (196, 544, ), and Output shape (196, 128, ), and the ID is 57
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 58
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (196, 576, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
(196, 128, ), and the ID is 59
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 60
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 608, ), Input shape (196, 608, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 304)
	Allocating LowPrecision Activations Tensors with Shape of (196, 304)
, and the ID is 61
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 62	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 320)
Applying Conv Low-Precision for Kernel shape (128, 640, ), Input shape (196, 640, ), and Output shape (196, 128, ), and the ID is 63
	Allocating LowPrecision Activations Tensors with Shape of (196, 320)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 64
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 336)
Applying Conv Low-Precision for Kernel shape (128, 672, ), Input shape (196, 672, ), and Output shape (196, 128, ), and the ID is 65
	Allocating LowPrecision Activations Tensors with Shape of (196, 336)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 66
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 352)
Applying Conv Low-Precision for Kernel shape (128, 704, ), Input shape (196, 704, ), and Output shape (196, 128, ), and the ID is 67
	Allocating LowPrecision Activations Tensors with Shape of (196, 352)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 68
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 368)
Applying Conv Low-Precision for Kernel shape (128, 736, ), Input shape (196, 736, ), and Output shape (196, 128, ), and the ID is 69
	Allocating LowPrecision Activations Tensors with Shape of (196, 368)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
, and the ID is 70
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 384)
Applying Conv Low-Precision for Kernel shape (128, 768, ), Input shape (196, 768, ), and Output shape (196, 128, ), and the ID is 71
	Allocating LowPrecision Activations Tensors with Shape of (196, 384)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 72
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 400)
Applying Conv Low-Precision for Kernel shape (128, 800, ), Input shape (196, 800, ), and Output shape (196, 128, ), and the ID is 73
	Allocating LowPrecision Activations Tensors with Shape of (196, 400)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 74	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 832, ), Input shape (196, 832, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 416)
(196, 128, ), and the ID is 75
	Allocating LowPrecision Activations Tensors with Shape of (196, 416)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 76
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 864, ), Input shape (196, 864, ), and Output shape (196, 128, ), and the ID is 77
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 432)
	Allocating LowPrecision Activations Tensors with Shape of (196, 432)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 78
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (196, 896, ), and Output shape (196, 128, ), and the ID is 79
	Allocating LowPrecision Activations Tensors with Shape of (196, 448)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 80
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (196, 928, ), and Output shape (196, 128, ), and the ID is 81
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 464)
	Allocating LowPrecision Activations Tensors with Shape of (196, 464)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 82
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (196, 960, ), and Output shape (196, 128, ), and the ID is 83
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (196, 480)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 84
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (196, 992, ), and Output shape (196, 128, ), and the ID is 85
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 496)
	Allocating LowPrecision Activations Tensors with Shape of (196, 496)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 86
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (196, 1024, ), and Output shape (196, 128, ), and the ID is 87
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (196, 512)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 88
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 528)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (196, 1056, ), and Output shape (196, 128, ), and the ID is 89
	Allocating LowPrecision Activations Tensors with Shape of (196, 528)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 90
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 544)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (196, 1088, ), and Output shape (196, 128, ), and the ID is 91
	Allocating LowPrecision Activations Tensors with Shape of (196, 544)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 92	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (196, 1120, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 560)
, and the ID is 93
	Allocating LowPrecision Activations Tensors with Shape of (196, 560)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (196, 1152, ), and Output shape (196, 128, ), and the ID is 95
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 96
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 592)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (196, 1184, ), and Output shape (196, 128, ), and the ID is 97
	Allocating LowPrecision Activations Tensors with Shape of (196, 592)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 98
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (196, 1216, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 608)
, and the ID is 99
	Allocating LowPrecision Activations Tensors with Shape of (196, 608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 100
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (196, 1248, ), and Output shape (196, 128, ), and the ID is 101
	Allocating LowPrecision Weight Tensors with Shape of (128, 624)
	Allocating LowPrecision Activations Tensors with Shape of (196, 624)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 102
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (196, 1280, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 640)
, and the ID is 103
	Allocating LowPrecision Activations Tensors with Shape of (196, 640)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 104
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 656)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (196, 1312, ), and Output shape (196, 128, ), and the ID is 105
	Allocating LowPrecision Activations Tensors with Shape of (196, 656)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 106
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 672)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (196, 1344, ), and Output shape (196, 128, ), and the ID is 107
	Allocating LowPrecision Activations Tensors with Shape of (196, 672)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 108
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (196, 1376, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 688)
(196, 128, ), and the ID is 109
	Allocating LowPrecision Activations Tensors with Shape of (196, 688)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 110
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 704)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (196, 1408, ), and Output shape (196, 128, ), and the ID is 111
	Allocating LowPrecision Activations Tensors with Shape of (196, 704)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 112
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (196, 1440, ), and Output shape (196, 128, ), and the ID is 113
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 720)
	Allocating LowPrecision Activations Tensors with Shape of (196, 720)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 114
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 736)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (196, 1472, ), and Output shape (196, 128, ), and the ID is 115
	Allocating LowPrecision Activations Tensors with Shape of (196, 736)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (196, 1504, ), and Output shape (196, 128, ), and the ID is 117
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 752)
	Allocating LowPrecision Activations Tensors with Shape of (196, 752)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 118
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 768)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (196, 1536, ), and Output shape (196, 128, ), and the ID is 119
	Allocating LowPrecision Activations Tensors with Shape of (196, 768)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 120
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 784)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (196, 1568, ), and Output shape (196, 128, ), and the ID is 121
	Allocating LowPrecision Activations Tensors with Shape of (196, 784)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 122
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 800)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (196, 1600, ), and Output shape (196, 128, ), and the ID is 123
	Allocating LowPrecision Activations Tensors with Shape of (196, 800)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 124	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)

Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (196, 1632, ), and Output shape (196, 128, ), and the ID is 125
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 816)
	Allocating LowPrecision Activations Tensors with Shape of (196, 816)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 126
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 832)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (196, 1664, ), and Output shape (196, 128, ), and the ID is 127
	Allocating LowPrecision Activations Tensors with Shape of (196, 832)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 128
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (196, 1696, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 848)
129
	Allocating LowPrecision Activations Tensors with Shape of (196, 848)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 130
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (196, 1728, ), and Output shape (196, 128, ), and the ID is 131
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 864)
	Allocating LowPrecision Activations Tensors with Shape of (196, 864)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 132
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (196, 1760, ), and Output shape (196, 128, ), and the ID is 133
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 880)
	Allocating LowPrecision Activations Tensors with Shape of (196, 880)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 134
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (896, 1792, ), Input shape (196, 1792, ), and Output shape (196, 896, ), and the ID is 135
	Allocating LowPrecision Weight Tensors with Shape of (896, 896)
	Allocating LowPrecision Activations Tensors with Shape of (196, 896)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (49, 896, ), and Output shape (49, 128, ), and the ID is 136
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (52, 448)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 137
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 464)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (49, 928, ), and Output shape (49, 128, ), and the ID is 138
	Allocating LowPrecision Activations Tensors with Shape of (52, 464)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (49, 960, ), and Output shape (49, 128, ), and the ID is 140
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (52, 480)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 141
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 496)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (49, 992, ), and Output shape (49, 128, ), and the ID is 142
	Allocating LowPrecision Activations Tensors with Shape of (52, 496)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 143
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (49, 1024, ), and Output shape (49, 128, ), and the ID is 144
	Allocating LowPrecision Activations Tensors with Shape of (52, 512)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 145
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 528)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (49, 1056, ), and Output shape (49, 128, ), and the ID is 146
	Allocating LowPrecision Activations Tensors with Shape of (52, 528)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 147
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (49, 1088, ), and Output shape (49, 128, ), and the ID is 148
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 544)
	Allocating LowPrecision Activations Tensors with Shape of (52, 544)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 560)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (49, 1120, ), and Output shape (49, 128, ), and the ID is 150
	Allocating LowPrecision Activations Tensors with Shape of (52, 560)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 151
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (49, 1152, ), and Output shape (49, 128, ), and the ID is 152
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 153
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 592)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (49, 1184, ), and Output shape (49, 128, ), and the ID is 154
	Allocating LowPrecision Activations Tensors with Shape of (52, 592)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 608)
	Allocating LowPrecision Activations Tensors with Shape of (52, 608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 624)
	Allocating LowPrecision Activations Tensors with Shape of (52, 624)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 640)
	Allocating LowPrecision Activations Tensors with Shape of (52, 640)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 656)
	Allocating LowPrecision Activations Tensors with Shape of (52, 656)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 672)
	Allocating LowPrecision Activations Tensors with Shape of (52, 672)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 688)
	Allocating LowPrecision Activations Tensors with Shape of (52, 688)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 704)
	Allocating LowPrecision Activations Tensors with Shape of (52, 704)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 720)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 155
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (49, 1216, ), and Output shape (49, 128, ), and the ID is 156
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 157
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (49, 1248, ), and Output shape (49, 128, ), and the ID is 158
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 159
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (49, 1280, ), and Output shape (49, 128, ), and the ID is 160
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 161
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (49, 1312, ), and Output shape (49, 128, ), and the ID is 162
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 163
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (49, 1344, ), and Output shape (49, 128, ), and the ID is 164
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 165
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (49, 1376, ), and Output shape (49, 128, ), and the ID is 166
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 167
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (49, 1408, ), and Output shape (49, 128, ), and the ID is 168
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 169
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (49, 1440, ), and Output shape (49, 128, ), and the ID is 170
	Allocating LowPrecision Activations Tensors with Shape of (52, 720)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 171
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 736)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (49, 1472, ), and Output shape (49, 128, ), and the ID is 172
	Allocating LowPrecision Activations Tensors with Shape of (52, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 173
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 752)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (49, 1504, ), and Output shape (49, 128, ), and the ID is 174
	Allocating LowPrecision Activations Tensors with Shape of (52, 752)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 175
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 768)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (49, 1536, ), and Output shape (49, 128, ), and the ID is 176
	Allocating LowPrecision Activations Tensors with Shape of (52, 768)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 177
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 784)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (49, 1568, ), and Output shape (49, 128, ), and the ID is 178
	Allocating LowPrecision Activations Tensors with Shape of (52, 784)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 179
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 800)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (49, 1600, ), and Output shape (49, 128, ), and the ID is 180
	Allocating LowPrecision Activations Tensors with Shape of (52, 800)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 181
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (49, 1632, ), and Output shape (49, 128, ), and the ID is 182
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 816)
	Allocating LowPrecision Activations Tensors with Shape of (52, 816)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 183
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (49, 1664, ), and Output shape (49, 128, ), and the ID is 184
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 832)
	Allocating LowPrecision Activations Tensors with Shape of (52, 832)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 185
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 848)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (49, 1696, ), and Output shape (49, 128, ), and the ID is 186
	Allocating LowPrecision Activations Tensors with Shape of (52, 848)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 187
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (49, 1728, ), and Output shape (49, 128, ), and the ID is 188
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 864)
	Allocating LowPrecision Activations Tensors with Shape of (52, 864)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 189
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (49, 1760, ), and Output shape (49, 128, ), and the ID is 190
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 880)
	Allocating LowPrecision Activations Tensors with Shape of (52, 880)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 191
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 896)
Applying Conv Low-Precision for Kernel shape (128, 1792, ), Input shape (49, 1792, ), and Output shape (49, 128, ), and the ID is 192
	Allocating LowPrecision Activations Tensors with Shape of (52, 896)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 193	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1824, ), Input shape (49, 1824, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 912)
194
	Allocating LowPrecision Activations Tensors with Shape of (52, 912)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 195
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1856, ), Input shape (49, 1856, ), and Output shape (49, 128, ), and the ID is 196
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 928)
	Allocating LowPrecision Activations Tensors with Shape of (52, 928)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 197
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1888, ), Input shape (49, 1888, ), and Output shape (49, 128, ), and the ID is 198
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 944)
	Allocating LowPrecision Activations Tensors with Shape of (52, 944)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 199
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Low-Precision for shape (1000, 1920, ) and Input shape (1, 1920, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 960)
	Transformed Activation Shape From: (1, 1920) To: (1, 960)
The input model file size (MB): 20.5199
Initialized session in 123.761ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=3538246 curr=3538499 min=3517021 max=3538499 avg=3.52823e+06 std=7283

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=3535128 curr=3532531 min=3525279 max=3551468 avg=3.53658e+06 std=7129

Inference timings in us: Init: 123761, First inference: 3538246, Warmup (avg): 3.52823e+06, Inference (avg): 3.53658e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=30.7305 overall=39.2305
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  100.902	  100.902	100.000%	100.000%	 17140.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  100.902	  100.902	100.000%	100.000%	 17140.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   100.902	   100.000%	   100.000%	 17140.000	        1

Timings (microseconds): count=1 curr=100902
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.025	    3.710	    3.706	  0.105%	  0.105%	     0.000	        1	[densenet201/zero_padding2d/Pad]:0
	                 CONV_2D	            3.740	   20.090	   20.255	  0.573%	  0.678%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                     PAD	           24.006	   18.203	   18.247	  0.517%	  1.195%	     0.000	        1	[densenet201/zero_padding2d_1/Pad]:2
	             MAX_POOL_2D	           42.264	    5.197	    5.229	  0.148%	  1.343%	     0.000	        1	[densenet201/pool1/MaxPool]:3
	                     MUL	           47.502	   14.593	   14.702	  0.416%	  1.759%	     0.000	        1	[densenet201/conv2_block1_0_bn/FusedBatchNormV31]:4
	                     ADD	           62.214	   18.709	   18.805	  0.532%	  2.292%	     0.000	        1	[densenet201/conv2_block1_0_relu/Relu;densenet201/conv2_block1_0_bn/FusedBatchNormV3]:5
	                 CONV_2D	           81.029	    2.812	    2.833	  0.080%	  2.372%	     0.000	        1	[densenet201/conv2_block1_1_relu/Relu;densenet201/conv2_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block1_1_conv/Conv2D]:6
	                 CONV_2D	           83.871	   13.225	   13.047	  0.369%	  2.741%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	           CONCATENATION	           96.930	    0.345	    0.369	  0.010%	  2.752%	     0.000	        1	[densenet201/conv2_block1_concat/concat]:8
	                     MUL	           97.307	   21.668	   21.715	  0.615%	  3.367%	     0.000	        1	[densenet201/conv2_block2_0_bn/FusedBatchNormV31]:9
	                     ADD	          119.035	   27.749	   28.006	  0.793%	  4.159%	     0.000	        1	[densenet201/conv2_block2_0_relu/Relu;densenet201/conv2_block2_0_bn/FusedBatchNormV3]:10
	                 CONV_2D	          147.052	    5.365	    5.455	  0.154%	  4.314%	     0.000	        1	[densenet201/conv2_block2_1_relu/Relu;densenet201/conv2_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block2_1_conv/Conv2D]:11
	                 CONV_2D	          152.516	   13.327	   13.114	  0.371%	  4.685%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	           CONCATENATION	          165.641	    0.403	    0.413	  0.012%	  4.697%	     0.000	        1	[densenet201/conv2_block2_concat/concat]:13
	                     MUL	          166.063	   28.648	   28.817	  0.816%	  5.513%	     0.000	        1	[densenet201/conv2_block3_0_bn/FusedBatchNormV3]:14
	                     ADD	          194.892	   36.932	   37.169	  1.052%	  6.565%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                 CONV_2D	          232.073	    4.457	    4.507	  0.128%	  6.693%	     0.000	        1	[densenet201/conv2_block3_1_relu/Relu;densenet201/conv2_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block3_1_conv/Conv2D]:16
	                 CONV_2D	          236.589	   13.235	   13.033	  0.369%	  7.062%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	           CONCATENATION	          249.634	    0.516	    0.483	  0.014%	  7.075%	     0.000	        1	[densenet201/conv2_block3_concat/concat]:18
	                     MUL	          250.127	   35.800	   35.851	  1.015%	  8.090%	     0.000	        1	[densenet201/conv2_block4_0_bn/FusedBatchNormV3]:19
	                     ADD	          285.990	   46.046	   46.315	  1.311%	  9.402%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                 CONV_2D	          332.316	    7.071	    7.133	  0.202%	  9.604%	     0.000	        1	[densenet201/conv2_block4_1_relu/Relu;densenet201/conv2_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block4_1_conv/Conv2D]:21
	                 CONV_2D	          339.460	   13.277	   13.203	  0.374%	  9.977%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	           CONCATENATION	          352.674	    0.503	    0.543	  0.015%	  9.993%	     0.000	        1	[densenet201/conv2_block4_concat/concat]:23
	                     MUL	          353.229	   42.935	   42.890	  1.214%	 11.207%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	          396.131	   55.325	   55.595	  1.574%	 12.781%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                 CONV_2D	          451.738	    6.166	    6.167	  0.175%	 12.956%	     0.000	        1	[densenet201/conv2_block5_1_relu/Relu;densenet201/conv2_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block5_1_conv/Conv2D]:26
	                 CONV_2D	          457.915	   13.172	   12.976	  0.367%	 13.323%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	           CONCATENATION	          470.902	    0.624	    0.686	  0.019%	 13.342%	     0.000	        1	[densenet201/conv2_block5_concat/concat]:28
	                     MUL	          471.601	   49.966	   49.922	  1.413%	 14.756%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	          521.536	   64.636	   64.656	  1.831%	 16.586%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                 CONV_2D	          586.204	    8.781	    8.826	  0.250%	 16.836%	     0.000	        1	[densenet201/conv2_block6_1_relu/Relu;densenet201/conv2_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block6_1_conv/Conv2D]:31
	                 CONV_2D	          595.041	   13.101	   13.157	  0.373%	 17.209%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	           CONCATENATION	          608.210	    0.774	    0.843	  0.024%	 17.233%	     0.000	        1	[densenet201/conv2_block6_concat/concat]:33
	                     MUL	          609.064	   56.951	   57.043	  1.615%	 18.848%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     ADD	          666.119	   73.509	   73.781	  2.089%	 20.937%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                 CONV_2D	          739.911	    7.799	    7.823	  0.221%	 21.158%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	         AVERAGE_POOL_2D	          747.745	   10.371	   10.391	  0.294%	 21.452%	     0.000	        1	[densenet201/pool2_pool/AvgPool]:37
	                     MUL	          758.144	    7.192	    7.207	  0.204%	 21.656%	     0.000	        1	[densenet201/conv3_block1_0_bn/FusedBatchNormV3]:38
	                     ADD	          765.361	    9.236	    9.306	  0.263%	 21.920%	     0.000	        1	[densenet201/conv3_block1_0_relu/Relu;densenet201/conv3_block1_0_bn/FusedBatchNormV3]:39
	                 CONV_2D	          774.676	    1.174	    1.163	  0.033%	 21.953%	     0.000	        1	[densenet201/conv3_block1_1_relu/Relu;densenet201/conv3_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block1_1_conv/Conv2D]:40
	                 CONV_2D	          775.847	    2.826	    2.937	  0.083%	 22.036%	     0.000	        1	[densenet201/conv3_block1_2_conv/Conv2D1]:41
	           CONCATENATION	          778.795	    0.175	    0.154	  0.004%	 22.040%	     0.000	        1	[densenet201/conv3_block1_concat/concat]:42
	                     MUL	          778.956	    8.904	    8.970	  0.254%	 22.294%	     0.000	        1	[densenet201/conv3_block2_0_bn/FusedBatchNormV31]:43
	                     ADD	          787.935	   11.510	   11.561	  0.327%	 22.621%	     0.000	        1	[densenet201/conv3_block2_0_relu/Relu;densenet201/conv3_block2_0_bn/FusedBatchNormV3]:44
	                 CONV_2D	          799.506	    1.852	    1.834	  0.052%	 22.673%	     0.000	        1	[densenet201/conv3_block2_1_relu/Relu;densenet201/conv3_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block2_1_conv/Conv2D]:45
	                 CONV_2D	          801.350	    2.789	    2.963	  0.084%	 22.757%	     0.000	        1	[densenet201/conv3_block2_2_conv/Conv2D1]:46
	           CONCATENATION	          804.323	    0.164	    0.193	  0.005%	 22.763%	     0.000	        1	[densenet201/conv3_block2_concat/concat]:47
	                     MUL	          804.523	   10.669	   10.741	  0.304%	 23.067%	     0.000	        1	[densenet201/conv3_block3_0_bn/FusedBatchNormV31]:48
	                     ADD	          815.274	   13.879	   13.916	  0.394%	 23.461%	     0.000	        1	[densenet201/conv3_block3_0_relu/Relu;densenet201/conv3_block3_0_bn/FusedBatchNormV3]:49
	                 CONV_2D	          829.200	    1.551	    1.581	  0.045%	 23.506%	     0.000	        1	[densenet201/conv3_block3_1_relu/Relu;densenet201/conv3_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block3_1_conv/Conv2D]:50
	                 CONV_2D	          830.789	    2.852	    2.911	  0.082%	 23.588%	     0.000	        1	[densenet201/conv3_block3_2_conv/Conv2D1]:51
	           CONCATENATION	          833.710	    0.176	    0.222	  0.006%	 23.594%	     0.000	        1	[densenet201/conv3_block3_concat/concat]:52
	                     MUL	          833.940	   12.552	   12.477	  0.353%	 23.948%	     0.000	        1	[densenet201/conv3_block4_0_bn/FusedBatchNormV31]:53
	                     ADD	          846.427	   16.157	   16.157	  0.457%	 24.405%	     0.000	        1	[densenet201/conv3_block4_0_relu/Relu;densenet201/conv3_block4_0_bn/FusedBatchNormV3]:54
	                 CONV_2D	          862.594	    2.217	    2.240	  0.063%	 24.468%	     0.000	        1	[densenet201/conv3_block4_1_relu/Relu;densenet201/conv3_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block4_1_conv/Conv2D]:55
	                 CONV_2D	          864.843	    2.861	    2.974	  0.084%	 24.553%	     0.000	        1	[densenet201/conv3_block4_2_conv/Conv2D1]:56
	           CONCATENATION	          867.828	    0.307	    0.260	  0.007%	 24.560%	     0.000	        1	[densenet201/conv3_block4_concat/concat]:57
	                     MUL	          868.097	   14.359	   14.261	  0.404%	 24.964%	     0.000	        1	[densenet201/conv3_block5_0_bn/FusedBatchNormV3]:58
	                     ADD	          882.368	   18.290	   18.460	  0.523%	 25.486%	     0.000	        1	[densenet201/conv3_block5_0_relu/Relu;densenet201/conv3_block5_0_bn/FusedBatchNormV3]:59
	                 CONV_2D	          900.837	    1.987	    2.026	  0.057%	 25.544%	     0.000	        1	[densenet201/conv3_block5_1_relu/Relu;densenet201/conv3_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block5_1_conv/Conv2D]:60
	                 CONV_2D	          902.877	    2.946	    2.977	  0.084%	 25.628%	     0.000	        1	[densenet201/conv3_block5_2_conv/Conv2D1]:61
	           CONCATENATION	          905.864	    0.294	    0.277	  0.008%	 25.636%	     0.000	        1	[densenet201/conv3_block5_concat/concat]:62
	                     MUL	          906.149	   15.913	   16.034	  0.454%	 26.090%	     0.000	        1	[densenet201/conv3_block6_0_bn/FusedBatchNormV3]:63
	                     ADD	          922.192	   20.513	   20.683	  0.586%	 26.675%	     0.000	        1	[densenet201/conv3_block6_0_relu/Relu;densenet201/conv3_block6_0_bn/FusedBatchNormV3]:64
	                 CONV_2D	          942.886	    2.781	    2.690	  0.076%	 26.752%	     0.000	        1	[densenet201/conv3_block6_1_relu/Relu;densenet201/conv3_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block6_1_conv/Conv2D]:65
	                 CONV_2D	          945.585	    2.755	    2.937	  0.083%	 26.835%	     0.000	        1	[densenet201/conv3_block6_2_conv/Conv2D1]:66
	           CONCATENATION	          948.532	    0.352	    0.317	  0.009%	 26.844%	     0.000	        1	[densenet201/conv3_block6_concat/concat]:67
	                     MUL	          948.858	   17.698	   17.761	  0.503%	 27.347%	     0.000	        1	[densenet201/conv3_block7_0_bn/FusedBatchNormV3]:68
	                     ADD	          966.629	   22.929	   22.939	  0.649%	 27.996%	     0.000	        1	[densenet201/conv3_block7_0_relu/Relu;densenet201/conv3_block7_0_bn/FusedBatchNormV3]:69
	                 CONV_2D	          989.579	    2.387	    2.414	  0.068%	 28.064%	     0.000	        1	[densenet201/conv3_block7_1_relu/Relu;densenet201/conv3_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block7_1_conv/Conv2D]:70
	                 CONV_2D	          992.002	    2.848	    2.945	  0.083%	 28.148%	     0.000	        1	[densenet201/conv3_block7_2_conv/Conv2D1]:71
	           CONCATENATION	          994.958	    0.367	    0.339	  0.010%	 28.157%	     0.000	        1	[densenet201/conv3_block7_concat/concat]:72
	                     MUL	          995.306	   19.564	   19.596	  0.555%	 28.712%	     0.000	        1	[densenet201/conv3_block8_0_bn/FusedBatchNormV3]:73
	                     ADD	         1014.912	   25.067	   25.240	  0.715%	 29.427%	     0.000	        1	[densenet201/conv3_block8_0_relu/Relu;densenet201/conv3_block8_0_bn/FusedBatchNormV3]:74
	                 CONV_2D	         1040.162	    3.095	    3.094	  0.088%	 29.514%	     0.000	        1	[densenet201/conv3_block8_1_relu/Relu;densenet201/conv3_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block8_1_conv/Conv2D]:75
	                 CONV_2D	         1043.265	    2.924	    2.937	  0.083%	 29.597%	     0.000	        1	[densenet201/conv3_block8_2_conv/Conv2D1]:76
	           CONCATENATION	         1046.212	    0.467	    0.396	  0.011%	 29.609%	     0.000	        1	[densenet201/conv3_block8_concat/concat]:77
	                     MUL	         1046.617	   21.159	   21.272	  0.602%	 30.211%	     0.000	        1	[densenet201/conv3_block9_0_bn/FusedBatchNormV3]:78
	                     ADD	         1067.900	   27.283	   27.458	  0.777%	 30.988%	     0.000	        1	[densenet201/conv3_block9_0_relu/Relu;densenet201/conv3_block9_0_bn/FusedBatchNormV3]:79
	                 CONV_2D	         1095.368	    2.810	    2.827	  0.080%	 31.068%	     0.000	        1	[densenet201/conv3_block9_1_relu/Relu;densenet201/conv3_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block9_1_conv/Conv2D]:80
	                 CONV_2D	         1098.205	    2.854	    2.979	  0.084%	 31.153%	     0.000	        1	[densenet201/conv3_block9_2_conv/Conv2D1]:81
	           CONCATENATION	         1101.194	    0.456	    0.402	  0.011%	 31.164%	     0.000	        1	[densenet201/conv3_block9_concat/concat]:82
	                     MUL	         1101.605	   22.870	   23.043	  0.652%	 31.816%	     0.000	        1	[densenet201/conv3_block10_0_bn/FusedBatchNormV3]:83
	                     ADD	         1124.659	   29.581	   29.785	  0.843%	 32.660%	     0.000	        1	[densenet201/conv3_block10_0_relu/Relu;densenet201/conv3_block10_0_bn/FusedBatchNormV3]:84
	                 CONV_2D	         1154.454	    3.526	    3.538	  0.100%	 32.760%	     0.000	        1	[densenet201/conv3_block10_1_relu/Relu;densenet201/conv3_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block10_1_conv/Conv2D]:85
	                 CONV_2D	         1158.002	    2.924	    2.956	  0.084%	 32.844%	     0.000	        1	[densenet201/conv3_block10_2_conv/Conv2D1]:86
	           CONCATENATION	         1160.968	    0.502	    0.455	  0.013%	 32.856%	     0.000	        1	[densenet201/conv3_block10_concat/concat]:87
	                     MUL	         1161.436	   24.641	   24.790	  0.702%	 33.558%	     0.000	        1	[densenet201/conv3_block11_0_bn/FusedBatchNormV3]:88
	                     ADD	         1186.237	   31.847	   32.041	  0.907%	 34.465%	     0.000	        1	[densenet201/conv3_block11_0_relu/Relu;densenet201/conv3_block11_0_bn/FusedBatchNormV3]:89
	                 CONV_2D	         1218.288	    3.303	    3.320	  0.094%	 34.559%	     0.000	        1	[densenet201/conv3_block11_1_relu/Relu;densenet201/conv3_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block11_1_conv/Conv2D]:90
	                 CONV_2D	         1221.617	    2.825	    2.912	  0.082%	 34.642%	     0.000	        1	[densenet201/conv3_block11_2_conv/Conv2D1]:91
	           CONCATENATION	         1224.540	    0.511	    0.474	  0.013%	 34.655%	     0.000	        1	[densenet201/conv3_block11_concat/concat]:92
	                     MUL	         1225.025	   26.792	   26.607	  0.753%	 35.409%	     0.000	        1	[densenet201/conv3_block12_0_bn/FusedBatchNormV3]:93
	                     ADD	         1251.643	   34.635	   34.382	  0.973%	 36.382%	     0.000	        1	[densenet201/conv3_block12_0_relu/Relu;densenet201/conv3_block12_0_bn/FusedBatchNormV3]:94
	                 CONV_2D	         1286.036	    3.957	    4.009	  0.114%	 36.496%	     0.000	        1	[densenet201/conv3_block12_1_relu/Relu;densenet201/conv3_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block12_1_conv/Conv2D]:95
	                 CONV_2D	         1290.055	    3.037	    2.992	  0.085%	 36.580%	     0.000	        1	[densenet201/conv3_block12_2_conv/Conv2D1]:96
	           CONCATENATION	         1293.058	    0.342	    0.415	  0.012%	 36.592%	     0.000	        1	[densenet201/conv3_block12_concat/concat]:97
	                     MUL	         1293.482	   28.260	   28.323	  0.802%	 37.394%	     0.000	        1	[densenet201/pool3_bn/FusedBatchNormV3]:98
	                     ADD	         1321.816	   36.478	   36.673	  1.038%	 38.432%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99
	                 CONV_2D	         1358.501	    7.067	    7.119	  0.202%	 38.634%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100
	         AVERAGE_POOL_2D	         1365.629	    5.117	    4.989	  0.141%	 38.775%	     0.000	        1	[densenet201/pool3_pool/AvgPool]:101
	                     MUL	         1370.627	    3.590	    3.580	  0.101%	 38.876%	     0.000	        1	[densenet201/conv4_block1_0_bn/FusedBatchNormV31]:102
	                     ADD	         1374.216	    4.775	    4.649	  0.132%	 39.008%	     0.000	        1	[densenet201/conv4_block1_0_relu/Relu;densenet201/conv4_block1_0_bn/FusedBatchNormV3]:103
	                 CONV_2D	         1378.874	    0.536	    0.534	  0.015%	 39.023%	     0.000	        1	[densenet201/conv4_block1_1_relu/Relu;densenet201/conv4_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block1_1_conv/Conv2D]:104
	                 CONV_2D	         1379.414	    0.665	    0.664	  0.019%	 39.042%	     0.000	        1	[densenet201/conv4_block1_2_conv/Conv2D1]:105
	           CONCATENATION	         1380.087	    0.085	    0.077	  0.002%	 39.044%	     0.000	        1	[densenet201/conv4_block1_concat/concat]:106
	                     MUL	         1380.171	    4.026	    4.009	  0.113%	 39.158%	     0.000	        1	[densenet201/conv4_block2_0_bn/FusedBatchNormV31]:107
	                     ADD	         1384.188	    5.194	    5.177	  0.147%	 39.304%	     0.000	        1	[densenet201/conv4_block2_0_relu/Relu;densenet201/conv4_block2_0_bn/FusedBatchNormV3]:108
	                 CONV_2D	         1389.374	    0.703	    0.697	  0.020%	 39.324%	     0.000	        1	[densenet201/conv4_block2_1_relu/Relu;densenet201/conv4_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block2_1_conv/Conv2D]:109
	                 CONV_2D	         1390.079	    0.680	    0.657	  0.019%	 39.343%	     0.000	        1	[densenet201/conv4_block2_2_conv/Conv2D1]:110
	           CONCATENATION	         1390.743	    0.069	    0.068	  0.002%	 39.344%	     0.000	        1	[densenet201/conv4_block2_concat/concat]:111
	                     MUL	         1390.817	    4.422	    4.452	  0.126%	 39.471%	     0.000	        1	[densenet201/conv4_block3_0_bn/FusedBatchNormV31]:112
	                     ADD	         1395.278	    5.692	    5.751	  0.163%	 39.633%	     0.000	        1	[densenet201/conv4_block3_0_relu/Relu;densenet201/conv4_block3_0_bn/FusedBatchNormV3]:113
	                 CONV_2D	         1401.038	    0.616	    0.635	  0.018%	 39.651%	     0.000	        1	[densenet201/conv4_block3_1_relu/Relu;densenet201/conv4_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block3_1_conv/Conv2D]:114
	                 CONV_2D	         1401.680	    0.641	    0.665	  0.019%	 39.670%	     0.000	        1	[densenet201/conv4_block3_2_conv/Conv2D1]:115
	           CONCATENATION	         1402.353	    0.052	    0.067	  0.002%	 39.672%	     0.000	        1	[densenet201/conv4_block3_concat/concat]:116
	                     MUL	         1402.427	    5.057	    4.902	  0.139%	 39.811%	     0.000	        1	[densenet201/conv4_block4_0_bn/FusedBatchNormV31]:117
	                     ADD	         1407.339	    6.376	    6.336	  0.179%	 39.990%	     0.000	        1	[densenet201/conv4_block4_0_relu/Relu;densenet201/conv4_block4_0_bn/FusedBatchNormV3]:118
	                 CONV_2D	         1413.685	    0.850	    0.809	  0.023%	 40.013%	     0.000	        1	[densenet201/conv4_block4_1_relu/Relu;densenet201/conv4_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block4_1_conv/Conv2D]:119
	                 CONV_2D	         1414.502	    0.691	    0.658	  0.019%	 40.032%	     0.000	        1	[densenet201/conv4_block4_2_conv/Conv2D1]:120
	           CONCATENATION	         1415.168	    0.106	    0.081	  0.002%	 40.034%	     0.000	        1	[densenet201/conv4_block4_concat/concat]:121
	                     MUL	         1415.256	    5.378	    5.366	  0.152%	 40.186%	     0.000	        1	[densenet201/conv4_block5_0_bn/FusedBatchNormV31]:122
	                     ADD	         1420.631	    6.969	    6.887	  0.195%	 40.381%	     0.000	        1	[densenet201/conv4_block5_0_relu/Relu;densenet201/conv4_block5_0_bn/FusedBatchNormV3]:123
	                 CONV_2D	         1427.527	    0.766	    0.743	  0.021%	 40.402%	     0.000	        1	[densenet201/conv4_block5_1_relu/Relu;densenet201/conv4_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block5_1_conv/Conv2D]:124
	                 CONV_2D	         1428.277	    0.729	    0.667	  0.019%	 40.421%	     0.000	        1	[densenet201/conv4_block5_2_conv/Conv2D1]:125
	           CONCATENATION	         1428.952	    0.104	    0.078	  0.002%	 40.423%	     0.000	        1	[densenet201/conv4_block5_concat/concat]:126
	                     MUL	         1429.037	    5.855	    5.765	  0.163%	 40.586%	     0.000	        1	[densenet201/conv4_block6_0_bn/FusedBatchNormV31]:127
	                     ADD	         1434.811	    7.524	    7.450	  0.211%	 40.797%	     0.000	        1	[densenet201/conv4_block6_0_relu/Relu;densenet201/conv4_block6_0_bn/FusedBatchNormV3]:128
	                 CONV_2D	         1442.270	    0.948	    0.919	  0.026%	 40.823%	     0.000	        1	[densenet201/conv4_block6_1_relu/Relu;densenet201/conv4_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block6_1_conv/Conv2D]:129
	                 CONV_2D	         1443.196	    0.704	    0.656	  0.019%	 40.842%	     0.000	        1	[densenet201/conv4_block6_2_conv/Conv2D1]:130
	           CONCATENATION	         1443.860	    0.134	    0.085	  0.002%	 40.844%	     0.000	        1	[densenet201/conv4_block6_concat/concat]:131
	                     MUL	         1443.954	    6.337	    6.208	  0.176%	 41.020%	     0.000	        1	[densenet201/conv4_block7_0_bn/FusedBatchNormV31]:132
	                     ADD	         1450.171	    8.154	    8.022	  0.227%	 41.247%	     0.000	        1	[densenet201/conv4_block7_0_relu/Relu;densenet201/conv4_block7_0_bn/FusedBatchNormV3]:133
	                 CONV_2D	         1458.202	    0.863	    0.849	  0.024%	 41.271%	     0.000	        1	[densenet201/conv4_block7_1_relu/Relu;densenet201/conv4_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block7_1_conv/Conv2D]:134
	                 CONV_2D	         1459.058	    0.726	    0.658	  0.019%	 41.290%	     0.000	        1	[densenet201/conv4_block7_2_conv/Conv2D1]:135
	           CONCATENATION	         1459.724	    0.134	    0.095	  0.003%	 41.292%	     0.000	        1	[densenet201/conv4_block7_concat/concat]:136
	                     MUL	         1459.825	    6.760	    6.643	  0.188%	 41.481%	     0.000	        1	[densenet201/conv4_block8_0_bn/FusedBatchNormV31]:137
	                     ADD	         1466.477	    8.691	    8.585	  0.243%	 41.724%	     0.000	        1	[densenet201/conv4_block8_0_relu/Relu;densenet201/conv4_block8_0_bn/FusedBatchNormV3]:138
	                 CONV_2D	         1475.071	    1.032	    1.016	  0.029%	 41.752%	     0.000	        1	[densenet201/conv4_block8_1_relu/Relu;densenet201/conv4_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block8_1_conv/Conv2D]:139
	                 CONV_2D	         1476.095	    0.729	    0.668	  0.019%	 41.771%	     0.000	        1	[densenet201/conv4_block8_2_conv/Conv2D1]:140
	           CONCATENATION	         1476.771	    0.155	    0.099	  0.003%	 41.774%	     0.000	        1	[densenet201/conv4_block8_concat/concat]:141
	                     MUL	         1476.877	    7.156	    7.092	  0.201%	 41.975%	     0.000	        1	[densenet201/conv4_block9_0_bn/FusedBatchNormV31]:142
	                     ADD	         1483.978	    9.385	    9.185	  0.260%	 42.235%	     0.000	        1	[densenet201/conv4_block9_0_relu/Relu;densenet201/conv4_block9_0_bn/FusedBatchNormV3]:143
	                 CONV_2D	         1493.172	    0.987	    0.958	  0.027%	 42.262%	     0.000	        1	[densenet201/conv4_block9_1_relu/Relu;densenet201/conv4_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block9_1_conv/Conv2D]:144
	                 CONV_2D	         1494.138	    0.716	    0.665	  0.019%	 42.281%	     0.000	        1	[densenet201/conv4_block9_2_conv/Conv2D1]:145
	           CONCATENATION	         1494.811	    0.135	    0.100	  0.003%	 42.284%	     0.000	        1	[densenet201/conv4_block9_concat/concat]:146
	                     MUL	         1494.917	    8.322	    7.560	  0.214%	 42.498%	     0.000	        1	[densenet201/conv4_block10_0_bn/FusedBatchNormV31]:147
	                     ADD	         1502.486	    9.917	    9.758	  0.276%	 42.774%	     0.000	        1	[densenet201/conv4_block10_0_relu/Relu;densenet201/conv4_block10_0_bn/FusedBatchNormV3]:148
	                 CONV_2D	         1512.253	    1.166	    1.130	  0.032%	 42.806%	     0.000	        1	[densenet201/conv4_block10_1_relu/Relu;densenet201/conv4_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block10_1_conv/Conv2D]:149
	                 CONV_2D	         1513.391	    0.729	    0.666	  0.019%	 42.825%	     0.000	        1	[densenet201/conv4_block10_2_conv/Conv2D1]:150
	           CONCATENATION	         1514.065	    0.197	    0.107	  0.003%	 42.828%	     0.000	        1	[densenet201/conv4_block10_concat/concat]:151
	                     MUL	         1514.178	    8.183	    7.963	  0.225%	 43.053%	     0.000	        1	[densenet201/conv4_block11_0_bn/FusedBatchNormV31]:152
	                     ADD	         1522.150	   10.509	   10.340	  0.293%	 43.346%	     0.000	        1	[densenet201/conv4_block11_0_relu/Relu;densenet201/conv4_block11_0_bn/FusedBatchNormV3]:153
	                 CONV_2D	         1532.500	    1.092	    1.067	  0.030%	 43.376%	     0.000	        1	[densenet201/conv4_block11_1_relu/Relu;densenet201/conv4_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block11_1_conv/Conv2D]:154
	                 CONV_2D	         1533.575	    0.702	    0.650	  0.018%	 43.395%	     0.000	        1	[densenet201/conv4_block11_2_conv/Conv2D1]:155
	           CONCATENATION	         1534.234	    0.167	    0.129	  0.004%	 43.398%	     0.000	        1	[densenet201/conv4_block11_concat/concat]:156
	                     MUL	         1534.370	    8.550	    8.414	  0.238%	 43.637%	     0.000	        1	[densenet201/conv4_block12_0_bn/FusedBatchNormV31]:157
	                     ADD	         1542.794	   11.113	   10.921	  0.309%	 43.946%	     0.000	        1	[densenet201/conv4_block12_0_relu/Relu;densenet201/conv4_block12_0_bn/FusedBatchNormV3]:158
	                 CONV_2D	         1553.724	    1.312	    1.234	  0.035%	 43.981%	     0.000	        1	[densenet201/conv4_block12_1_relu/Relu;densenet201/conv4_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block12_1_conv/Conv2D]:159
	                 CONV_2D	         1554.966	    0.728	    0.678	  0.019%	 44.000%	     0.000	        1	[densenet201/conv4_block12_2_conv/Conv2D1]:160
	           CONCATENATION	         1555.652	    0.190	    0.115	  0.003%	 44.003%	     0.000	        1	[densenet201/conv4_block12_concat/concat]:161
	                     MUL	         1555.773	    8.967	    8.851	  0.251%	 44.254%	     0.000	        1	[densenet201/conv4_block13_0_bn/FusedBatchNormV31]:162
	                     ADD	         1564.633	   11.616	   11.456	  0.324%	 44.578%	     0.000	        1	[densenet201/conv4_block13_0_relu/Relu;densenet201/conv4_block13_0_bn/FusedBatchNormV3]:163
	                 CONV_2D	         1576.098	    1.202	    1.182	  0.033%	 44.612%	     0.000	        1	[densenet201/conv4_block13_1_relu/Relu;densenet201/conv4_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block13_1_conv/Conv2D]:164
	                 CONV_2D	         1577.288	    0.712	    0.653	  0.018%	 44.630%	     0.000	        1	[densenet201/conv4_block13_2_conv/Conv2D1]:165
	           CONCATENATION	         1577.950	    0.169	    0.123	  0.003%	 44.634%	     0.000	        1	[densenet201/conv4_block13_concat/concat]:166
	                     MUL	         1578.081	    9.364	    9.267	  0.262%	 44.896%	     0.000	        1	[densenet201/conv4_block14_0_bn/FusedBatchNormV31]:167
	                     ADD	         1587.357	   12.204	   12.057	  0.341%	 45.237%	     0.000	        1	[densenet201/conv4_block14_0_relu/Relu;densenet201/conv4_block14_0_bn/FusedBatchNormV3]:168
	                 CONV_2D	         1599.425	    1.367	    1.354	  0.038%	 45.276%	     0.000	        1	[densenet201/conv4_block14_1_relu/Relu;densenet201/conv4_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block14_1_conv/Conv2D]:169
	                 CONV_2D	         1600.787	    0.718	    0.680	  0.019%	 45.295%	     0.000	        1	[densenet201/conv4_block14_2_conv/Conv2D1]:170
	           CONCATENATION	         1601.476	    0.193	    0.129	  0.004%	 45.299%	     0.000	        1	[densenet201/conv4_block14_concat/concat]:171
	                     MUL	         1601.612	    9.893	    9.716	  0.275%	 45.574%	     0.000	        1	[densenet201/conv4_block15_0_bn/FusedBatchNormV31]:172
	                     ADD	         1611.336	   12.817	   12.671	  0.359%	 45.932%	     0.000	        1	[densenet201/conv4_block15_0_relu/Relu;densenet201/conv4_block15_0_bn/FusedBatchNormV3]:173
	                 CONV_2D	         1624.016	    1.278	    1.278	  0.036%	 45.969%	     0.000	        1	[densenet201/conv4_block15_1_relu/Relu;densenet201/conv4_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block15_1_conv/Conv2D]:174
	                 CONV_2D	         1625.302	    0.718	    0.661	  0.019%	 45.987%	     0.000	        1	[densenet201/conv4_block15_2_conv/Conv2D1]:175
	           CONCATENATION	         1625.970	    0.163	    0.133	  0.004%	 45.991%	     0.000	        1	[densenet201/conv4_block15_concat/concat]:176
	                     MUL	         1626.110	   10.205	   10.167	  0.288%	 46.279%	     0.000	        1	[densenet201/conv4_block16_0_bn/FusedBatchNormV31]:177
	                     ADD	         1636.286	   13.316	   13.217	  0.374%	 46.653%	     0.000	        1	[densenet201/conv4_block16_0_relu/Relu;densenet201/conv4_block16_0_bn/FusedBatchNormV3]:178
	                 CONV_2D	         1649.513	    1.451	    1.458	  0.041%	 46.694%	     0.000	        1	[densenet201/conv4_block16_1_relu/Relu;densenet201/conv4_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block16_1_conv/Conv2D]:179
	                 CONV_2D	         1650.979	    0.665	    0.672	  0.019%	 46.713%	     0.000	        1	[densenet201/conv4_block16_2_conv/Conv2D1]:180
	           CONCATENATION	         1651.659	    0.201	    0.145	  0.004%	 46.717%	     0.000	        1	[densenet201/conv4_block16_concat/concat]:181
	                     MUL	         1651.812	   10.813	   10.601	  0.300%	 47.018%	     0.000	        1	[densenet201/conv4_block17_0_bn/FusedBatchNormV31]:182
	                     ADD	         1662.423	   14.112	   13.758	  0.390%	 47.407%	     0.000	        1	[densenet201/conv4_block17_0_relu/Relu;densenet201/conv4_block17_0_bn/FusedBatchNormV3]:183
	                 CONV_2D	         1676.191	    1.449	    1.382	  0.039%	 47.446%	     0.000	        1	[densenet201/conv4_block17_1_relu/Relu;densenet201/conv4_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block17_1_conv/Conv2D]:184
	                 CONV_2D	         1677.583	    0.740	    0.672	  0.019%	 47.465%	     0.000	        1	[densenet201/conv4_block17_2_conv/Conv2D1]:185
	           CONCATENATION	         1678.263	    0.255	    0.158	  0.004%	 47.470%	     0.000	        1	[densenet201/conv4_block17_concat/concat]:186
	                     MUL	         1678.429	   11.282	   11.053	  0.313%	 47.783%	     0.000	        1	[densenet201/conv4_block18_0_bn/FusedBatchNormV31]:187
	                     ADD	         1689.492	   14.807	   14.390	  0.407%	 48.190%	     0.000	        1	[densenet201/conv4_block18_0_relu/Relu;densenet201/conv4_block18_0_bn/FusedBatchNormV3]:188
	                 CONV_2D	         1703.891	    1.601	    1.559	  0.044%	 48.234%	     0.000	        1	[densenet201/conv4_block18_1_relu/Relu;densenet201/conv4_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block18_1_conv/Conv2D]:189
	                 CONV_2D	         1705.459	    0.737	    0.698	  0.020%	 48.254%	     0.000	        1	[densenet201/conv4_block18_2_conv/Conv2D1]:190
	           CONCATENATION	         1706.165	    0.191	    0.156	  0.004%	 48.258%	     0.000	        1	[densenet201/conv4_block18_concat/concat]:191
	                     MUL	         1706.328	   11.669	   11.521	  0.326%	 48.585%	     0.000	        1	[densenet201/conv4_block19_0_bn/FusedBatchNormV31]:192
	                     ADD	         1717.859	   15.122	   14.966	  0.424%	 49.008%	     0.000	        1	[densenet201/conv4_block19_0_relu/Relu;densenet201/conv4_block19_0_bn/FusedBatchNormV3]:193
	                 CONV_2D	         1732.835	    1.508	    1.480	  0.042%	 49.050%	     0.000	        1	[densenet201/conv4_block19_1_relu/Relu;densenet201/conv4_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block19_1_conv/Conv2D]:194
	                 CONV_2D	         1734.323	    0.755	    0.660	  0.019%	 49.069%	     0.000	        1	[densenet201/conv4_block19_2_conv/Conv2D1]:195
	           CONCATENATION	         1734.992	    0.188	    0.149	  0.004%	 49.073%	     0.000	        1	[densenet201/conv4_block19_concat/concat]:196
	                     MUL	         1735.148	   12.132	   11.945	  0.338%	 49.411%	     0.000	        1	[densenet201/conv4_block20_0_bn/FusedBatchNormV31]:197
	                     ADD	         1747.104	   15.661	   15.515	  0.439%	 49.851%	     0.000	        1	[densenet201/conv4_block20_0_relu/Relu;densenet201/conv4_block20_0_bn/FusedBatchNormV3]:198
	                 CONV_2D	         1762.629	    1.702	    1.674	  0.047%	 49.898%	     0.000	        1	[densenet201/conv4_block20_1_relu/Relu;densenet201/conv4_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block20_1_conv/Conv2D]:199
	                 CONV_2D	         1764.312	    0.698	    0.685	  0.019%	 49.917%	     0.000	        1	[densenet201/conv4_block20_2_conv/Conv2D1]:200
	           CONCATENATION	         1765.005	    0.212	    0.160	  0.005%	 49.922%	     0.000	        1	[densenet201/conv4_block20_concat/concat]:201
	                     MUL	         1765.172	   12.428	   12.351	  0.350%	 50.272%	     0.000	        1	[densenet201/conv4_block21_0_bn/FusedBatchNormV3]:202
	                     ADD	         1777.533	   16.166	   16.045	  0.454%	 50.726%	     0.000	        1	[densenet201/conv4_block21_0_relu/Relu;densenet201/conv4_block21_0_bn/FusedBatchNormV3]:203
	                 CONV_2D	         1793.588	    1.636	    1.598	  0.045%	 50.771%	     0.000	        1	[densenet201/conv4_block21_1_relu/Relu;densenet201/conv4_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block21_1_conv/Conv2D]:204
	                 CONV_2D	         1795.194	    0.698	    0.670	  0.019%	 50.790%	     0.000	        1	[densenet201/conv4_block21_2_conv/Conv2D1]:205
	           CONCATENATION	         1795.872	    0.203	    0.166	  0.005%	 50.795%	     0.000	        1	[densenet201/conv4_block21_concat/concat]:206
	                     MUL	         1796.046	   12.895	   12.829	  0.363%	 51.158%	     0.000	        1	[densenet201/conv4_block22_0_bn/FusedBatchNormV3]:207
	                     ADD	         1808.884	   16.884	   16.651	  0.471%	 51.629%	     0.000	        1	[densenet201/conv4_block22_0_relu/Relu;densenet201/conv4_block22_0_bn/FusedBatchNormV3]:208
	                 CONV_2D	         1825.545	    1.829	    1.788	  0.051%	 51.680%	     0.000	        1	[densenet201/conv4_block22_1_relu/Relu;densenet201/conv4_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block22_1_conv/Conv2D]:209
	                 CONV_2D	         1827.342	    0.791	    0.704	  0.020%	 51.700%	     0.000	        1	[densenet201/conv4_block22_2_conv/Conv2D1]:210
	           CONCATENATION	         1828.054	    0.215	    0.182	  0.005%	 51.705%	     0.000	        1	[densenet201/conv4_block22_concat/concat]:211
	                     MUL	         1828.245	   13.323	   13.253	  0.375%	 52.080%	     0.000	        1	[densenet201/conv4_block23_0_bn/FusedBatchNormV3]:212
	                     ADD	         1841.508	   17.330	   17.222	  0.488%	 52.568%	     0.000	        1	[densenet201/conv4_block23_0_relu/Relu;densenet201/conv4_block23_0_bn/FusedBatchNormV3]:213
	                 CONV_2D	         1858.739	    1.704	    1.713	  0.049%	 52.616%	     0.000	        1	[densenet201/conv4_block23_1_relu/Relu;densenet201/conv4_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block23_1_conv/Conv2D]:214
	                 CONV_2D	         1860.462	    0.727	    0.665	  0.019%	 52.635%	     0.000	        1	[densenet201/conv4_block23_2_conv/Conv2D1]:215
	           CONCATENATION	         1861.135	    0.194	    0.163	  0.005%	 52.640%	     0.000	        1	[densenet201/conv4_block23_concat/concat]:216
	                     MUL	         1861.306	   13.864	   13.684	  0.387%	 53.027%	     0.000	        1	[densenet201/conv4_block24_0_bn/FusedBatchNormV3]:217
	                     ADD	         1874.999	   17.850	   17.765	  0.503%	 53.530%	     0.000	        1	[densenet201/conv4_block24_0_relu/Relu;densenet201/conv4_block24_0_bn/FusedBatchNormV3]:218
	                 CONV_2D	         1892.773	    1.912	    1.875	  0.053%	 53.583%	     0.000	        1	[densenet201/conv4_block24_1_relu/Relu;densenet201/conv4_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block24_1_conv/Conv2D]:219
	                 CONV_2D	         1894.657	    0.701	    0.688	  0.019%	 53.603%	     0.000	        1	[densenet201/conv4_block24_2_conv/Conv2D1]:220
	           CONCATENATION	         1895.354	    0.229	    0.186	  0.005%	 53.608%	     0.000	        1	[densenet201/conv4_block24_concat/concat]:221
	                     MUL	         1895.548	   14.222	   14.127	  0.400%	 54.008%	     0.000	        1	[densenet201/conv4_block25_0_bn/FusedBatchNormV3]:222
	                     ADD	         1909.685	   18.567	   18.480	  0.523%	 54.531%	     0.000	        1	[densenet201/conv4_block25_0_relu/Relu;densenet201/conv4_block25_0_bn/FusedBatchNormV3]:223
	                 CONV_2D	         1928.175	    1.931	    1.804	  0.051%	 54.582%	     0.000	        1	[densenet201/conv4_block25_1_relu/Relu;densenet201/conv4_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block25_1_conv/Conv2D]:224
	                 CONV_2D	         1929.987	    0.700	    0.662	  0.019%	 54.601%	     0.000	        1	[densenet201/conv4_block25_2_conv/Conv2D1]:225
	           CONCATENATION	         1930.657	    0.228	    0.172	  0.005%	 54.606%	     0.000	        1	[densenet201/conv4_block25_concat/concat]:226
	                     MUL	         1930.837	   14.676	   14.569	  0.412%	 55.018%	     0.000	        1	[densenet201/conv4_block26_0_bn/FusedBatchNormV3]:227
	                     ADD	         1945.416	   19.147	   18.921	  0.536%	 55.554%	     0.000	        1	[densenet201/conv4_block26_0_relu/Relu;densenet201/conv4_block26_0_bn/FusedBatchNormV3]:228
	                 CONV_2D	         1964.347	    2.041	    2.002	  0.057%	 55.611%	     0.000	        1	[densenet201/conv4_block26_1_relu/Relu;densenet201/conv4_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block26_1_conv/Conv2D]:229
	                 CONV_2D	         1966.357	    0.714	    0.673	  0.019%	 55.630%	     0.000	        1	[densenet201/conv4_block26_2_conv/Conv2D1]:230
	           CONCATENATION	         1967.038	    0.234	    0.207	  0.006%	 55.636%	     0.000	        1	[densenet201/conv4_block26_concat/concat]:231
	                     MUL	         1967.253	   15.140	   15.034	  0.426%	 56.061%	     0.000	        1	[densenet201/conv4_block27_0_bn/FusedBatchNormV3]:232
	                     ADD	         1982.297	   19.709	   19.470	  0.551%	 56.613%	     0.000	        1	[densenet201/conv4_block27_0_relu/Relu;densenet201/conv4_block27_0_bn/FusedBatchNormV3]:233
	                 CONV_2D	         2001.777	    1.956	    1.928	  0.055%	 56.667%	     0.000	        1	[densenet201/conv4_block27_1_relu/Relu;densenet201/conv4_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block27_1_conv/Conv2D]:234
	                 CONV_2D	         2003.713	    0.772	    0.677	  0.019%	 56.686%	     0.000	        1	[densenet201/conv4_block27_2_conv/Conv2D1]:235
	           CONCATENATION	         2004.398	    0.223	    0.198	  0.006%	 56.692%	     0.000	        1	[densenet201/conv4_block27_concat/concat]:236
	                     MUL	         2004.604	   15.501	   15.437	  0.437%	 57.129%	     0.000	        1	[densenet201/conv4_block28_0_bn/FusedBatchNormV3]:237
	                     ADD	         2020.051	   20.170	   20.058	  0.568%	 57.697%	     0.000	        1	[densenet201/conv4_block28_0_relu/Relu;densenet201/conv4_block28_0_bn/FusedBatchNormV3]:238
	                 CONV_2D	         2040.119	    2.098	    2.122	  0.060%	 57.757%	     0.000	        1	[densenet201/conv4_block28_1_relu/Relu;densenet201/conv4_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block28_1_conv/Conv2D]:239
	                 CONV_2D	         2042.249	    0.680	    0.690	  0.020%	 57.777%	     0.000	        1	[densenet201/conv4_block28_2_conv/Conv2D1]:240
	           CONCATENATION	         2042.947	    0.189	    0.177	  0.005%	 57.782%	     0.000	        1	[densenet201/conv4_block28_concat/concat]:241
	                     MUL	         2043.132	   16.065	   15.918	  0.451%	 58.232%	     0.000	        1	[densenet201/conv4_block29_0_bn/FusedBatchNormV3]:242
	                     ADD	         2059.060	   20.914	   20.676	  0.585%	 58.818%	     0.000	        1	[densenet201/conv4_block29_0_relu/Relu;densenet201/conv4_block29_0_bn/FusedBatchNormV3]:243
	                 CONV_2D	         2079.747	    2.006	    2.015	  0.057%	 58.875%	     0.000	        1	[densenet201/conv4_block29_1_relu/Relu;densenet201/conv4_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block29_1_conv/Conv2D]:244
	                 CONV_2D	         2081.770	    0.749	    0.684	  0.019%	 58.894%	     0.000	        1	[densenet201/conv4_block29_2_conv/Conv2D1]:245
	           CONCATENATION	         2082.463	    0.189	    0.161	  0.005%	 58.899%	     0.000	        1	[densenet201/conv4_block29_concat/concat]:246
	                     MUL	         2082.632	   16.460	   16.324	  0.462%	 59.361%	     0.000	        1	[densenet201/conv4_block30_0_bn/FusedBatchNormV3]:247
	                     ADD	         2098.966	   21.422	   21.223	  0.601%	 59.962%	     0.000	        1	[densenet201/conv4_block30_0_relu/Relu;densenet201/conv4_block30_0_bn/FusedBatchNormV3]:248
	                 CONV_2D	         2120.198	    2.225	    2.220	  0.063%	 60.024%	     0.000	        1	[densenet201/conv4_block30_1_relu/Relu;densenet201/conv4_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block30_1_conv/Conv2D]:249
	                 CONV_2D	         2122.426	    0.744	    0.682	  0.019%	 60.044%	     0.000	        1	[densenet201/conv4_block30_2_conv/Conv2D1]:250
	           CONCATENATION	         2123.117	    0.210	    0.186	  0.005%	 60.049%	     0.000	        1	[densenet201/conv4_block30_concat/concat]:251
	                     MUL	         2123.310	   16.840	   16.777	  0.475%	 60.524%	     0.000	        1	[densenet201/conv4_block31_0_bn/FusedBatchNormV3]:252
	                     ADD	         2140.097	   21.873	   21.753	  0.616%	 61.140%	     0.000	        1	[densenet201/conv4_block31_0_relu/Relu;densenet201/conv4_block31_0_bn/FusedBatchNormV3]:253
	                 CONV_2D	         2161.860	    2.143	    2.123	  0.060%	 61.200%	     0.000	        1	[densenet201/conv4_block31_1_relu/Relu;densenet201/conv4_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block31_1_conv/Conv2D]:254
	                 CONV_2D	         2163.992	    0.684	    0.684	  0.019%	 61.219%	     0.000	        1	[densenet201/conv4_block31_2_conv/Conv2D1]:255
	           CONCATENATION	         2164.685	    0.189	    0.171	  0.005%	 61.224%	     0.000	        1	[densenet201/conv4_block31_concat/concat]:256
	                     MUL	         2164.864	   17.303	   17.206	  0.487%	 61.711%	     0.000	        1	[densenet201/conv4_block32_0_bn/FusedBatchNormV3]:257
	                     ADD	         2182.080	   22.421	   22.371	  0.633%	 62.345%	     0.000	        1	[densenet201/conv4_block32_0_relu/Relu;densenet201/conv4_block32_0_bn/FusedBatchNormV3]:258
	                 CONV_2D	         2204.461	    2.356	    2.328	  0.066%	 62.411%	     0.000	        1	[densenet201/conv4_block32_1_relu/Relu;densenet201/conv4_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block32_1_conv/Conv2D]:259
	                 CONV_2D	         2206.797	    0.661	    0.689	  0.019%	 62.430%	     0.000	        1	[densenet201/conv4_block32_2_conv/Conv2D1]:260
	           CONCATENATION	         2207.494	    0.182	    0.196	  0.006%	 62.436%	     0.000	        1	[densenet201/conv4_block32_concat/concat]:261
	                     MUL	         2207.698	   17.528	   17.631	  0.499%	 62.935%	     0.000	        1	[densenet201/conv4_block33_0_bn/FusedBatchNormV3]:262
	                     ADD	         2225.339	   22.755	   22.968	  0.650%	 63.585%	     0.000	        1	[densenet201/conv4_block33_0_relu/Relu;densenet201/conv4_block33_0_bn/FusedBatchNormV3]:263
	                 CONV_2D	         2248.317	    2.191	    2.220	  0.063%	 63.648%	     0.000	        1	[densenet201/conv4_block33_1_relu/Relu;densenet201/conv4_block33_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block33_1_conv/Conv2D]:264
	                 CONV_2D	         2250.545	    0.687	    0.675	  0.019%	 63.667%	     0.000	        1	[densenet201/conv4_block33_2_conv/Conv2D1]:265
	           CONCATENATION	         2251.228	    0.157	    0.175	  0.005%	 63.672%	     0.000	        1	[densenet201/conv4_block33_concat/concat]:266
	                     MUL	         2251.412	   18.086	   18.135	  0.513%	 64.185%	     0.000	        1	[densenet201/conv4_block34_0_bn/FusedBatchNormV3]:267
	                     ADD	         2269.557	   23.430	   23.541	  0.666%	 64.852%	     0.000	        1	[densenet201/conv4_block34_0_relu/Relu;densenet201/conv4_block34_0_bn/FusedBatchNormV3]:268
	                 CONV_2D	         2293.109	    2.459	    2.575	  0.073%	 64.925%	     0.000	        1	[densenet201/conv4_block34_1_relu/Relu;densenet201/conv4_block34_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block34_1_conv/Conv2D]:269
	                 CONV_2D	         2295.692	    0.655	    0.696	  0.020%	 64.945%	     0.000	        1	[densenet201/conv4_block34_2_conv/Conv2D1]:270
	           CONCATENATION	         2296.396	    0.186	    0.213	  0.006%	 64.951%	     0.000	        1	[densenet201/conv4_block34_concat/concat]:271
	                     MUL	         2296.618	   18.417	   18.526	  0.525%	 65.475%	     0.000	        1	[densenet201/conv4_block35_0_bn/FusedBatchNormV3]:272
	                     ADD	         2315.153	   23.920	   24.047	  0.681%	 66.156%	     0.000	        1	[densenet201/conv4_block35_0_relu/Relu;densenet201/conv4_block35_0_bn/FusedBatchNormV3]:273
	                 CONV_2D	         2339.210	    2.299	    2.335	  0.066%	 66.222%	     0.000	        1	[densenet201/conv4_block35_1_relu/Relu;densenet201/conv4_block35_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block35_1_conv/Conv2D]:274
	                 CONV_2D	         2341.554	    0.656	    0.686	  0.019%	 66.241%	     0.000	        1	[densenet201/conv4_block35_2_conv/Conv2D1]:275
	           CONCATENATION	         2342.247	    0.172	    0.203	  0.006%	 66.247%	     0.000	        1	[densenet201/conv4_block35_concat/concat]:276
	                     MUL	         2342.457	   18.832	   19.028	  0.539%	 66.786%	     0.000	        1	[densenet201/conv4_block36_0_bn/FusedBatchNormV3]:277
	                     ADD	         2361.495	   24.494	   24.643	  0.698%	 67.484%	     0.000	        1	[densenet201/conv4_block36_0_relu/Relu;densenet201/conv4_block36_0_bn/FusedBatchNormV3]:278
	                 CONV_2D	         2386.148	    2.540	    2.542	  0.072%	 67.556%	     0.000	        1	[densenet201/conv4_block36_1_relu/Relu;densenet201/conv4_block36_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block36_1_conv/Conv2D]:279
	                 CONV_2D	         2388.700	    0.658	    0.694	  0.020%	 67.575%	     0.000	        1	[densenet201/conv4_block36_2_conv/Conv2D1]:280
	           CONCATENATION	         2389.404	    0.217	    0.232	  0.007%	 67.582%	     0.000	        1	[densenet201/conv4_block36_concat/concat]:281
	                     MUL	         2389.644	   19.266	   19.437	  0.550%	 68.132%	     0.000	        1	[densenet201/conv4_block37_0_bn/FusedBatchNormV3]:282
	                     ADD	         2409.090	   25.053	   25.325	  0.717%	 68.849%	     0.000	        1	[densenet201/conv4_block37_0_relu/Relu;densenet201/conv4_block37_0_bn/FusedBatchNormV3]:283
	                 CONV_2D	         2434.426	    2.398	    2.429	  0.069%	 68.918%	     0.000	        1	[densenet201/conv4_block37_1_relu/Relu;densenet201/conv4_block37_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block37_1_conv/Conv2D]:284
	                 CONV_2D	         2436.863	    0.660	    0.684	  0.019%	 68.937%	     0.000	        1	[densenet201/conv4_block37_2_conv/Conv2D1]:285
	           CONCATENATION	         2437.555	    0.189	    0.200	  0.006%	 68.943%	     0.000	        1	[densenet201/conv4_block37_concat/concat]:286
	                     MUL	         2437.762	   19.693	   19.870	  0.563%	 69.506%	     0.000	        1	[densenet201/conv4_block38_0_bn/FusedBatchNormV3]:287
	                     ADD	         2457.642	   25.790	   25.785	  0.730%	 70.236%	     0.000	        1	[densenet201/conv4_block38_0_relu/Relu;densenet201/conv4_block38_0_bn/FusedBatchNormV3]:288
	                 CONV_2D	         2483.438	    2.646	    2.654	  0.075%	 70.311%	     0.000	        1	[densenet201/conv4_block38_1_relu/Relu;densenet201/conv4_block38_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block38_1_conv/Conv2D]:289
	                 CONV_2D	         2486.100	    0.668	    0.692	  0.020%	 70.330%	     0.000	        1	[densenet201/conv4_block38_2_conv/Conv2D1]:290
	           CONCATENATION	         2486.800	    0.229	    0.241	  0.007%	 70.337%	     0.000	        1	[densenet201/conv4_block38_concat/concat]:291
	                     MUL	         2487.050	   20.214	   20.272	  0.574%	 70.911%	     0.000	        1	[densenet201/conv4_block39_0_bn/FusedBatchNormV3]:292
	                     ADD	         2507.332	   26.226	   26.393	  0.747%	 71.658%	     0.000	        1	[densenet201/conv4_block39_0_relu/Relu;densenet201/conv4_block39_0_bn/FusedBatchNormV3]:293
	                 CONV_2D	         2533.736	    2.511	    2.541	  0.072%	 71.730%	     0.000	        1	[densenet201/conv4_block39_1_relu/Relu;densenet201/conv4_block39_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block39_1_conv/Conv2D]:294
	                 CONV_2D	         2536.285	    0.673	    0.700	  0.020%	 71.750%	     0.000	        1	[densenet201/conv4_block39_2_conv/Conv2D1]:295
	           CONCATENATION	         2536.993	    0.271	    0.221	  0.006%	 71.756%	     0.000	        1	[densenet201/conv4_block39_concat/concat]:296
	                     MUL	         2537.222	   20.607	   20.738	  0.587%	 72.343%	     0.000	        1	[densenet201/conv4_block40_0_bn/FusedBatchNormV3]:297
	                     ADD	         2557.970	   26.893	   26.949	  0.763%	 73.106%	     0.000	        1	[densenet201/conv4_block40_0_relu/Relu;densenet201/conv4_block40_0_bn/FusedBatchNormV3]:298
	                 CONV_2D	         2584.929	    2.761	    2.805	  0.079%	 73.186%	     0.000	        1	[densenet201/conv4_block40_1_relu/Relu;densenet201/conv4_block40_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block40_1_conv/Conv2D]:299
	                 CONV_2D	         2587.744	    0.663	    0.698	  0.020%	 73.206%	     0.000	        1	[densenet201/conv4_block40_2_conv/Conv2D1]:300
	           CONCATENATION	         2588.451	    0.240	    0.253	  0.007%	 73.213%	     0.000	        1	[densenet201/conv4_block40_concat/concat]:301
	                     MUL	         2588.713	   21.051	   21.154	  0.599%	 73.812%	     0.000	        1	[densenet201/conv4_block41_0_bn/FusedBatchNormV3]:302
	                     ADD	         2609.877	   27.443	   27.512	  0.779%	 74.591%	     0.000	        1	[densenet201/conv4_block41_0_relu/Relu;densenet201/conv4_block41_0_bn/FusedBatchNormV3]:303
	                 CONV_2D	         2637.400	    2.625	    2.634	  0.075%	 74.665%	     0.000	        1	[densenet201/conv4_block41_1_relu/Relu;densenet201/conv4_block41_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block41_1_conv/Conv2D]:304
	                 CONV_2D	         2640.043	    0.643	    0.704	  0.020%	 74.685%	     0.000	        1	[densenet201/conv4_block41_2_conv/Conv2D1]:305
	           CONCATENATION	         2640.754	    0.231	    0.231	  0.007%	 74.692%	     0.000	        1	[densenet201/conv4_block41_concat/concat]:306
	                     MUL	         2640.993	   21.528	   21.616	  0.612%	 75.304%	     0.000	        1	[densenet201/conv4_block42_0_bn/FusedBatchNormV3]:307
	                     ADD	         2662.619	   28.092	   28.217	  0.799%	 76.102%	     0.000	        1	[densenet201/conv4_block42_0_relu/Relu;densenet201/conv4_block42_0_bn/FusedBatchNormV3]:308
	                 CONV_2D	         2690.847	    2.848	    2.891	  0.082%	 76.184%	     0.000	        1	[densenet201/conv4_block42_1_relu/Relu;densenet201/conv4_block42_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block42_1_conv/Conv2D]:309
	                 CONV_2D	         2693.746	    0.688	    0.702	  0.020%	 76.204%	     0.000	        1	[densenet201/conv4_block42_2_conv/Conv2D1]:310
	           CONCATENATION	         2694.456	    0.264	    0.270	  0.008%	 76.212%	     0.000	        1	[densenet201/conv4_block42_concat/concat]:311
	                     MUL	         2694.734	   21.956	   22.059	  0.625%	 76.836%	     0.000	        1	[densenet201/conv4_block43_0_bn/FusedBatchNormV3]:312
	                     ADD	         2716.803	   28.508	   28.610	  0.810%	 77.646%	     0.000	        1	[densenet201/conv4_block43_0_relu/Relu;densenet201/conv4_block43_0_bn/FusedBatchNormV3]:313
	                 CONV_2D	         2745.423	    2.728	    2.753	  0.078%	 77.724%	     0.000	        1	[densenet201/conv4_block43_1_relu/Relu;densenet201/conv4_block43_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block43_1_conv/Conv2D]:314
	                 CONV_2D	         2748.184	    0.674	    0.707	  0.020%	 77.744%	     0.000	        1	[densenet201/conv4_block43_2_conv/Conv2D1]:315
	           CONCATENATION	         2748.898	    0.256	    0.249	  0.007%	 77.751%	     0.000	        1	[densenet201/conv4_block43_concat/concat]:316
	                     MUL	         2749.156	   22.394	   22.465	  0.636%	 78.387%	     0.000	        1	[densenet201/conv4_block44_0_bn/FusedBatchNormV3]:317
	                     ADD	         2771.631	   29.090	   29.209	  0.827%	 79.214%	     0.000	        1	[densenet201/conv4_block44_0_relu/Relu;densenet201/conv4_block44_0_bn/FusedBatchNormV3]:318
	                 CONV_2D	         2800.850	    2.987	    2.991	  0.085%	 79.299%	     0.000	        1	[densenet201/conv4_block44_1_relu/Relu;densenet201/conv4_block44_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block44_1_conv/Conv2D]:319
	                 CONV_2D	         2803.851	    0.677	    0.695	  0.020%	 79.319%	     0.000	        1	[densenet201/conv4_block44_2_conv/Conv2D1]:320
	           CONCATENATION	         2804.554	    0.270	    0.278	  0.008%	 79.327%	     0.000	        1	[densenet201/conv4_block44_concat/concat]:321
	                     MUL	         2804.840	   22.930	   22.939	  0.649%	 79.976%	     0.000	        1	[densenet201/conv4_block45_0_bn/FusedBatchNormV3]:322
	                     ADD	         2827.789	   29.572	   29.828	  0.844%	 80.820%	     0.000	        1	[densenet201/conv4_block45_0_relu/Relu;densenet201/conv4_block45_0_bn/FusedBatchNormV3]:323
	                 CONV_2D	         2857.628	    2.840	    2.849	  0.081%	 80.901%	     0.000	        1	[densenet201/conv4_block45_1_relu/Relu;densenet201/conv4_block45_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block45_1_conv/Conv2D]:324
	                 CONV_2D	         2860.485	    0.642	    0.700	  0.020%	 80.921%	     0.000	        1	[densenet201/conv4_block45_2_conv/Conv2D1]:325
	           CONCATENATION	         2861.193	    0.268	    0.269	  0.008%	 80.929%	     0.000	        1	[densenet201/conv4_block45_concat/concat]:326
	                     MUL	         2861.472	   23.347	   23.417	  0.663%	 81.592%	     0.000	        1	[densenet201/conv4_block46_0_bn/FusedBatchNormV3]:327
	                     ADD	         2884.899	   30.411	   30.362	  0.860%	 82.451%	     0.000	        1	[densenet201/conv4_block46_0_relu/Relu;densenet201/conv4_block46_0_bn/FusedBatchNormV3]:328
	                 CONV_2D	         2915.271	    3.111	    3.115	  0.088%	 82.539%	     0.000	        1	[densenet201/conv4_block46_1_relu/Relu;densenet201/conv4_block46_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block46_1_conv/Conv2D]:329
	                 CONV_2D	         2918.396	    0.760	    0.714	  0.020%	 82.560%	     0.000	        1	[densenet201/conv4_block46_2_conv/Conv2D1]:330
	           CONCATENATION	         2919.118	    0.270	    0.318	  0.009%	 82.569%	     0.000	        1	[densenet201/conv4_block46_concat/concat]:331
	                     MUL	         2919.444	   23.990	   23.931	  0.678%	 83.246%	     0.000	        1	[densenet201/conv4_block47_0_bn/FusedBatchNormV3]:332
	                     ADD	         2943.385	   30.885	   31.019	  0.878%	 84.124%	     0.000	        1	[densenet201/conv4_block47_0_relu/Relu;densenet201/conv4_block47_0_bn/FusedBatchNormV3]:333
	                 CONV_2D	         2974.415	    2.930	    2.944	  0.083%	 84.208%	     0.000	        1	[densenet201/conv4_block47_1_relu/Relu;densenet201/conv4_block47_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block47_1_conv/Conv2D]:334
	                 CONV_2D	         2977.368	    0.674	    0.709	  0.020%	 84.228%	     0.000	        1	[densenet201/conv4_block47_2_conv/Conv2D1]:335
	           CONCATENATION	         2978.085	    0.212	    0.281	  0.008%	 84.236%	     0.000	        1	[densenet201/conv4_block47_concat/concat]:336
	                     MUL	         2978.374	   24.118	   24.247	  0.686%	 84.922%	     0.000	        1	[densenet201/conv4_block48_0_bn/FusedBatchNormV3]:337
	                     ADD	         3002.631	   31.390	   31.508	  0.892%	 85.814%	     0.000	        1	[densenet201/conv4_block48_0_relu/Relu;densenet201/conv4_block48_0_bn/FusedBatchNormV3]:338
	                 CONV_2D	         3034.149	    3.329	    3.226	  0.091%	 85.906%	     0.000	        1	[densenet201/conv4_block48_1_relu/Relu;densenet201/conv4_block48_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block48_1_conv/Conv2D]:339
	                 CONV_2D	         3037.384	    0.702	    0.719	  0.020%	 85.926%	     0.000	        1	[densenet201/conv4_block48_2_conv/Conv2D1]:340
	           CONCATENATION	         3038.112	    0.241	    0.321	  0.009%	 85.935%	     0.000	        1	[densenet201/conv4_block48_concat/concat]:341
	                     MUL	         3038.441	   24.557	   24.663	  0.698%	 86.633%	     0.000	        1	[densenet201/pool4_bn/FusedBatchNormV3]:342
	                     ADD	         3063.115	   31.976	   32.106	  0.909%	 87.542%	     0.000	        1	[densenet201/pool4_relu/Relu;densenet201/pool4_bn/FusedBatchNormV3]:343
	                 CONV_2D	         3095.232	   19.904	   20.017	  0.567%	 88.109%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	         AVERAGE_POOL_2D	         3115.258	    4.520	    4.563	  0.129%	 88.238%	     0.000	        1	[densenet201/pool4_pool/AvgPool]:345
	                     MUL	         3119.830	    3.066	    3.105	  0.088%	 88.326%	     0.000	        1	[densenet201/conv5_block1_0_bn/FusedBatchNormV31]:346
	                     ADD	         3122.942	    3.983	    4.019	  0.114%	 88.440%	     0.000	        1	[densenet201/conv5_block1_0_relu/Relu;densenet201/conv5_block1_0_bn/FusedBatchNormV3]:347
	                 CONV_2D	         3126.971	    0.437	    0.458	  0.013%	 88.453%	     0.000	        1	[densenet201/conv5_block1_1_relu/Relu;densenet201/conv5_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block1_1_conv/Conv2D]:348
	                 CONV_2D	         3127.436	    0.197	    0.196	  0.006%	 88.458%	     0.000	        1	[densenet201/conv5_block1_2_conv/Conv2D1]:349
	           CONCATENATION	         3127.640	    0.039	    0.046	  0.001%	 88.460%	     0.000	        1	[densenet201/conv5_block1_concat/concat]:350
	                     MUL	         3127.693	    3.175	    3.218	  0.091%	 88.551%	     0.000	        1	[densenet201/conv5_block2_0_bn/FusedBatchNormV31]:351
	                     ADD	         3130.919	    4.158	    4.178	  0.118%	 88.669%	     0.000	        1	[densenet201/conv5_block2_0_relu/Relu;densenet201/conv5_block2_0_bn/FusedBatchNormV3]:352
	                 CONV_2D	         3135.105	    0.479	    0.500	  0.014%	 88.683%	     0.000	        1	[densenet201/conv5_block2_1_relu/Relu;densenet201/conv5_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block2_1_conv/Conv2D]:353
	                 CONV_2D	         3135.612	    0.197	    0.193	  0.005%	 88.689%	     0.000	        1	[densenet201/conv5_block2_2_conv/Conv2D1]:354
	           CONCATENATION	         3135.812	    0.028	    0.038	  0.001%	 88.690%	     0.000	        1	[densenet201/conv5_block2_concat/concat]:355
	                     MUL	         3135.857	    3.276	    3.322	  0.094%	 88.784%	     0.000	        1	[densenet201/conv5_block3_0_bn/FusedBatchNormV31]:356
	                     ADD	         3139.186	    4.262	    4.304	  0.122%	 88.906%	     0.000	        1	[densenet201/conv5_block3_0_relu/Relu;densenet201/conv5_block3_0_bn/FusedBatchNormV3]:357
	                 CONV_2D	         3143.499	    0.468	    0.475	  0.013%	 88.919%	     0.000	        1	[densenet201/conv5_block3_1_relu/Relu;densenet201/conv5_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block3_1_conv/Conv2D]:358
	                 CONV_2D	         3143.982	    0.185	    0.191	  0.005%	 88.925%	     0.000	        1	[densenet201/conv5_block3_2_conv/Conv2D1]:359
	           CONCATENATION	         3144.180	    0.025	    0.044	  0.001%	 88.926%	     0.000	        1	[densenet201/conv5_block3_concat/concat]:360
	                     MUL	         3144.230	    3.391	    3.426	  0.097%	 89.023%	     0.000	        1	[densenet201/conv5_block4_0_bn/FusedBatchNormV31]:361
	                     ADD	         3147.665	    4.414	    4.448	  0.126%	 89.149%	     0.000	        1	[densenet201/conv5_block4_0_relu/Relu;densenet201/conv5_block4_0_bn/FusedBatchNormV3]:362
	                 CONV_2D	         3152.123	    0.494	    0.525	  0.015%	 89.164%	     0.000	        1	[densenet201/conv5_block4_1_relu/Relu;densenet201/conv5_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block4_1_conv/Conv2D]:363
	                 CONV_2D	         3152.657	    0.184	    0.195	  0.006%	 89.169%	     0.000	        1	[densenet201/conv5_block4_2_conv/Conv2D1]:364
	           CONCATENATION	         3152.859	    0.031	    0.040	  0.001%	 89.170%	     0.000	        1	[densenet201/conv5_block4_concat/concat]:365
	                     MUL	         3152.905	    3.509	    3.540	  0.100%	 89.270%	     0.000	        1	[densenet201/conv5_block5_0_bn/FusedBatchNormV31]:366
	                     ADD	         3156.453	    4.558	    4.598	  0.130%	 89.401%	     0.000	        1	[densenet201/conv5_block5_0_relu/Relu;densenet201/conv5_block5_0_bn/FusedBatchNormV3]:367
	                 CONV_2D	         3161.060	    0.480	    0.494	  0.014%	 89.415%	     0.000	        1	[densenet201/conv5_block5_1_relu/Relu;densenet201/conv5_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block5_1_conv/Conv2D]:368
	                 CONV_2D	         3161.560	    0.172	    0.188	  0.005%	 89.420%	     0.000	        1	[densenet201/conv5_block5_2_conv/Conv2D1]:369
	           CONCATENATION	         3161.755	    0.026	    0.034	  0.001%	 89.421%	     0.000	        1	[densenet201/conv5_block5_concat/concat]:370
	                     MUL	         3161.796	    3.634	    3.646	  0.103%	 89.524%	     0.000	        1	[densenet201/conv5_block6_0_bn/FusedBatchNormV31]:371
	                     ADD	         3165.450	    4.697	    4.733	  0.134%	 89.658%	     0.000	        1	[densenet201/conv5_block6_0_relu/Relu;densenet201/conv5_block6_0_bn/FusedBatchNormV3]:372
	                 CONV_2D	         3170.192	    0.527	    0.554	  0.016%	 89.674%	     0.000	        1	[densenet201/conv5_block6_1_relu/Relu;densenet201/conv5_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block6_1_conv/Conv2D]:373
	                 CONV_2D	         3170.753	    0.184	    0.187	  0.005%	 89.679%	     0.000	        1	[densenet201/conv5_block6_2_conv/Conv2D1]:374
	           CONCATENATION	         3170.947	    0.031	    0.041	  0.001%	 89.680%	     0.000	        1	[densenet201/conv5_block6_concat/concat]:375
	                     MUL	         3170.995	    3.733	    3.768	  0.107%	 89.787%	     0.000	        1	[densenet201/conv5_block7_0_bn/FusedBatchNormV31]:376
	                     ADD	         3174.771	    4.834	    4.905	  0.139%	 89.926%	     0.000	        1	[densenet201/conv5_block7_0_relu/Relu;densenet201/conv5_block7_0_bn/FusedBatchNormV3]:377
	                 CONV_2D	         3179.684	    0.514	    0.538	  0.015%	 89.941%	     0.000	        1	[densenet201/conv5_block7_1_relu/Relu;densenet201/conv5_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block7_1_conv/Conv2D]:378
	                 CONV_2D	         3180.229	    0.204	    0.195	  0.006%	 89.947%	     0.000	        1	[densenet201/conv5_block7_2_conv/Conv2D1]:379
	           CONCATENATION	         3180.432	    0.031	    0.043	  0.001%	 89.948%	     0.000	        1	[densenet201/conv5_block7_concat/concat]:380
	                     MUL	         3180.481	    3.842	    3.857	  0.109%	 90.057%	     0.000	        1	[densenet201/conv5_block8_0_bn/FusedBatchNormV31]:381
	                     ADD	         3184.347	    4.976	    5.007	  0.142%	 90.199%	     0.000	        1	[densenet201/conv5_block8_0_relu/Relu;densenet201/conv5_block8_0_bn/FusedBatchNormV3]:382
	                 CONV_2D	         3189.362	    0.552	    0.580	  0.016%	 90.215%	     0.000	        1	[densenet201/conv5_block8_1_relu/Relu;densenet201/conv5_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block8_1_conv/Conv2D]:383
	                 CONV_2D	         3189.949	    0.182	    0.194	  0.005%	 90.221%	     0.000	        1	[densenet201/conv5_block8_2_conv/Conv2D1]:384
	           CONCATENATION	         3190.149	    0.028	    0.035	  0.001%	 90.222%	     0.000	        1	[densenet201/conv5_block8_concat/concat]:385
	                     MUL	         3190.192	    3.950	    3.976	  0.113%	 90.334%	     0.000	        1	[densenet201/conv5_block9_0_bn/FusedBatchNormV31]:386
	                     ADD	         3194.177	    5.113	    5.170	  0.146%	 90.481%	     0.000	        1	[densenet201/conv5_block9_0_relu/Relu;densenet201/conv5_block9_0_bn/FusedBatchNormV3]:387
	                 CONV_2D	         3199.354	    0.533	    0.559	  0.016%	 90.496%	     0.000	        1	[densenet201/conv5_block9_1_relu/Relu;densenet201/conv5_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block9_1_conv/Conv2D]:388
	                 CONV_2D	         3199.921	    0.181	    0.186	  0.005%	 90.502%	     0.000	        1	[densenet201/conv5_block9_2_conv/Conv2D1]:389
	           CONCATENATION	         3200.114	    0.027	    0.041	  0.001%	 90.503%	     0.000	        1	[densenet201/conv5_block9_concat/concat]:390
	                     MUL	         3200.161	    4.048	    4.082	  0.116%	 90.618%	     0.000	        1	[densenet201/conv5_block10_0_bn/FusedBatchNormV31]:391
	                     ADD	         3204.251	    5.272	    5.309	  0.150%	 90.769%	     0.000	        1	[densenet201/conv5_block10_0_relu/Relu;densenet201/conv5_block10_0_bn/FusedBatchNormV3]:392
	                 CONV_2D	         3209.568	    0.586	    0.609	  0.017%	 90.786%	     0.000	        1	[densenet201/conv5_block10_1_relu/Relu;densenet201/conv5_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block10_1_conv/Conv2D]:393
	                 CONV_2D	         3210.185	    0.173	    0.193	  0.005%	 90.791%	     0.000	        1	[densenet201/conv5_block10_2_conv/Conv2D1]:394
	           CONCATENATION	         3210.385	    0.039	    0.043	  0.001%	 90.793%	     0.000	        1	[densenet201/conv5_block10_concat/concat]:395
	                     MUL	         3210.435	    4.160	    4.197	  0.119%	 90.911%	     0.000	        1	[densenet201/conv5_block11_0_bn/FusedBatchNormV31]:396
	                     ADD	         3214.640	    5.397	    5.447	  0.154%	 91.066%	     0.000	        1	[densenet201/conv5_block11_0_relu/Relu;densenet201/conv5_block11_0_bn/FusedBatchNormV3]:397
	                 CONV_2D	         3220.095	    0.558	    0.582	  0.016%	 91.082%	     0.000	        1	[densenet201/conv5_block11_1_relu/Relu;densenet201/conv5_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block11_1_conv/Conv2D]:398
	                 CONV_2D	         3220.685	    0.195	    0.197	  0.006%	 91.088%	     0.000	        1	[densenet201/conv5_block11_2_conv/Conv2D1]:399
	           CONCATENATION	         3220.889	    0.028	    0.037	  0.001%	 91.089%	     0.000	        1	[densenet201/conv5_block11_concat/concat]:400
	                     MUL	         3220.933	    4.278	    4.307	  0.122%	 91.211%	     0.000	        1	[densenet201/conv5_block12_0_bn/FusedBatchNormV31]:401
	                     ADD	         3225.248	    5.546	    5.599	  0.159%	 91.369%	     0.000	        1	[densenet201/conv5_block12_0_relu/Relu;densenet201/conv5_block12_0_bn/FusedBatchNormV3]:402
	                 CONV_2D	         3230.855	    0.605	    0.639	  0.018%	 91.387%	     0.000	        1	[densenet201/conv5_block12_1_relu/Relu;densenet201/conv5_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block12_1_conv/Conv2D]:403
	                 CONV_2D	         3231.502	    0.185	    0.199	  0.006%	 91.393%	     0.000	        1	[densenet201/conv5_block12_2_conv/Conv2D1]:404
	           CONCATENATION	         3231.708	    0.027	    0.043	  0.001%	 91.394%	     0.000	        1	[densenet201/conv5_block12_concat/concat]:405
	                     MUL	         3231.758	    4.371	    4.416	  0.125%	 91.519%	     0.000	        1	[densenet201/conv5_block13_0_bn/FusedBatchNormV31]:406
	                     ADD	         3236.182	    5.696	    5.739	  0.162%	 91.682%	     0.000	        1	[densenet201/conv5_block13_0_relu/Relu;densenet201/conv5_block13_0_bn/FusedBatchNormV3]:407
	                 CONV_2D	         3241.929	    0.594	    0.617	  0.017%	 91.699%	     0.000	        1	[densenet201/conv5_block13_1_relu/Relu;densenet201/conv5_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block13_1_conv/Conv2D]:408
	                 CONV_2D	         3242.553	    0.171	    0.193	  0.005%	 91.705%	     0.000	        1	[densenet201/conv5_block13_2_conv/Conv2D1]:409
	           CONCATENATION	         3242.753	    0.029	    0.042	  0.001%	 91.706%	     0.000	        1	[densenet201/conv5_block13_concat/concat]:410
	                     MUL	         3242.802	    4.487	    4.521	  0.128%	 91.834%	     0.000	        1	[densenet201/conv5_block14_0_bn/FusedBatchNormV31]:411
	                     ADD	         3247.331	    5.839	    5.873	  0.166%	 92.000%	     0.000	        1	[densenet201/conv5_block14_0_relu/Relu;densenet201/conv5_block14_0_bn/FusedBatchNormV3]:412
	                 CONV_2D	         3253.212	    0.633	    0.670	  0.019%	 92.019%	     0.000	        1	[densenet201/conv5_block14_1_relu/Relu;densenet201/conv5_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block14_1_conv/Conv2D]:413
	                 CONV_2D	         3253.889	    0.183	    0.198	  0.006%	 92.025%	     0.000	        1	[densenet201/conv5_block14_2_conv/Conv2D1]:414
	           CONCATENATION	         3254.094	    0.028	    0.044	  0.001%	 92.026%	     0.000	        1	[densenet201/conv5_block14_concat/concat]:415
	                     MUL	         3254.145	    4.606	    4.643	  0.131%	 92.157%	     0.000	        1	[densenet201/conv5_block15_0_bn/FusedBatchNormV31]:416
	                     ADD	         3258.796	    5.959	    6.015	  0.170%	 92.328%	     0.000	        1	[densenet201/conv5_block15_0_relu/Relu;densenet201/conv5_block15_0_bn/FusedBatchNormV3]:417
	                 CONV_2D	         3264.820	    0.617	    0.647	  0.018%	 92.346%	     0.000	        1	[densenet201/conv5_block15_1_relu/Relu;densenet201/conv5_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block15_1_conv/Conv2D]:418
	                 CONV_2D	         3265.475	    0.184	    0.201	  0.006%	 92.352%	     0.000	        1	[densenet201/conv5_block15_2_conv/Conv2D1]:419
	           CONCATENATION	         3265.684	    0.029	    0.045	  0.001%	 92.353%	     0.000	        1	[densenet201/conv5_block15_concat/concat]:420
	                     MUL	         3265.735	    4.700	    4.747	  0.134%	 92.487%	     0.000	        1	[densenet201/conv5_block16_0_bn/FusedBatchNormV31]:421
	                     ADD	         3270.491	    6.101	    6.167	  0.175%	 92.662%	     0.000	        1	[densenet201/conv5_block16_0_relu/Relu;densenet201/conv5_block16_0_bn/FusedBatchNormV3]:422
	                 CONV_2D	         3276.667	    0.675	    0.738	  0.021%	 92.683%	     0.000	        1	[densenet201/conv5_block16_1_relu/Relu;densenet201/conv5_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block16_1_conv/Conv2D]:423
	                 CONV_2D	         3277.413	    0.195	    0.201	  0.006%	 92.688%	     0.000	        1	[densenet201/conv5_block16_2_conv/Conv2D1]:424
	           CONCATENATION	         3277.622	    0.035	    0.049	  0.001%	 92.690%	     0.000	        1	[densenet201/conv5_block16_concat/concat]:425
	                     MUL	         3277.677	    4.812	    4.857	  0.138%	 92.827%	     0.000	        1	[densenet201/conv5_block17_0_bn/FusedBatchNormV31]:426
	                     ADD	         3282.543	    6.309	    6.328	  0.179%	 93.007%	     0.000	        1	[densenet201/conv5_block17_0_relu/Relu;densenet201/conv5_block17_0_bn/FusedBatchNormV3]:427
	                 CONV_2D	         3288.880	    0.657	    0.669	  0.019%	 93.025%	     0.000	        1	[densenet201/conv5_block17_1_relu/Relu;densenet201/conv5_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block17_1_conv/Conv2D]:428
	                 CONV_2D	         3289.556	    0.209	    0.196	  0.006%	 93.031%	     0.000	        1	[densenet201/conv5_block17_2_conv/Conv2D1]:429
	           CONCATENATION	         3289.759	    0.037	    0.046	  0.001%	 93.032%	     0.000	        1	[densenet201/conv5_block17_concat/concat]:430
	                     MUL	         3289.812	    4.925	    4.968	  0.141%	 93.173%	     0.000	        1	[densenet201/conv5_block18_0_bn/FusedBatchNormV31]:431
	                     ADD	         3294.788	    6.384	    6.457	  0.183%	 93.356%	     0.000	        1	[densenet201/conv5_block18_0_relu/Relu;densenet201/conv5_block18_0_bn/FusedBatchNormV3]:432
	                 CONV_2D	         3301.255	    0.712	    0.734	  0.021%	 93.377%	     0.000	        1	[densenet201/conv5_block18_1_relu/Relu;densenet201/conv5_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block18_1_conv/Conv2D]:433
	                 CONV_2D	         3301.997	    0.182	    0.201	  0.006%	 93.382%	     0.000	        1	[densenet201/conv5_block18_2_conv/Conv2D1]:434
	           CONCATENATION	         3302.205	    0.031	    0.051	  0.001%	 93.384%	     0.000	        1	[densenet201/conv5_block18_concat/concat]:435
	                     MUL	         3302.263	    5.034	    5.074	  0.144%	 93.527%	     0.000	        1	[densenet201/conv5_block19_0_bn/FusedBatchNormV31]:436
	                     ADD	         3307.345	    6.529	    6.633	  0.188%	 93.715%	     0.000	        1	[densenet201/conv5_block19_0_relu/Relu;densenet201/conv5_block19_0_bn/FusedBatchNormV3]:437
	                 CONV_2D	         3313.987	    0.669	    0.697	  0.020%	 93.735%	     0.000	        1	[densenet201/conv5_block19_1_relu/Relu;densenet201/conv5_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block19_1_conv/Conv2D]:438
	                 CONV_2D	         3314.692	    0.187	    0.211	  0.006%	 93.741%	     0.000	        1	[densenet201/conv5_block19_2_conv/Conv2D1]:439
	           CONCATENATION	         3314.910	    0.030	    0.046	  0.001%	 93.742%	     0.000	        1	[densenet201/conv5_block19_concat/concat]:440
	                     MUL	         3314.963	    5.144	    5.186	  0.147%	 93.889%	     0.000	        1	[densenet201/conv5_block20_0_bn/FusedBatchNormV31]:441
	                     ADD	         3320.158	    6.724	    6.741	  0.191%	 94.080%	     0.000	        1	[densenet201/conv5_block20_0_relu/Relu;densenet201/conv5_block20_0_bn/FusedBatchNormV3]:442
	                 CONV_2D	         3326.908	    0.761	    0.761	  0.022%	 94.101%	     0.000	        1	[densenet201/conv5_block20_1_relu/Relu;densenet201/conv5_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block20_1_conv/Conv2D]:443
	                 CONV_2D	         3327.677	    0.189	    0.191	  0.005%	 94.107%	     0.000	        1	[densenet201/conv5_block20_2_conv/Conv2D1]:444
	           CONCATENATION	         3327.875	    0.077	    0.054	  0.002%	 94.108%	     0.000	        1	[densenet201/conv5_block20_concat/concat]:445
	                     MUL	         3327.936	    5.269	    5.300	  0.150%	 94.258%	     0.000	        1	[densenet201/conv5_block21_0_bn/FusedBatchNormV31]:446
	                     ADD	         3333.245	    6.821	    6.895	  0.195%	 94.454%	     0.000	        1	[densenet201/conv5_block21_0_relu/Relu;densenet201/conv5_block21_0_bn/FusedBatchNormV3]:447
	                 CONV_2D	         3340.149	    0.699	    0.728	  0.021%	 94.474%	     0.000	        1	[densenet201/conv5_block21_1_relu/Relu;densenet201/conv5_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block21_1_conv/Conv2D]:448
	                 CONV_2D	         3340.885	    0.185	    0.195	  0.006%	 94.480%	     0.000	        1	[densenet201/conv5_block21_2_conv/Conv2D1]:449
	           CONCATENATION	         3341.086	    0.032	    0.048	  0.001%	 94.481%	     0.000	        1	[densenet201/conv5_block21_concat/concat]:450
	                     MUL	         3341.141	    5.380	    5.417	  0.153%	 94.634%	     0.000	        1	[densenet201/conv5_block22_0_bn/FusedBatchNormV31]:451
	                     ADD	         3346.568	    7.055	    7.030	  0.199%	 94.834%	     0.000	        1	[densenet201/conv5_block22_0_relu/Relu;densenet201/conv5_block22_0_bn/FusedBatchNormV3]:452
	                 CONV_2D	         3353.606	    0.779	    0.778	  0.022%	 94.856%	     0.000	        1	[densenet201/conv5_block22_1_relu/Relu;densenet201/conv5_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block22_1_conv/Conv2D]:453
	                 CONV_2D	         3354.392	    0.174	    0.200	  0.006%	 94.861%	     0.000	        1	[densenet201/conv5_block22_2_conv/Conv2D1]:454
	           CONCATENATION	         3354.599	    0.049	    0.048	  0.001%	 94.863%	     0.000	        1	[densenet201/conv5_block22_concat/concat]:455
	                     MUL	         3354.655	    5.510	    5.513	  0.156%	 95.019%	     0.000	        1	[densenet201/conv5_block23_0_bn/FusedBatchNormV31]:456
	                     ADD	         3360.177	    7.115	    7.161	  0.203%	 95.221%	     0.000	        1	[densenet201/conv5_block23_0_relu/Relu;densenet201/conv5_block23_0_bn/FusedBatchNormV3]:457
	                 CONV_2D	         3367.348	    0.733	    0.753	  0.021%	 95.243%	     0.000	        1	[densenet201/conv5_block23_1_relu/Relu;densenet201/conv5_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block23_1_conv/Conv2D]:458
	                 CONV_2D	         3368.108	    0.183	    0.195	  0.006%	 95.248%	     0.000	        1	[densenet201/conv5_block23_2_conv/Conv2D1]:459
	           CONCATENATION	         3368.310	    0.035	    0.051	  0.001%	 95.250%	     0.000	        1	[densenet201/conv5_block23_concat/concat]:460
	                     MUL	         3368.368	    5.586	    5.787	  0.164%	 95.414%	     0.000	        1	[densenet201/conv5_block24_0_bn/FusedBatchNormV31]:461
	                     ADD	         3374.162	    7.308	    7.312	  0.207%	 95.621%	     0.000	        1	[densenet201/conv5_block24_0_relu/Relu;densenet201/conv5_block24_0_bn/FusedBatchNormV3]:462
	                 CONV_2D	         3381.483	    0.789	    0.819	  0.023%	 95.644%	     0.000	        1	[densenet201/conv5_block24_1_relu/Relu;densenet201/conv5_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block24_1_conv/Conv2D]:463
	                 CONV_2D	         3382.310	    0.185	    0.193	  0.005%	 95.649%	     0.000	        1	[densenet201/conv5_block24_2_conv/Conv2D1]:464
	           CONCATENATION	         3382.510	    0.040	    0.056	  0.002%	 95.651%	     0.000	        1	[densenet201/conv5_block24_concat/concat]:465
	                     MUL	         3382.572	    5.702	    5.737	  0.162%	 95.813%	     0.000	        1	[densenet201/conv5_block25_0_bn/FusedBatchNormV31]:466
	                     ADD	         3388.319	    7.392	    7.465	  0.211%	 96.025%	     0.000	        1	[densenet201/conv5_block25_0_relu/Relu;densenet201/conv5_block25_0_bn/FusedBatchNormV3]:467
	                 CONV_2D	         3395.793	    0.754	    0.789	  0.022%	 96.047%	     0.000	        1	[densenet201/conv5_block25_1_relu/Relu;densenet201/conv5_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block25_1_conv/Conv2D]:468
	                 CONV_2D	         3396.590	    0.186	    0.205	  0.006%	 96.053%	     0.000	        1	[densenet201/conv5_block25_2_conv/Conv2D1]:469
	           CONCATENATION	         3396.803	    0.035	    0.058	  0.002%	 96.054%	     0.000	        1	[densenet201/conv5_block25_concat/concat]:470
	                     MUL	         3396.867	    5.798	    5.867	  0.166%	 96.220%	     0.000	        1	[densenet201/conv5_block26_0_bn/FusedBatchNormV31]:471
	                     ADD	         3402.743	    7.573	    7.607	  0.215%	 96.436%	     0.000	        1	[densenet201/conv5_block26_0_relu/Relu;densenet201/conv5_block26_0_bn/FusedBatchNormV3]:472
	                 CONV_2D	         3410.359	    0.819	    0.840	  0.024%	 96.460%	     0.000	        1	[densenet201/conv5_block26_1_relu/Relu;densenet201/conv5_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block26_1_conv/Conv2D]:473
	                 CONV_2D	         3411.207	    0.182	    0.203	  0.006%	 96.465%	     0.000	        1	[densenet201/conv5_block26_2_conv/Conv2D1]:474
	           CONCATENATION	         3411.416	    0.035	    0.054	  0.002%	 96.467%	     0.000	        1	[densenet201/conv5_block26_concat/concat]:475
	                     MUL	         3411.477	    5.919	    5.975	  0.169%	 96.636%	     0.000	        1	[densenet201/conv5_block27_0_bn/FusedBatchNormV31]:476
	                     ADD	         3417.460	    7.666	    7.770	  0.220%	 96.856%	     0.000	        1	[densenet201/conv5_block27_0_relu/Relu;densenet201/conv5_block27_0_bn/FusedBatchNormV3]:477
	                 CONV_2D	         3425.239	    0.780	    0.813	  0.023%	 96.879%	     0.000	        1	[densenet201/conv5_block27_1_relu/Relu;densenet201/conv5_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block27_1_conv/Conv2D]:478
	                 CONV_2D	         3426.059	    0.174	    0.203	  0.006%	 96.885%	     0.000	        1	[densenet201/conv5_block27_2_conv/Conv2D1]:479
	           CONCATENATION	         3426.269	    0.031	    0.050	  0.001%	 96.886%	     0.000	        1	[densenet201/conv5_block27_concat/concat]:480
	                     MUL	         3426.326	    6.033	    6.080	  0.172%	 97.058%	     0.000	        1	[densenet201/conv5_block28_0_bn/FusedBatchNormV31]:481
	                     ADD	         3432.414	    7.823	    7.899	  0.224%	 97.282%	     0.000	        1	[densenet201/conv5_block28_0_relu/Relu;densenet201/conv5_block28_0_bn/FusedBatchNormV3]:482
	                 CONV_2D	         3440.322	    0.836	    0.876	  0.025%	 97.307%	     0.000	        1	[densenet201/conv5_block28_1_relu/Relu;densenet201/conv5_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block28_1_conv/Conv2D]:483
	                 CONV_2D	         3441.206	    0.187	    0.197	  0.006%	 97.312%	     0.000	        1	[densenet201/conv5_block28_2_conv/Conv2D1]:484
	           CONCATENATION	         3441.410	    0.036	    0.058	  0.002%	 97.314%	     0.000	        1	[densenet201/conv5_block28_concat/concat]:485
	                     MUL	         3441.474	    6.348	    6.191	  0.175%	 97.489%	     0.000	        1	[densenet201/conv5_block29_0_bn/FusedBatchNormV31]:486
	                     ADD	         3447.674	    8.047	    8.052	  0.228%	 97.717%	     0.000	        1	[densenet201/conv5_block29_0_relu/Relu;densenet201/conv5_block29_0_bn/FusedBatchNormV3]:487
	                 CONV_2D	         3455.735	    0.825	    0.846	  0.024%	 97.741%	     0.000	        1	[densenet201/conv5_block29_1_relu/Relu;densenet201/conv5_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block29_1_conv/Conv2D]:488
	                 CONV_2D	         3456.588	    0.185	    0.200	  0.006%	 97.747%	     0.000	        1	[densenet201/conv5_block29_2_conv/Conv2D1]:489
	           CONCATENATION	         3456.795	    0.047	    0.060	  0.002%	 97.748%	     0.000	        1	[densenet201/conv5_block29_concat/concat]:490
	                     MUL	         3456.861	    6.247	    6.292	  0.178%	 97.927%	     0.000	        1	[densenet201/conv5_block30_0_bn/FusedBatchNormV31]:491
	                     ADD	         3463.162	    8.111	    8.178	  0.232%	 98.158%	     0.000	        1	[densenet201/conv5_block30_0_relu/Relu;densenet201/conv5_block30_0_bn/FusedBatchNormV3]:492
	                 CONV_2D	         3471.353	    0.870	    0.902	  0.026%	 98.184%	     0.000	        1	[densenet201/conv5_block30_1_relu/Relu;densenet201/conv5_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block30_1_conv/Conv2D]:493
	                 CONV_2D	         3472.263	    0.184	    0.197	  0.006%	 98.189%	     0.000	        1	[densenet201/conv5_block30_2_conv/Conv2D1]:494
	           CONCATENATION	         3472.467	    0.037	    0.056	  0.002%	 98.191%	     0.000	        1	[densenet201/conv5_block30_concat/concat]:495
	                     MUL	         3472.530	    6.337	    6.401	  0.181%	 98.372%	     0.000	        1	[densenet201/conv5_block31_0_bn/FusedBatchNormV31]:496
	                     ADD	         3478.939	    8.258	    8.338	  0.236%	 98.608%	     0.000	        1	[densenet201/conv5_block31_0_relu/Relu;densenet201/conv5_block31_0_bn/FusedBatchNormV3]:497
	                 CONV_2D	         3487.286	    0.865	    0.871	  0.025%	 98.633%	     0.000	        1	[densenet201/conv5_block31_1_relu/Relu;densenet201/conv5_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block31_1_conv/Conv2D]:498
	                 CONV_2D	         3488.165	    0.183	    0.192	  0.005%	 98.638%	     0.000	        1	[densenet201/conv5_block31_2_conv/Conv2D1]:499
	           CONCATENATION	         3488.364	    0.043	    0.059	  0.002%	 98.640%	     0.000	        1	[densenet201/conv5_block31_concat/concat]:500
	                     MUL	         3488.429	    6.444	    6.526	  0.185%	 98.825%	     0.000	        1	[densenet201/conv5_block32_0_bn/FusedBatchNormV31]:501
	                     ADD	         3494.963	    8.390	    8.453	  0.239%	 99.064%	     0.000	        1	[densenet201/conv5_block32_0_relu/Relu;densenet201/conv5_block32_0_bn/FusedBatchNormV3]:502
	                 CONV_2D	         3503.425	    0.959	    0.931	  0.026%	 99.090%	     0.000	        1	[densenet201/conv5_block32_1_relu/Relu;densenet201/conv5_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block32_1_conv/Conv2D]:503
	                 CONV_2D	         3504.363	    0.186	    0.198	  0.006%	 99.096%	     0.000	        1	[densenet201/conv5_block32_2_conv/Conv2D210]:504
	           CONCATENATION	         3504.568	    0.042	    0.054	  0.002%	 99.098%	     0.000	        1	[densenet201/conv5_block32_concat/concat]:505
	                     MUL	         3504.629	    6.614	    6.647	  0.188%	 99.286%	     0.000	        1	[densenet201/bn/FusedBatchNormV31]:506
	                     ADD	         3511.285	    8.529	    8.579	  0.243%	 99.529%	     0.000	        1	[densenet201/relu/Relu;densenet201/bn/FusedBatchNormV3]:507
	                    MEAN	         3519.873	   16.002	   16.132	  0.457%	 99.985%	     0.000	        1	[densenet201/avg_pool/Mean]:508
	         FULLY_CONNECTED	         3536.014	    0.456	    0.432	  0.012%	 99.998%	     0.000	        1	[densenet201/predictions/MatMul;densenet201/predictions/BiasAdd]:509
	                 SOFTMAX	         3536.453	    0.090	    0.086	  0.002%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:510

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     ADD	          666.119	   73.509	   73.781	  2.089%	  2.089%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                     ADD	          521.536	   64.636	   64.656	  1.831%	  3.919%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                     MUL	          609.064	   56.951	   57.043	  1.615%	  5.534%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     ADD	          396.131	   55.325	   55.595	  1.574%	  7.108%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                     MUL	          471.601	   49.966	   49.922	  1.413%	  8.522%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	          285.990	   46.046	   46.315	  1.311%	  9.833%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                     MUL	          353.229	   42.935	   42.890	  1.214%	 11.047%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	          194.892	   36.932	   37.169	  1.052%	 12.100%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                     ADD	         1321.816	   36.478	   36.673	  1.038%	 13.138%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99
	                     MUL	          250.127	   35.800	   35.851	  1.015%	 14.153%	     0.000	        1	[densenet201/conv2_block4_0_bn/FusedBatchNormV3]:19

Number of nodes executed: 511
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                     ADD	      102	  1731.027	    49.012%	    49.012%	     0.000	      102
	                     MUL	      102	  1335.273	    37.807%	    86.819%	     0.000	      102
	                 CONV_2D	      200	   384.664	    10.891%	    97.711%	     0.000	      200
	                     PAD	        2	    21.953	     0.622%	    98.332%	     0.000	        2
	         AVERAGE_POOL_2D	        3	    19.941	     0.565%	    98.897%	     0.000	        3
	           CONCATENATION	       98	    17.089	     0.484%	    99.381%	     0.000	       98
	                    MEAN	        1	    16.132	     0.457%	    99.837%	     0.000	        1
	             MAX_POOL_2D	        1	     5.228	     0.148%	    99.985%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.431	     0.012%	    99.998%	     0.000	        1
	                 SOFTMAX	        1	     0.085	     0.002%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=3530634 curr=3527990 min=3521066 max=3546793 avg=3.53207e+06 std=6993
Memory (bytes): count=0
511 nodes observed



[ perf record: Woken up 300 times to write data ]
[ perf record: Captured and wrote 75.162 MB /tmp/data.record (424837 samples) ]

107.680

