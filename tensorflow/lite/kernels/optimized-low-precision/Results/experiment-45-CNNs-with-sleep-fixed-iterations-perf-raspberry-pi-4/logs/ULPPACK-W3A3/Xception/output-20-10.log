STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/Xception.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/Xception.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (32, 27, ), Input shape (89401, 3, ), and Output shape (22201, 32, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 32)
	Allocating LowPrecision Activations Tensors with Shape of (22208, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 288)
Applying Conv Low-Precision for Kernel shape (64, 288, ), Input shape (22201, 32, ), and Output shape (21609, 64, ), and the ID is 1
	Allocating LowPrecision Activations Tensors with Shape of (21616, 288)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (21609, 64, ), and Output shape (21609, 128, ), and the ID is 2
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (21616, 64)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (21609, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
(21609, 128, ), and the ID is 3
	Allocating LowPrecision Activations Tensors with Shape of (21616, 128)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (21609, 64, ), and Output shape (5476, 128, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (5480, 64)
Applying Conv Low-Precision for Kernel shape (256, 128, ), Input shape (5476, 128, ), and Output shape (5476, 256, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (5480, 128)
Applying Conv Low-Precision for Kernel shape (256, 256, ), Input shape (5476, 256, ), and Output shape (5476, 256, ), and the ID is 6
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 256)
	Allocating LowPrecision Activations Tensors with Shape of (5480, 256)
Applying Conv Low-Precision for Kernel shape (256, 128, ), Input shape (5476, 128, ), and Output shape (1369, 256, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (1376, 128)
Applying Conv Low-Precision for Kernel shape (728, 256, ), Input shape (1369, 256, ), and Output shape (1369, 728, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 256)
	Allocating LowPrecision Activations Tensors with Shape of (1376, 256)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (1369, 728, ), and Output shape (1369, 728, ), and the ID is 9	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (1376, 736)
Applying Conv Low-Precision for Kernel shape (728, 256, ), Input shape (1369, 256, ), and Output shape (361, 728, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 256)
	Allocating LowPrecision Activations Tensors with Shape of (368, 256)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 11
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 12
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 13
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 14
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 15
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 16
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 17
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 18
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 19
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 22
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 24
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 25
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 26
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 27
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 28
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 29
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 30
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 31
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 32
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 33
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 34
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 35
	Allocating LowPrecision Weight Tensors with Shape of (728, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 728, ), Input shape (361, 728, ), and Output shape (361, 1024, ), and the ID is 36
	Allocating LowPrecision Weight Tensors with Shape of (1024, 736)
	Allocating LowPrecision Activations Tensors with Shape of (368, 736)
Applying Conv Low-Precision for Kernel shape (1024, 728, ), Input shape (361, 728, ), and Output shape (100, 1024, ), and the ID is 37
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 736)
	Allocating LowPrecision Activations Tensors with Shape of (104, 736)
Applying Conv Low-Precision for Kernel shape (1536, 1024, ), Input shape (100, 1024, ), and Output shape (100, 1536, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1536, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (104, 1024)
Applying Conv Low-Precision for Kernel shape (2048, 1536, ), Input shape (100, 1536, ), and Output shape (100, 2048, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 39
	Allocating LowPrecision Weight Tensors with Shape of (2048, 1536)
	Allocating LowPrecision Activations Tensors with Shape of (104, 1536)
Applying Low-Precision for shape (1000, 2048, ) and Input shape (1, 2048, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 2048)
	Transformed Activation Shape From: (1, 2048) To: (8, 2048)
The input model file size (MB): 24.0822
Initialized session in 358.391ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=8 first=19532061 curr=19482847 min=19459639 max=19532061 avg=1.94902e+07 std=21938

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=8 first=19501884 curr=19473897 min=19458689 max=19534489 avg=1.94946e+07 std=24014

Inference timings in us: Init: 358391, First inference: 19532061, Warmup (avg): 1.94902e+07, Inference (avg): 1.94946e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=50.1328 overall=64.2617
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  329.639	  329.639	100.000%	100.000%	 44016.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  329.639	  329.639	100.000%	100.000%	 44016.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   329.639	   100.000%	   100.000%	 44016.000	        1

Timings (microseconds): count=1 curr=329639
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.025	   91.709	   92.528	  0.475%	  0.475%	     0.000	        1	[xception/block1_conv1_act/Relu;xception/block1_conv1_bn/FusedBatchNormV3;xception/block1_conv1_bn/FusedBatchNormV3/ReadVariableOp;xception/block1_conv1/Conv2D]:0
	                 CONV_2D	           92.566	 1017.855	 1013.540	  5.199%	  5.674%	     0.000	        1	[xception/block1_conv2_act/Relu;xception/block1_conv2_bn/FusedBatchNormV3;xception/block1_conv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block1_conv2/Conv2D]:1
	       DEPTHWISE_CONV_2D	         1106.120	    5.810	    5.564	  0.029%	  5.703%	     0.000	        1	[xception/block2_sepconv1/separable_conv2d/depthwise1]:2
	                 CONV_2D	         1111.697	  505.372	  504.891	  2.590%	  8.293%	     0.000	        1	[xception/block2_sepconv2_act/Relu;xception/block2_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv1/separable_conv2d]:3
	       DEPTHWISE_CONV_2D	         1616.599	   13.844	   13.993	  0.072%	  8.364%	     0.000	        1	[xception/block2_sepconv2/separable_conv2d/depthwise1]:4
	                 CONV_2D	         1630.604	  902.150	  906.647	  4.651%	 13.015%	     0.000	        1	[xception/block2_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv2/separable_conv2d]:5
	             MAX_POOL_2D	         2537.265	   16.566	   16.769	  0.086%	 13.102%	     0.000	        1	[xception/block2_pool/MaxPool]:6
	                 CONV_2D	         2554.045	  125.533	  126.480	  0.649%	 13.750%	     0.000	        1	[xception/batch_normalization_297/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/conv2d_297/Conv2D]:7
	                     ADD	         2680.537	   63.761	   63.785	  0.327%	 14.078%	     0.000	        1	[xception/add_4/add]:8
	                    RELU	         2744.336	   85.721	   86.023	  0.441%	 14.519%	     0.000	        1	[xception/block3_sepconv1_act/Relu]:9
	       DEPTHWISE_CONV_2D	         2830.372	    3.506	    3.554	  0.018%	 14.537%	     0.000	        1	[xception/block3_sepconv1/separable_conv2d/depthwise1]:10
	                 CONV_2D	         2833.938	  435.009	  436.813	  2.241%	 16.778%	     0.000	        1	[xception/block3_sepconv2_act/Relu;xception/block3_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv1/separable_conv2d]:11
	       DEPTHWISE_CONV_2D	         3270.763	    7.372	    7.154	  0.037%	 16.815%	     0.000	        1	[xception/block3_sepconv2/separable_conv2d/depthwise1]:12
	                 CONV_2D	         3277.929	  831.650	  827.198	  4.243%	 21.058%	     0.000	        1	[xception/block3_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv2/separable_conv2d]:13
	             MAX_POOL_2D	         4105.140	    8.166	    8.178	  0.042%	 21.100%	     0.000	        1	[xception/block3_pool/MaxPool]:14
	                 CONV_2D	         4113.328	  109.237	  108.973	  0.559%	 21.659%	     0.000	        1	[xception/batch_normalization_298/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/conv2d_298/Conv2D]:15
	                     ADD	         4222.312	   32.078	   32.041	  0.164%	 21.823%	     0.000	        1	[xception/add_5/add]:16
	                    RELU	         4254.364	   41.677	   41.538	  0.213%	 22.037%	     0.000	        1	[xception/block4_sepconv1_act/Relu]:17
	       DEPTHWISE_CONV_2D	         4295.914	    1.686	    1.707	  0.009%	 22.045%	     0.000	        1	[xception/block4_sepconv1/separable_conv2d/depthwise1]:18
	                 CONV_2D	         4297.631	  561.005	  564.666	  2.897%	 24.942%	     0.000	        1	[xception/block4_sepconv2_act/Relu;xception/block4_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv1/separable_conv2d]:19
	       DEPTHWISE_CONV_2D	         4862.310	    5.291	    5.385	  0.028%	 24.970%	     0.000	        1	[xception/block4_sepconv2/separable_conv2d/depthwise1]:20
	                 CONV_2D	         4867.706	 1591.319	 1589.619	  8.155%	 33.124%	     0.000	        1	[xception/block4_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv2/separable_conv2d]:21
	             MAX_POOL_2D	         6457.337	    6.398	    6.478	  0.033%	 33.157%	     0.000	        1	[xception/block4_pool/MaxPool]:22
	                 CONV_2D	         6463.826	  149.575	  150.165	  0.770%	 33.928%	     0.000	        1	[xception/batch_normalization_299/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/conv2d_299/Conv2D]:23
	                     ADD	         6614.002	   23.901	   24.018	  0.123%	 34.051%	     0.000	        1	[xception/add_6/add]:24
	                    RELU	         6638.034	   30.899	   31.061	  0.159%	 34.210%	     0.000	        1	[xception/block5_sepconv1_act/Relu]:25
	       DEPTHWISE_CONV_2D	         6669.106	    1.283	    1.382	  0.007%	 34.217%	     0.000	        1	[xception/block5_sepconv1/separable_conv2d/depthwise1]:26
	                 CONV_2D	         6670.498	  414.406	  419.005	  2.149%	 36.367%	     0.000	        1	[xception/block5_sepconv2_act/Relu;xception/block5_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block5_sepconv1/separable_conv2d]:27
	       DEPTHWISE_CONV_2D	         7089.514	    1.360	    1.417	  0.007%	 36.374%	     0.000	        1	[xception/block5_sepconv2/separable_conv2d/depthwise1]:28
	                 CONV_2D	         7090.940	  414.730	  418.850	  2.149%	 38.523%	     0.000	        1	[xception/block5_sepconv3_act/Relu;xception/block5_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block5_sepconv2/separable_conv2d]:29
	       DEPTHWISE_CONV_2D	         7509.803	    1.367	    1.403	  0.007%	 38.530%	     0.000	        1	[xception/block5_sepconv3/separable_conv2d/depthwise1]:30
	                 CONV_2D	         7511.215	  414.609	  417.721	  2.143%	 40.673%	     0.000	        1	[xception/block5_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block5_sepconv3/separable_conv2d]:31
	                     ADD	         7928.947	   23.892	   23.990	  0.123%	 40.796%	     0.000	        1	[xception/add_7/add]:32
	                    RELU	         7952.948	   30.961	   31.043	  0.159%	 40.955%	     0.000	        1	[xception/block6_sepconv1_act/Relu]:33
	       DEPTHWISE_CONV_2D	         7984.003	    1.365	    1.409	  0.007%	 40.963%	     0.000	        1	[xception/block6_sepconv1/separable_conv2d/depthwise1]:34
	                 CONV_2D	         7985.422	  426.036	  420.401	  2.157%	 43.119%	     0.000	        1	[xception/block6_sepconv2_act/Relu;xception/block6_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv1/separable_conv2d]:35
	       DEPTHWISE_CONV_2D	         8405.834	    1.509	    1.449	  0.007%	 43.127%	     0.000	        1	[xception/block6_sepconv2/separable_conv2d/depthwise1]:36
	                 CONV_2D	         8407.293	  430.538	  421.921	  2.164%	 45.291%	     0.000	        1	[xception/block6_sepconv3_act/Relu;xception/block6_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv2/separable_conv2d]:37
	       DEPTHWISE_CONV_2D	         8829.225	    1.429	    1.448	  0.007%	 45.298%	     0.000	        1	[xception/block6_sepconv3/separable_conv2d/depthwise1]:38
	                 CONV_2D	         8830.683	  418.801	  420.910	  2.159%	 47.458%	     0.000	        1	[xception/block6_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv3/separable_conv2d]:39
	                     ADD	         9251.606	   23.885	   24.021	  0.123%	 47.581%	     0.000	        1	[xception/add_8/add]:40
	                    RELU	         9275.638	   30.968	   31.125	  0.160%	 47.741%	     0.000	        1	[xception/block7_sepconv1_act/Relu]:41
	       DEPTHWISE_CONV_2D	         9306.774	    1.290	    1.411	  0.007%	 47.748%	     0.000	        1	[xception/block7_sepconv1/separable_conv2d/depthwise1]:42
	                 CONV_2D	         9308.194	  414.825	  418.076	  2.145%	 49.893%	     0.000	        1	[xception/block7_sepconv2_act/Relu;xception/block7_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block7_sepconv1/separable_conv2d]:43
	       DEPTHWISE_CONV_2D	         9726.282	    1.359	    1.414	  0.007%	 49.900%	     0.000	        1	[xception/block7_sepconv2/separable_conv2d/depthwise1]:44
	                 CONV_2D	         9727.706	  414.842	  418.407	  2.146%	 52.046%	     0.000	        1	[xception/block7_sepconv3_act/Relu;xception/block7_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block7_sepconv2/separable_conv2d]:45
	       DEPTHWISE_CONV_2D	        10146.126	    1.365	    1.429	  0.007%	 52.054%	     0.000	        1	[xception/block7_sepconv3/separable_conv2d/depthwise1]:46
	                 CONV_2D	        10147.563	  415.806	  418.310	  2.146%	 54.199%	     0.000	        1	[xception/block7_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block7_sepconv3/separable_conv2d]:47
	                     ADD	        10565.885	   24.007	   23.987	  0.123%	 54.322%	     0.000	        1	[xception/add_9/add]:48
	                    RELU	        10589.883	   30.931	   31.045	  0.159%	 54.482%	     0.000	        1	[xception/block8_sepconv1_act/Relu]:49
	       DEPTHWISE_CONV_2D	        10620.938	    1.285	    1.363	  0.007%	 54.489%	     0.000	        1	[xception/block8_sepconv1/separable_conv2d/depthwise1]:50
	                 CONV_2D	        10622.310	  430.640	  421.508	  2.162%	 56.651%	     0.000	        1	[xception/block8_sepconv2_act/Relu;xception/block8_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block8_sepconv1/separable_conv2d]:51
	       DEPTHWISE_CONV_2D	        11043.829	    1.417	    1.419	  0.007%	 56.658%	     0.000	        1	[xception/block8_sepconv2/separable_conv2d/depthwise1]:52
	                 CONV_2D	        11045.258	  422.432	  421.561	  2.163%	 58.821%	     0.000	        1	[xception/block8_sepconv3_act/Relu;xception/block8_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block8_sepconv2/separable_conv2d]:53
	       DEPTHWISE_CONV_2D	        11466.830	    1.344	    1.430	  0.007%	 58.828%	     0.000	        1	[xception/block8_sepconv3/separable_conv2d/depthwise1]:54
	                 CONV_2D	        11468.270	  414.511	  419.852	  2.154%	 60.982%	     0.000	        1	[xception/block8_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block8_sepconv3/separable_conv2d]:55
	                     ADD	        11888.133	   23.886	   24.034	  0.123%	 61.105%	     0.000	        1	[xception/add_10/add]:56
	                    RELU	        11912.178	   30.929	   31.105	  0.160%	 61.265%	     0.000	        1	[xception/block9_sepconv1_act/Relu]:57
	       DEPTHWISE_CONV_2D	        11943.293	    1.291	    1.383	  0.007%	 61.272%	     0.000	        1	[xception/block9_sepconv1/separable_conv2d/depthwise1]:58
	                 CONV_2D	        11944.685	  414.361	  419.414	  2.152%	 63.424%	     0.000	        1	[xception/block9_sepconv2_act/Relu;xception/block9_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block9_sepconv1/separable_conv2d]:59
	       DEPTHWISE_CONV_2D	        12364.110	    1.393	    1.433	  0.007%	 63.431%	     0.000	        1	[xception/block9_sepconv2/separable_conv2d/depthwise1]:60
	                 CONV_2D	        12365.552	  414.667	  418.310	  2.146%	 65.577%	     0.000	        1	[xception/block9_sepconv3_act/Relu;xception/block9_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block9_sepconv2/separable_conv2d]:61
	       DEPTHWISE_CONV_2D	        12783.875	    1.394	    1.417	  0.007%	 65.584%	     0.000	        1	[xception/block9_sepconv3/separable_conv2d/depthwise1]:62
	                 CONV_2D	        12785.301	  416.996	  418.838	  2.149%	 67.733%	     0.000	        1	[xception/block9_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block9_sepconv3/separable_conv2d]:63
	                     ADD	        13204.152	   24.274	   24.010	  0.123%	 67.856%	     0.000	        1	[xception/add_11/add]:64
	                    RELU	        13228.174	   31.860	   31.102	  0.160%	 68.015%	     0.000	        1	[xception/block10_sepconv1_act/Relu]:65
	       DEPTHWISE_CONV_2D	        13259.287	    1.602	    1.431	  0.007%	 68.023%	     0.000	        1	[xception/block10_sepconv1/separable_conv2d/depthwise1]:66
	                 CONV_2D	        13260.730	  431.110	  420.746	  2.158%	 70.181%	     0.000	        1	[xception/block10_sepconv2_act/Relu;xception/block10_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block10_sepconv1/separable_conv2d]:67
	       DEPTHWISE_CONV_2D	        13681.487	    1.529	    1.466	  0.008%	 70.189%	     0.000	        1	[xception/block10_sepconv2/separable_conv2d/depthwise1]:68
	                 CONV_2D	        13682.963	  425.895	  421.548	  2.163%	 72.351%	     0.000	        1	[xception/block10_sepconv3_act/Relu;xception/block10_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block10_sepconv2/separable_conv2d]:69
	       DEPTHWISE_CONV_2D	        14104.525	    1.472	    1.437	  0.007%	 72.359%	     0.000	        1	[xception/block10_sepconv3/separable_conv2d/depthwise1]:70
	                 CONV_2D	        14105.971	  416.923	  418.854	  2.149%	 74.507%	     0.000	        1	[xception/block10_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block10_sepconv3/separable_conv2d]:71
	                     ADD	        14524.838	   23.867	   23.970	  0.123%	 74.630%	     0.000	        1	[xception/add_12/add]:72
	                    RELU	        14548.818	   30.956	   31.018	  0.159%	 74.789%	     0.000	        1	[xception/block11_sepconv1_act/Relu]:73
	       DEPTHWISE_CONV_2D	        14579.849	    1.399	    1.377	  0.007%	 74.796%	     0.000	        1	[xception/block11_sepconv1/separable_conv2d/depthwise1]:74
	                 CONV_2D	        14581.237	  414.352	  419.023	  2.150%	 76.946%	     0.000	        1	[xception/block11_sepconv2_act/Relu;xception/block11_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block11_sepconv1/separable_conv2d]:75
	       DEPTHWISE_CONV_2D	        15000.279	    1.380	    1.380	  0.007%	 76.953%	     0.000	        1	[xception/block11_sepconv2/separable_conv2d/depthwise1]:76
	                 CONV_2D	        15001.667	  414.407	  417.868	  2.144%	 79.097%	     0.000	        1	[xception/block11_sepconv3_act/Relu;xception/block11_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block11_sepconv2/separable_conv2d]:77
	       DEPTHWISE_CONV_2D	        15419.547	    1.341	    1.387	  0.007%	 79.104%	     0.000	        1	[xception/block11_sepconv3/separable_conv2d/depthwise1]:78
	                 CONV_2D	        15420.942	  424.428	  418.833	  2.149%	 81.252%	     0.000	        1	[xception/block11_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block11_sepconv3/separable_conv2d]:79
	                     ADD	        15839.787	   24.164	   23.991	  0.123%	 81.376%	     0.000	        1	[xception/add_13/add]:80
	                    RELU	        15863.790	   31.130	   30.988	  0.159%	 81.534%	     0.000	        1	[xception/block12_sepconv1_act/Relu]:81
	       DEPTHWISE_CONV_2D	        15894.787	    1.525	    1.371	  0.007%	 81.542%	     0.000	        1	[xception/block12_sepconv1/separable_conv2d/depthwise1]:82
	                 CONV_2D	        15896.167	  429.638	  420.986	  2.160%	 83.701%	     0.000	        1	[xception/block12_sepconv2_act/Relu;xception/block12_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block12_sepconv1/separable_conv2d]:83
	       DEPTHWISE_CONV_2D	        16317.165	    1.478	    1.450	  0.007%	 83.709%	     0.000	        1	[xception/block12_sepconv2/separable_conv2d/depthwise1]:84
	                 CONV_2D	        16318.624	  418.725	  421.013	  2.160%	 85.868%	     0.000	        1	[xception/block12_sepconv3_act/Relu;xception/block12_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block12_sepconv2/separable_conv2d]:85
	       DEPTHWISE_CONV_2D	        16739.649	    1.426	    1.454	  0.007%	 85.876%	     0.000	        1	[xception/block12_sepconv3/separable_conv2d/depthwise1]:86
	                 CONV_2D	        16741.113	  414.318	  418.489	  2.147%	 88.023%	     0.000	        1	[xception/block12_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block12_sepconv3/separable_conv2d]:87
	                     ADD	        17159.614	   23.873	   23.986	  0.123%	 88.146%	     0.000	        1	[xception/add_14/add]:88
	                    RELU	        17183.610	   30.893	   31.032	  0.159%	 88.305%	     0.000	        1	[xception/block13_sepconv1_act/Relu]:89
	       DEPTHWISE_CONV_2D	        17214.652	    1.336	    1.394	  0.007%	 88.312%	     0.000	        1	[xception/block13_sepconv1/separable_conv2d/depthwise1]:90
	                 CONV_2D	        17216.056	  414.826	  419.119	  2.150%	 90.462%	     0.000	        1	[xception/block13_sepconv2_act/Relu;xception/block13_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block13_sepconv1/separable_conv2d]:91
	       DEPTHWISE_CONV_2D	        17635.186	    1.343	    1.373	  0.007%	 90.469%	     0.000	        1	[xception/block13_sepconv2/separable_conv2d/depthwise1]:92
	                 CONV_2D	        17636.568	  584.583	  586.429	  3.008%	 93.477%	     0.000	        1	[xception/block13_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_300/FusedBatchNormV3/ReadVariableOp;xception/block13_sepconv2/separable_conv2d]:93
	             MAX_POOL_2D	        18223.013	    2.513	    2.437	  0.013%	 93.490%	     0.000	        1	[xception/block13_pool/MaxPool]:94
	                 CONV_2D	        18225.459	  172.859	  166.091	  0.852%	 94.342%	     0.000	        1	[xception/batch_normalization_300/FusedBatchNormV3;xception/batch_normalization_300/FusedBatchNormV3/ReadVariableOp;xception/conv2d_300/Conv2D]:95
	                     ADD	        18391.562	    9.774	    9.452	  0.048%	 94.390%	     0.000	        1	[xception/add_15/add]:96
	       DEPTHWISE_CONV_2D	        18401.024	    0.609	    0.536	  0.003%	 94.393%	     0.000	        1	[xception/block14_sepconv1/separable_conv2d/depthwise1]:97
	                 CONV_2D	        18401.568	  351.451	  341.496	  1.752%	 96.145%	     0.000	        1	[xception/block14_sepconv1_act/Relu;xception/block14_sepconv1_bn/FusedBatchNormV3;xception/block14_sepconv1_bn/FusedBatchNormV3/ReadVariableOp;xception/block14_sepconv1/separable_conv2d]:98
	       DEPTHWISE_CONV_2D	        18743.076	    0.878	    0.860	  0.004%	 96.150%	     0.000	        1	[xception/block14_sepconv2/separable_conv2d/depthwise1]:99
	                 CONV_2D	        18743.945	  681.983	  680.689	  3.492%	 99.641%	     0.000	        1	[xception/block14_sepconv2_act/Relu;xception/block14_sepconv2_bn/FusedBatchNormV3;xception/block14_sepconv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block14_sepconv2/separable_conv2d]:100
	                    MEAN	        19424.645	   34.993	   35.333	  0.181%	 99.823%	     0.000	        1	[xception/avg_pool/Mean]:101
	         FULLY_CONNECTED	        19459.989	   34.269	   34.481	  0.177%	100.000%	     0.000	        1	[xception/predictions/MatMul;xception/predictions/BiasAdd]:102
	                 SOFTMAX	        19494.480	    0.086	    0.090	  0.000%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:103

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	         4867.706	 1591.319	 1589.619	  8.155%	  8.155%	     0.000	        1	[xception/block4_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv2/separable_conv2d]:21
	                 CONV_2D	           92.566	 1017.855	 1013.540	  5.199%	 13.354%	     0.000	        1	[xception/block1_conv2_act/Relu;xception/block1_conv2_bn/FusedBatchNormV3;xception/block1_conv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block1_conv2/Conv2D]:1
	                 CONV_2D	         1630.604	  902.150	  906.647	  4.651%	 18.005%	     0.000	        1	[xception/block2_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv2/separable_conv2d]:5
	                 CONV_2D	         3277.929	  831.650	  827.198	  4.243%	 22.249%	     0.000	        1	[xception/block3_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv2/separable_conv2d]:13
	                 CONV_2D	        18743.945	  681.983	  680.689	  3.492%	 25.740%	     0.000	        1	[xception/block14_sepconv2_act/Relu;xception/block14_sepconv2_bn/FusedBatchNormV3;xception/block14_sepconv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block14_sepconv2/separable_conv2d]:100
	                 CONV_2D	        17636.568	  584.583	  586.429	  3.008%	 28.749%	     0.000	        1	[xception/block13_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_300/FusedBatchNormV3/ReadVariableOp;xception/block13_sepconv2/separable_conv2d]:93
	                 CONV_2D	         4297.631	  561.005	  564.666	  2.897%	 31.646%	     0.000	        1	[xception/block4_sepconv2_act/Relu;xception/block4_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv1/separable_conv2d]:19
	                 CONV_2D	         1111.697	  505.372	  504.891	  2.590%	 34.236%	     0.000	        1	[xception/block2_sepconv2_act/Relu;xception/block2_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv1/separable_conv2d]:3
	                 CONV_2D	         2833.938	  435.009	  436.813	  2.241%	 36.476%	     0.000	        1	[xception/block3_sepconv2_act/Relu;xception/block3_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv1/separable_conv2d]:11
	                 CONV_2D	         8407.293	  430.538	  421.921	  2.164%	 38.641%	     0.000	        1	[xception/block6_sepconv3_act/Relu;xception/block6_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv2/separable_conv2d]:37

Number of nodes executed: 104
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       40	 18585.766	    95.344%	    95.344%	     0.000	       40
	                    RELU	       11	   407.076	     2.088%	    97.432%	     0.000	       11
	                     ADD	       12	   321.281	     1.648%	    99.081%	     0.000	       12
	       DEPTHWISE_CONV_2D	       34	    75.461	     0.387%	    99.468%	     0.000	       34
	                    MEAN	        1	    35.333	     0.181%	    99.649%	     0.000	        1
	         FULLY_CONNECTED	        1	    34.480	     0.177%	    99.826%	     0.000	        1
	             MAX_POOL_2D	        4	    33.859	     0.174%	   100.000%	     0.000	        4
	                 SOFTMAX	        1	     0.090	     0.000%	   100.000%	     0.000	        1

Timings (microseconds): count=8 first=19500668 curr=19472633 min=19457479 max=19533291 avg=1.94934e+07 std=24031
Memory (bytes): count=0
104 nodes observed



double free or corruption (out)
[ perf record: Woken up 984 times to write data ]
[ perf record: Captured and wrote 246.158 MB /tmp/data.record (1247857 samples) ]

315.420

