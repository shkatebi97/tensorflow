STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/VGG19.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/VGG19.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 27, ), Input shape (50176, 3, ), and Output shape (50176, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (50176, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 576)
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (50176, 64, ), and Output shape (50176, 64, ), and the ID is 1
	Allocating LowPrecision Activations Tensors with Shape of (50176, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (12544, 64, ), and Output shape (12544, 128, ), and the ID is 2
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 576)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (12544, 128, ), and Output shape (12544, 128, ), and the ID is 3	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 1152)
Applying Conv Low-Precision for Kernel shape (256, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 256, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 5	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 2304)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 6
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 2304)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 2304)
Applying Conv Low-Precision for Kernel shape (512, 2304, ), Input shape (784, 256, ), and Output shape (784, 512, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
8
	Allocating LowPrecision Weight Tensors with Shape of (512, 2304)
	Allocating LowPrecision Activations Tensors with Shape of (784, 2304)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (784, 4608)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (784, 4608)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 11
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (784, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 12
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 14
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 4608)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 15
	Allocating LowPrecision Activations Tensors with Shape of (200, 4608)
Applying Low-Precision for shape (4096, 25088, ) and Input shape (1, 25088, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (4096, 25088)
	Transformed Activation Shape From: (1, 25088) To: (8, 25088)
Applying Low-Precision for shape (4096, 4096, ) and Input shape (1, 4096, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (4096, 4096)
	Transformed Activation Shape From: (1, 4096) To: (8, 4096)
Applying Low-Precision for shape (1000, 4096, ) and Input shape (1, 4096, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 4096)
	Transformed Activation Shape From: (1, 4096) To: (8, 4096)
The input model file size (MB): 143.801
Initialized session in 3179.06ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=4 first=46885437 curr=46678762 min=46678762 max=46885437 avg=4.67854e+07 std=77699

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=4 first=46677533 curr=46733452 min=46677533 max=46816239 avg=4.67422e+07 std=49347

Inference timings in us: Init: 3179061, First inference: 46885437, Warmup (avg): 4.67854e+07, Inference (avg): 4.67422e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=277.555 overall=341.246
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	 3173.540	 3173.540	100.000%	100.000%	279636.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	 3173.540	 3173.540	100.000%	100.000%	279636.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	  3173.540	   100.000%	   100.000%	279636.000	        1

Timings (microseconds): count=1 curr=3173540
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.018	  384.366	  381.900	  0.817%	  0.817%	     0.000	        1	[vgg19/block1_conv1/Relu;vgg19/block1_conv1/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv1/Conv2D]:0
	                 CONV_2D	          381.930	 5092.169	 5140.895	 10.998%	 11.816%	     0.000	        1	[vgg19/block1_conv2/Relu;vgg19/block1_conv2/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv2/Conv2D]:1
	             MAX_POOL_2D	         5522.837	   12.532	   12.558	  0.027%	 11.842%	     0.000	        1	[vgg19/block1_pool/MaxPool]:2
	                 CONV_2D	         5535.406	 2146.958	 2150.084	  4.600%	 16.442%	     0.000	        1	[vgg19/block2_conv1/Relu;vgg19/block2_conv1/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv1/Conv2D]:3
	                 CONV_2D	         7685.502	 4309.294	 4311.921	  9.225%	 25.667%	     0.000	        1	[vgg19/block2_conv2/Relu;vgg19/block2_conv2/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv2/Conv2D]:4
	             MAX_POOL_2D	        11997.435	    5.739	    5.722	  0.012%	 25.680%	     0.000	        1	[vgg19/block2_pool/MaxPool]:5
	                 CONV_2D	        12003.168	 2045.683	 2046.994	  4.379%	 30.059%	     0.000	        1	[vgg19/block3_conv1/Relu;vgg19/block3_conv1/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv1/Conv2D]:6
	                 CONV_2D	        14050.174	 4119.026	 4122.023	  8.819%	 38.878%	     0.000	        1	[vgg19/block3_conv2/Relu;vgg19/block3_conv2/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv2/Conv2D]:7
	                 CONV_2D	        18172.209	 4114.595	 4134.199	  8.845%	 47.722%	     0.000	        1	[vgg19/block3_conv3/Relu;vgg19/block3_conv3/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv3/Conv2D]:8
	                 CONV_2D	        22306.420	 4167.042	 4136.924	  8.851%	 56.573%	     0.000	        1	[vgg19/block3_conv4/Relu;vgg19/block3_conv4/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv4/Conv2D]:9
	             MAX_POOL_2D	        26443.356	    2.760	    2.789	  0.006%	 56.579%	     0.000	        1	[vgg19/block3_pool/MaxPool]:10
	                 CONV_2D	        26446.154	 2020.238	 2017.262	  4.316%	 60.895%	     0.000	        1	[vgg19/block4_conv1/Relu;vgg19/block4_conv1/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv1/Conv2D]:11
	                 CONV_2D	        28463.428	 4036.712	 4045.649	  8.655%	 69.550%	     0.000	        1	[vgg19/block4_conv2/Relu;vgg19/block4_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv2/Conv2D]:12
	                 CONV_2D	        32509.094	 4051.689	 4047.218	  8.659%	 78.209%	     0.000	        1	[vgg19/block4_conv3/Relu;vgg19/block4_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv3/Conv2D]:13
	                 CONV_2D	        36556.324	 4036.021	 4035.717	  8.634%	 86.843%	     0.000	        1	[vgg19/block4_conv4/Relu;vgg19/block4_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv4/Conv2D]:14
	             MAX_POOL_2D	        40592.053	    1.493	    1.502	  0.003%	 86.846%	     0.000	        1	[vgg19/block4_pool/MaxPool]:15
	                 CONV_2D	        40593.564	 1011.015	 1021.787	  2.186%	 89.032%	     0.000	        1	[vgg19/block5_conv1/Relu;vgg19/block5_conv1/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv1/Conv2D]:16
	                 CONV_2D	        41615.363	 1025.759	 1016.833	  2.175%	 91.207%	     0.000	        1	[vgg19/block5_conv2/Relu;vgg19/block5_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv2/Conv2D]:17
	                 CONV_2D	        42632.207	 1014.859	 1023.078	  2.189%	 93.396%	     0.000	        1	[vgg19/block5_conv3/Relu;vgg19/block5_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv3/Conv2D]:18
	                 CONV_2D	        43655.298	 1016.910	 1018.330	  2.179%	 95.575%	     0.000	        1	[vgg19/block5_conv4/Relu;vgg19/block5_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv4/Conv2D]:19
	             MAX_POOL_2D	        44673.641	    0.393	    0.390	  0.001%	 95.576%	     0.000	        1	[vgg19/block5_pool/MaxPool]:20
	                 RESHAPE	        44674.038	    0.018	    0.014	  0.000%	 95.576%	     0.000	        1	[vgg19/flatten/Reshape]:21
	         FULLY_CONNECTED	        44674.058	 1715.242	 1720.282	  3.680%	 99.256%	     0.000	        1	[vgg19/fc1/MatMul;vgg19/fc1/Relu;vgg19/fc1/BiasAdd]:22
	         FULLY_CONNECTED	        46394.351	  277.959	  278.857	  0.597%	 99.853%	     0.000	        1	[vgg19/fc2/MatMul;vgg19/fc2/Relu;vgg19/fc2/BiasAdd]:23
	         FULLY_CONNECTED	        46673.220	   68.607	   68.796	  0.147%	100.000%	     0.000	        1	[vgg19/predictions/MatMul;vgg19/predictions/BiasAdd]:24
	                 SOFTMAX	        46742.028	    0.100	    0.090	  0.000%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:25

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          381.930	 5092.169	 5140.895	 10.998%	 10.998%	     0.000	        1	[vgg19/block1_conv2/Relu;vgg19/block1_conv2/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv2/Conv2D]:1
	                 CONV_2D	         7685.502	 4309.294	 4311.921	  9.225%	 20.223%	     0.000	        1	[vgg19/block2_conv2/Relu;vgg19/block2_conv2/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv2/Conv2D]:4
	                 CONV_2D	        22306.420	 4167.042	 4136.924	  8.851%	 29.074%	     0.000	        1	[vgg19/block3_conv4/Relu;vgg19/block3_conv4/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv4/Conv2D]:9
	                 CONV_2D	        18172.209	 4114.595	 4134.199	  8.845%	 37.919%	     0.000	        1	[vgg19/block3_conv3/Relu;vgg19/block3_conv3/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv3/Conv2D]:8
	                 CONV_2D	        14050.174	 4119.026	 4122.023	  8.819%	 46.738%	     0.000	        1	[vgg19/block3_conv2/Relu;vgg19/block3_conv2/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv2/Conv2D]:7
	                 CONV_2D	        32509.094	 4051.689	 4047.218	  8.659%	 55.396%	     0.000	        1	[vgg19/block4_conv3/Relu;vgg19/block4_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv3/Conv2D]:13
	                 CONV_2D	        28463.428	 4036.712	 4045.649	  8.655%	 64.051%	     0.000	        1	[vgg19/block4_conv2/Relu;vgg19/block4_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv2/Conv2D]:12
	                 CONV_2D	        36556.324	 4036.021	 4035.717	  8.634%	 72.686%	     0.000	        1	[vgg19/block4_conv4/Relu;vgg19/block4_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv4/Conv2D]:14
	                 CONV_2D	         5535.406	 2146.958	 2150.084	  4.600%	 77.285%	     0.000	        1	[vgg19/block2_conv1/Relu;vgg19/block2_conv1/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv1/Conv2D]:3
	                 CONV_2D	        12003.168	 2045.683	 2046.994	  4.379%	 81.665%	     0.000	        1	[vgg19/block3_conv1/Relu;vgg19/block3_conv1/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv1/Conv2D]:6

Number of nodes executed: 26
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       16	 44650.812	    95.526%	    95.526%	     0.000	       16
	         FULLY_CONNECTED	        3	  2067.935	     4.424%	    99.951%	     0.000	        3
	             MAX_POOL_2D	        5	    22.959	     0.049%	   100.000%	     0.000	        5
	                 SOFTMAX	        1	     0.089	     0.000%	   100.000%	     0.000	        1
	                 RESHAPE	        1	     0.013	     0.000%	   100.000%	     0.000	        1

Timings (microseconds): count=4 first=46677179 curr=46733109 min=46677179 max=46815911 avg=4.67418e+07 std=49356
Memory (bytes): count=0
26 nodes observed



double free or corruption (out)
[ perf record: Woken up 1190 times to write data ]
Warning:
Processed 1507286 events and lost 2 chunks!

Check IO/CPU overload!

[ perf record: Captured and wrote 297.837 MB /tmp/data.record (1504674 samples) ]

380.555

