STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 160)
, and the ID is 0
	Allocating LowPrecision Activations Tensors with Shape of (12544, 160)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (3136, 64, ), and Output shape (3136, 128, ), and the ID is 1
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 2
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 96, ), Input shape (3136, 96, ), and Output shape (3136, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
, and the ID is 3
	Allocating LowPrecision Activations Tensors with Shape of (3136, 96)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 4
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (3136, 128, ), and Output shape (3136, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
5
	Allocating LowPrecision Activations Tensors with Shape of (3136, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 6
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (3136, 160, ), and Output shape (3136, 128, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (3136, 192, ), and Output shape (3136, 128, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (3136, 224, ), and Output shape (3136, 128, ), and the ID is 11
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 1152)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (3136, 256, ), and Output shape (3136, 128, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 256)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 15
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (784, 160, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (784, 160)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 17
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (784, 192, ), and Output shape (784, 128, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (784, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (784, 224, ), and Output shape (784, 128, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (784, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 22
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 23
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (784, 288, ), and Output shape (784, 128, ), and the ID is 24
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
	Allocating LowPrecision Activations Tensors with Shape of (784, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 25
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (784, 320, ), and Output shape (784, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 320)
26
	Allocating LowPrecision Activations Tensors with Shape of (784, 320)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 27
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (784, 352, ), and Output shape (784, 128, ), and the ID is 28	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 352)

	Allocating LowPrecision Activations Tensors with Shape of (784, 352)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (784, 384, ), and Output shape (784, 128, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 384)
	Allocating LowPrecision Activations Tensors with Shape of (784, 384)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 31
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (784, 416, ), and Output shape (784, 128, ), and the ID is 32
	Allocating LowPrecision Weight Tensors with Shape of (128, 416)
	Allocating LowPrecision Activations Tensors with Shape of (784, 416)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 33
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (784, 448, ), and Output shape (784, 128, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (784, 448)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (784, 480, ), and Output shape (784, 128, ), and the ID is 36
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (784, 480)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 37
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (784, 512, ), and Output shape (784, 256, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 512)
	Allocating LowPrecision Activations Tensors with Shape of (784, 512)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (196, 256, ), and Output shape (196, 128, ), and the ID is 39
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (200, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 40
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (196, 288, ), and Output shape (196, 128, ), and the ID is 41
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
	Allocating LowPrecision Activations Tensors with Shape of (200, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 42
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 320)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (196, 320, ), and Output shape (196, 128, ), and the ID is 43
	Allocating LowPrecision Activations Tensors with Shape of (200, 320)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 44
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (196, 352, ), and Output shape (196, 128, ), and the ID is 45
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 352)
	Allocating LowPrecision Activations Tensors with Shape of (200, 352)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 46
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (196, 384, ), and Output shape (196, 128, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 384)
	Allocating LowPrecision Activations Tensors with Shape of (200, 384)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 48
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (196, 416, ), and Output shape (196, 128, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 416)
	Allocating LowPrecision Activations Tensors with Shape of (200, 416)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 50
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (196, 448, ), and Output shape (196, 128, ), and the ID is 51
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (200, 448)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 52
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (196, 480, ), and Output shape (196, 128, ), and the ID is 53
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (200, 480)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 54
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (196, 512, ), and Output shape (196, 128, ), and the ID is 55
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (200, 512)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 56
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 544, ), Input shape (196, 544, ), and Output shape (196, 128, ), and the ID is 57
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 544)
	Allocating LowPrecision Activations Tensors with Shape of (200, 544)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 58
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (196, 576, ), and Output shape (196, 128, ), and the ID is 59
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
	Allocating LowPrecision Activations Tensors with Shape of (200, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 60
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 608, ), Input shape (196, 608, ), and Output shape (196, 128, ), and the ID is 61	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 608)

	Allocating LowPrecision Activations Tensors with Shape of (200, 608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 62
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 640, ), Input shape (196, 640, ), and Output shape (196, 128, ), and the ID is 63
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 640)
	Allocating LowPrecision Activations Tensors with Shape of (200, 640)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 64
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 672, ), Input shape (196, 672, ), and Output shape (196, 128, ), and the ID is 65
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 672)
	Allocating LowPrecision Activations Tensors with Shape of (200, 672)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 66
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 704, ), Input shape (196, 704, ), and Output shape (196, 128, ), and the ID is 67
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 704)
	Allocating LowPrecision Activations Tensors with Shape of (200, 704)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 68
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 736, ), Input shape (196, 736, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 736)
, and the ID is 69
	Allocating LowPrecision Activations Tensors with Shape of (200, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 70
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 768, ), Input shape (196, 768, ), and Output shape (196, 128, ), and the ID is 71
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 768)
	Allocating LowPrecision Activations Tensors with Shape of (200, 768)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 72
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 800, ), Input shape (196, 800, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 800)
73
	Allocating LowPrecision Activations Tensors with Shape of (200, 800)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 74
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 832, ), Input shape (196, 832, ), and Output shape (196, 128, ), and the ID is 75
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 832)
	Allocating LowPrecision Activations Tensors with Shape of (200, 832)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 76
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 864, ), Input shape (196, 864, ), and Output shape (196, 128, ), and the ID is 77
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 864)
	Allocating LowPrecision Activations Tensors with Shape of (200, 864)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 78
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (196, 896, ), and Output shape (196, 128, ), and the ID is 79
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 896)
	Allocating LowPrecision Activations Tensors with Shape of (200, 896)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 80
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (196, 928, ), and Output shape (196, 128, ), and the ID is 81
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 928)
	Allocating LowPrecision Activations Tensors with Shape of (200, 928)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 82
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (196, 960, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 960)
83
	Allocating LowPrecision Activations Tensors with Shape of (200, 960)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 84
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (196, 992, ), and Output shape (196, 128, ), and the ID is 85
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 992)
	Allocating LowPrecision Activations Tensors with Shape of (200, 992)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 86
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (196, 1024, ), and Output shape (196, 128, ), and the ID is 87
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1024)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1024)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 88
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (196, 1056, ), and Output shape (196, 128, ), and the ID is 89
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1056)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1056)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 90
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (196, 1088, ), and Output shape (196, 128, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1088)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1088)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 92
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (196, 1120, ), and Output shape (196, 128, ), and the ID is 93
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1120)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1120)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (196, 1152, ), and Output shape (196, 128, ), and the ID is 95
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 96
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (196, 1184, ), and Output shape (196, 128, ), and the ID is 97
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1184)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1184)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 98
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (196, 1216, ), and Output shape (196, 128, ), and the ID is 99
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1216)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1216)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 100
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (196, 1248, ), and Output shape (196, 128, ), and the ID is 101	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1248)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1248)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 102
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (196, 1280, ), and Output shape (196, 128, ), and the ID is 103
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1280)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1280)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 104
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (196, 1312, ), and Output shape (196, 128, ), and the ID is 105
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1312)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1312)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 106
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (196, 1344, ), and Output shape (196, 128, ), and the ID is 107
	Allocating LowPrecision Weight Tensors with Shape of (128, 1344)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1344)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 108
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (196, 1376, ), and Output shape (196, 128, ), and the ID is 109	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1376)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1376)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 110
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (196, 1408, ), and Output shape (196, 128, ), and the ID is 111
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1408)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1408)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 112
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (196, 1440, ), and Output shape (196, 128, ), and the ID is 113	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1440)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1440)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 114
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (196, 1472, ), and Output shape (196, 128, ), and the ID is 115	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1472)

	Allocating LowPrecision Activations Tensors with Shape of (200, 1472)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (196, 1504, ), and Output shape (196, 128, ), and the ID is 117
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1504)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1504)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 118
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (196, 1536, ), and Output shape (196, 128, ), and the ID is 119
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1536)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1536)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 120
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (196, 1568, ), and Output shape (196, 128, ), and the ID is 121
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1568)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1568)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 122
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (196, 1600, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1600)
123
	Allocating LowPrecision Activations Tensors with Shape of (200, 1600)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 124
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (196, 1632, ), and Output shape (196, 128, ), and the ID is 125
	Allocating LowPrecision Weight Tensors with Shape of (128, 1632)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1632)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 126
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (196, 1664, ), and Output shape (196, 128, ), and the ID is 127
	Allocating LowPrecision Weight Tensors with Shape of (128, 1664)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1664)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 128
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (196, 1696, ), and Output shape (196, 128, ), and the ID is 129
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1696)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1696)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 130
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (196, 1728, ), and Output shape (196, 128, ), and the ID is 131
	Allocating LowPrecision Weight Tensors with Shape of (128, 1728)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1728)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 132
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (196, 1760, ), and Output shape (196, 128, ), and the ID is 133
	Allocating LowPrecision Weight Tensors with Shape of (128, 1760)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1760)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 134
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1152)
Applying Conv Low-Precision for Kernel shape (896, 1792, ), Input shape (196, 1792, ), and Output shape (196, 896, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
135
	Allocating LowPrecision Weight Tensors with Shape of (896, 1792)
	Allocating LowPrecision Activations Tensors with Shape of (200, 1792)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (49, 896, ), and Output shape (49, 128, ), and the ID is 136
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 896)
	Allocating LowPrecision Activations Tensors with Shape of (56, 896)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 137
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (49, 928, ), and Output shape (49, 128, ), and the ID is 138
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 928)
	Allocating LowPrecision Activations Tensors with Shape of (56, 928)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (49, 960, ), and Output shape (49, 128, ), and the ID is 140
	Allocating LowPrecision Weight Tensors with Shape of (128, 960)
	Allocating LowPrecision Activations Tensors with Shape of (56, 960)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 141
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (49, 992, ), and Output shape (49, 128, ), and the ID is 142
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 992)
	Allocating LowPrecision Activations Tensors with Shape of (56, 992)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 143
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (49, 1024, ), and Output shape (49, 128, ), and the ID is 144	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1024)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1024)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 145
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (49, 1056, ), and Output shape (49, 128, ), and the ID is 146
	Allocating LowPrecision Weight Tensors with Shape of (128, 1056)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1056)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 147
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (49, 1088, ), and Output shape (49, 128, ), and the ID is 148
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1088)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1088)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1120)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (49, 1120, ), and Output shape (49, 128, ), and the ID is 150
	Allocating LowPrecision Activations Tensors with Shape of (56, 1120)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 151
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (49, 1152, ), and Output shape (49, 128, ), and the ID is 152
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 153
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (49, 1184, ), and Output shape (49, 128, ), and the ID is 154	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1184)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1184)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 155
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (49, 1216, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1216)
, and the ID is 156
	Allocating LowPrecision Activations Tensors with Shape of (56, 1216)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 157
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (49, 1248, ), and Output shape (49, 128, ), and the ID is 158
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1248)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1248)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 159
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (49, 1280, ), and Output shape (49, 128, ), and the ID is 160
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1280)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1280)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 161
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (49, 1312, ), and Output shape (49, 128, ), and the ID is 162
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1312)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1312)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 163
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (49, 1344, ), and Output shape (49, 128, ), and the ID is 164	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1344)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1344)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 165
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (49, 1376, ), and Output shape (49, 128, ), and the ID is 166
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1376)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1376)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 167
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (49, 1408, ), and Output shape (49, 128, ), and the ID is 168
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1408)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1408)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 169
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (49, 1440, ), and Output shape (49, 128, ), and the ID is 170
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1440)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1440)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 171
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (49, 1472, ), and Output shape (49, 128, ), and the ID is 172
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1472)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1472)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 173
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (49, 1504, ), and Output shape (49, 128, ), and the ID is 174	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1504)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1504)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 175
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (49, 1536, ), and Output shape (49, 128, ), and the ID is 176	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1536)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1536)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 177
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (49, 1568, ), and Output shape (49, 128, ), and the ID is 178
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1568)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1568)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 179
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (49, 1600, ), and Output shape (49, 128, ), and the ID is 180	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1600)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1600)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 181
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (49, 1632, ), and Output shape (49, 128, ), and the ID is 182	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1632)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1632)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 183
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (49, 1664, ), and Output shape (49, 128, ), and the ID is 184
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1664)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1664)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 185
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (49, 1696, ), and Output shape (49, 128, ), and the ID is 186
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1696)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1696)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 187
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1728)
(49, 1728, ), and Output shape (49, 128, ), and the ID is 188
	Allocating LowPrecision Activations Tensors with Shape of (56, 1728)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 189
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (49, 1760, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1760)
(49, 128, ), and the ID is 190
	Allocating LowPrecision Activations Tensors with Shape of (56, 1760)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 191
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1792, ), Input shape (49, 1792, ), and Output shape (49, 128, ), and the ID is 192
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1792)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1792)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 193
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1824, ), Input shape (49, 1824, ), and Output shape (49, 128, ), and the ID is 194	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1824)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1824)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 195
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1856, ), Input shape (49, 1856, ), and Output shape (49, 128, ), and the ID is 196
	Allocating LowPrecision Weight Tensors with Shape of (128, 1856)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1856)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 197
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Conv Low-Precision for Kernel shape (128, 1888, ), Input shape (49, 1888, ), and Output shape (49, 128, ), and the ID is 198	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 1888)

	Allocating LowPrecision Activations Tensors with Shape of (56, 1888)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 199
	Allocating LowPrecision Weight Tensors with Shape of (32, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (56, 1152)
Applying Low-Precision for shape (1000, 1920, ) and Input shape (1, 1920, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 1920)
	Transformed Activation Shape From: (1, 1920) To: (8, 1920)
The input model file size (MB): 20.5199
Initialized session in 298.509ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=13584820 curr=13623404 min=13578090 max=13635849 avg=1.35932e+07 std=18865

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=12 first=13620898 curr=13607552 min=13580386 max=13623406 avg=1.36013e+07 std=13428

Inference timings in us: Init: 298509, First inference: 13584820, Warmup (avg): 1.35932e+07, Inference (avg): 1.36013e+07
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=42.8672 overall=53.3984
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  277.224	  277.224	100.000%	100.000%	 29748.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  277.224	  277.224	100.000%	100.000%	 29748.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   277.224	   100.000%	   100.000%	 29748.000	        1

Timings (microseconds): count=1 curr=277224
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.029	    3.712	    3.714	  0.027%	  0.027%	     0.000	        1	[densenet201/zero_padding2d/Pad]:0
	                 CONV_2D	            3.753	  331.643	  331.413	  2.438%	  2.465%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                     PAD	          335.177	   18.118	   18.211	  0.134%	  2.599%	     0.000	        1	[densenet201/zero_padding2d_1/Pad]:2
	             MAX_POOL_2D	          353.399	    5.168	    5.219	  0.038%	  2.637%	     0.000	        1	[densenet201/pool1/MaxPool]:3
	                     MUL	          358.628	   14.632	   14.678	  0.108%	  2.745%	     0.000	        1	[densenet201/conv2_block1_0_bn/FusedBatchNormV31]:4
	                     ADD	          373.316	   18.681	   18.794	  0.138%	  2.883%	     0.000	        1	[densenet201/conv2_block1_0_relu/Relu;densenet201/conv2_block1_0_bn/FusedBatchNormV3]:5
	                 CONV_2D	          392.120	   70.331	   70.552	  0.519%	  3.402%	     0.000	        1	[densenet201/conv2_block1_1_relu/Relu;densenet201/conv2_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block1_1_conv/Conv2D]:6
	                 CONV_2D	          462.683	  315.934	  316.844	  2.330%	  5.733%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	           CONCATENATION	          779.541	    0.354	    0.368	  0.003%	  5.735%	     0.000	        1	[densenet201/conv2_block1_concat/concat]:8
	                     MUL	          779.917	   21.671	   21.697	  0.160%	  5.895%	     0.000	        1	[densenet201/conv2_block2_0_bn/FusedBatchNormV31]:9
	                     ADD	          801.624	   27.722	   27.910	  0.205%	  6.100%	     0.000	        1	[densenet201/conv2_block2_0_relu/Relu;densenet201/conv2_block2_0_bn/FusedBatchNormV3]:10
	                 CONV_2D	          829.547	   96.820	   97.611	  0.718%	  6.818%	     0.000	        1	[densenet201/conv2_block2_1_relu/Relu;densenet201/conv2_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block2_1_conv/Conv2D]:11
	                 CONV_2D	          927.170	  317.855	  320.828	  2.360%	  9.178%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	           CONCATENATION	         1248.010	    0.387	    0.433	  0.003%	  9.181%	     0.000	        1	[densenet201/conv2_block2_concat/concat]:13
	                     MUL	         1248.451	   28.657	   28.854	  0.212%	  9.393%	     0.000	        1	[densenet201/conv2_block3_0_bn/FusedBatchNormV3]:14
	                     ADD	         1277.316	   36.936	   37.224	  0.274%	  9.667%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                 CONV_2D	         1314.550	  123.460	  125.189	  0.921%	 10.588%	     0.000	        1	[densenet201/conv2_block3_1_relu/Relu;densenet201/conv2_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block3_1_conv/Conv2D]:16
	                 CONV_2D	         1439.751	  315.754	  317.794	  2.337%	 12.925%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	           CONCATENATION	         1757.557	    0.466	    0.456	  0.003%	 12.928%	     0.000	        1	[densenet201/conv2_block3_concat/concat]:18
	                     MUL	         1758.022	   35.787	   35.794	  0.263%	 13.192%	     0.000	        1	[densenet201/conv2_block4_0_bn/FusedBatchNormV3]:19
	                     ADD	         1793.827	   46.120	   46.204	  0.340%	 13.531%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                 CONV_2D	         1840.043	  152.442	  151.516	  1.114%	 14.646%	     0.000	        1	[densenet201/conv2_block4_1_relu/Relu;densenet201/conv2_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block4_1_conv/Conv2D]:21
	                 CONV_2D	         1991.570	  329.014	  319.671	  2.351%	 16.997%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	           CONCATENATION	         2311.254	    0.657	    0.612	  0.005%	 17.002%	     0.000	        1	[densenet201/conv2_block4_concat/concat]:23
	                     MUL	         2311.876	   43.846	   43.085	  0.317%	 17.318%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	         2354.972	   56.238	   55.839	  0.411%	 17.729%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                 CONV_2D	         2410.822	  180.743	  179.958	  1.324%	 19.053%	     0.000	        1	[densenet201/conv2_block5_1_relu/Relu;densenet201/conv2_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block5_1_conv/Conv2D]:26
	                 CONV_2D	         2590.791	  319.043	  318.670	  2.344%	 21.396%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	           CONCATENATION	         2909.474	    0.620	    0.672	  0.005%	 21.401%	     0.000	        1	[densenet201/conv2_block5_concat/concat]:28
	                     MUL	         2910.156	   49.719	   49.828	  0.366%	 21.768%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	         2959.995	   64.246	   64.434	  0.474%	 22.242%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                 CONV_2D	         3024.440	  205.859	  206.491	  1.519%	 23.761%	     0.000	        1	[densenet201/conv2_block6_1_relu/Relu;densenet201/conv2_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block6_1_conv/Conv2D]:31
	                 CONV_2D	         3230.943	  316.730	  318.556	  2.343%	 26.103%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	           CONCATENATION	         3549.512	    0.747	    0.842	  0.006%	 26.110%	     0.000	        1	[densenet201/conv2_block6_concat/concat]:33
	                     MUL	         3550.364	   56.797	   57.104	  0.420%	 26.530%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     ADD	         3607.482	   73.402	   74.084	  0.545%	 27.075%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                 CONV_2D	         3681.578	  233.570	  235.193	  1.730%	 28.804%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	         AVERAGE_POOL_2D	         3916.784	   10.403	   10.380	  0.076%	 28.881%	     0.000	        1	[densenet201/pool2_pool/AvgPool]:37
	                     MUL	         3927.173	    7.201	    7.234	  0.053%	 28.934%	     0.000	        1	[densenet201/conv3_block1_0_bn/FusedBatchNormV3]:38
	                     ADD	         3934.415	    9.222	    9.303	  0.068%	 29.002%	     0.000	        1	[densenet201/conv3_block1_0_relu/Relu;densenet201/conv3_block1_0_bn/FusedBatchNormV3]:39
	                 CONV_2D	         3943.727	   30.542	   31.001	  0.228%	 29.230%	     0.000	        1	[densenet201/conv3_block1_1_relu/Relu;densenet201/conv3_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block1_1_conv/Conv2D]:40
	                 CONV_2D	         3974.739	   79.704	   79.541	  0.585%	 29.815%	     0.000	        1	[densenet201/conv3_block1_2_conv/Conv2D1]:41
	           CONCATENATION	         4054.292	    0.116	    0.130	  0.001%	 29.816%	     0.000	        1	[densenet201/conv3_block1_concat/concat]:42
	                     MUL	         4054.431	    8.984	    8.971	  0.066%	 29.882%	     0.000	        1	[densenet201/conv3_block2_0_bn/FusedBatchNormV31]:43
	                     ADD	         4063.411	   11.474	   11.559	  0.085%	 29.967%	     0.000	        1	[densenet201/conv3_block2_0_relu/Relu;densenet201/conv3_block2_0_bn/FusedBatchNormV3]:44
	                 CONV_2D	         4074.978	   37.024	   37.527	  0.276%	 30.243%	     0.000	        1	[densenet201/conv3_block2_1_relu/Relu;densenet201/conv3_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block2_1_conv/Conv2D]:45
	                 CONV_2D	         4112.524	   79.379	   79.072	  0.582%	 30.825%	     0.000	        1	[densenet201/conv3_block2_2_conv/Conv2D1]:46
	           CONCATENATION	         4191.607	    0.149	    0.157	  0.001%	 30.826%	     0.000	        1	[densenet201/conv3_block2_concat/concat]:47
	                     MUL	         4191.772	   10.652	   10.687	  0.079%	 30.905%	     0.000	        1	[densenet201/conv3_block3_0_bn/FusedBatchNormV31]:48
	                     ADD	         4202.467	   13.753	   13.866	  0.102%	 31.007%	     0.000	        1	[densenet201/conv3_block3_0_relu/Relu;densenet201/conv3_block3_0_bn/FusedBatchNormV3]:49
	                 CONV_2D	         4216.344	   43.886	   44.225	  0.325%	 31.332%	     0.000	        1	[densenet201/conv3_block3_1_relu/Relu;densenet201/conv3_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block3_1_conv/Conv2D]:50
	                 CONV_2D	         4260.579	   79.270	   79.216	  0.583%	 31.915%	     0.000	        1	[densenet201/conv3_block3_2_conv/Conv2D1]:51
	           CONCATENATION	         4339.806	    0.177	    0.167	  0.001%	 31.916%	     0.000	        1	[densenet201/conv3_block3_concat/concat]:52
	                     MUL	         4339.979	   12.363	   12.422	  0.091%	 32.007%	     0.000	        1	[densenet201/conv3_block4_0_bn/FusedBatchNormV31]:53
	                     ADD	         4352.410	   16.016	   16.090	  0.118%	 32.125%	     0.000	        1	[densenet201/conv3_block4_0_relu/Relu;densenet201/conv3_block4_0_bn/FusedBatchNormV3]:54
	                 CONV_2D	         4368.510	   50.921	   50.611	  0.372%	 32.498%	     0.000	        1	[densenet201/conv3_block4_1_relu/Relu;densenet201/conv3_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block4_1_conv/Conv2D]:55
	                 CONV_2D	         4419.131	   80.995	   78.973	  0.581%	 33.079%	     0.000	        1	[densenet201/conv3_block4_2_conv/Conv2D1]:56
	           CONCATENATION	         4498.117	    0.249	    0.198	  0.001%	 33.080%	     0.000	        1	[densenet201/conv3_block4_concat/concat]:57
	                     MUL	         4498.322	   16.501	   14.395	  0.106%	 33.186%	     0.000	        1	[densenet201/conv3_block5_0_bn/FusedBatchNormV3]:58
	                     ADD	         4512.726	   18.629	   18.392	  0.135%	 33.321%	     0.000	        1	[densenet201/conv3_block5_0_relu/Relu;densenet201/conv3_block5_0_bn/FusedBatchNormV3]:59
	                 CONV_2D	         4531.129	   59.844	   57.955	  0.426%	 33.747%	     0.000	        1	[densenet201/conv3_block5_1_relu/Relu;densenet201/conv3_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block5_1_conv/Conv2D]:60
	                 CONV_2D	         4589.096	   81.355	   79.539	  0.585%	 34.332%	     0.000	        1	[densenet201/conv3_block5_2_conv/Conv2D1]:61
	           CONCATENATION	         4668.647	    0.272	    0.231	  0.002%	 34.334%	     0.000	        1	[densenet201/conv3_block5_concat/concat]:62
	                     MUL	         4668.886	   16.339	   16.010	  0.118%	 34.452%	     0.000	        1	[densenet201/conv3_block6_0_bn/FusedBatchNormV3]:63
	                     ADD	         4684.907	   21.499	   20.705	  0.152%	 34.604%	     0.000	        1	[densenet201/conv3_block6_0_relu/Relu;densenet201/conv3_block6_0_bn/FusedBatchNormV3]:64
	                 CONV_2D	         4705.623	   66.663	   65.073	  0.479%	 35.083%	     0.000	        1	[densenet201/conv3_block6_1_relu/Relu;densenet201/conv3_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block6_1_conv/Conv2D]:65
	                 CONV_2D	         4770.708	   81.254	   79.804	  0.587%	 35.670%	     0.000	        1	[densenet201/conv3_block6_2_conv/Conv2D1]:66
	           CONCATENATION	         4850.523	    0.226	    0.234	  0.002%	 35.671%	     0.000	        1	[densenet201/conv3_block6_concat/concat]:67
	                     MUL	         4850.765	   17.798	   17.809	  0.131%	 35.802%	     0.000	        1	[densenet201/conv3_block7_0_bn/FusedBatchNormV3]:68
	                     ADD	         4868.584	   23.031	   22.983	  0.169%	 35.971%	     0.000	        1	[densenet201/conv3_block7_0_relu/Relu;densenet201/conv3_block7_0_bn/FusedBatchNormV3]:69
	                 CONV_2D	         4891.577	   72.800	   71.848	  0.528%	 36.500%	     0.000	        1	[densenet201/conv3_block7_1_relu/Relu;densenet201/conv3_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block7_1_conv/Conv2D]:70
	                 CONV_2D	         4963.436	   79.684	   79.012	  0.581%	 37.081%	     0.000	        1	[densenet201/conv3_block7_2_conv/Conv2D1]:71
	           CONCATENATION	         5042.460	    0.213	    0.239	  0.002%	 37.083%	     0.000	        1	[densenet201/conv3_block7_concat/concat]:72
	                     MUL	         5042.708	   19.651	   19.530	  0.144%	 37.226%	     0.000	        1	[densenet201/conv3_block8_0_bn/FusedBatchNormV3]:73
	                     ADD	         5062.249	   25.323	   25.213	  0.185%	 37.412%	     0.000	        1	[densenet201/conv3_block8_0_relu/Relu;densenet201/conv3_block8_0_bn/FusedBatchNormV3]:74
	                 CONV_2D	         5087.474	   79.730	   78.507	  0.577%	 37.989%	     0.000	        1	[densenet201/conv3_block8_1_relu/Relu;densenet201/conv3_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block8_1_conv/Conv2D]:75
	                 CONV_2D	         5165.992	   79.971	   79.316	  0.583%	 38.573%	     0.000	        1	[densenet201/conv3_block8_2_conv/Conv2D1]:76
	           CONCATENATION	         5245.320	    0.231	    0.268	  0.002%	 38.575%	     0.000	        1	[densenet201/conv3_block8_concat/concat]:77
	                     MUL	         5245.596	   21.207	   21.241	  0.156%	 38.731%	     0.000	        1	[densenet201/conv3_block9_0_bn/FusedBatchNormV3]:78
	                     ADD	         5266.848	   27.276	   27.425	  0.202%	 38.933%	     0.000	        1	[densenet201/conv3_block9_0_relu/Relu;densenet201/conv3_block9_0_bn/FusedBatchNormV3]:79
	                 CONV_2D	         5294.283	   83.663	   84.853	  0.624%	 39.557%	     0.000	        1	[densenet201/conv3_block9_1_relu/Relu;densenet201/conv3_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block9_1_conv/Conv2D]:80
	                 CONV_2D	         5379.146	   77.892	   78.493	  0.577%	 40.134%	     0.000	        1	[densenet201/conv3_block9_2_conv/Conv2D1]:81
	           CONCATENATION	         5457.651	    0.292	    0.309	  0.002%	 40.136%	     0.000	        1	[densenet201/conv3_block9_concat/concat]:82
	                     MUL	         5457.969	   22.954	   22.978	  0.169%	 40.305%	     0.000	        1	[densenet201/conv3_block10_0_bn/FusedBatchNormV3]:83
	                     ADD	         5480.957	   29.592	   29.655	  0.218%	 40.523%	     0.000	        1	[densenet201/conv3_block10_0_relu/Relu;densenet201/conv3_block10_0_bn/FusedBatchNormV3]:84
	                 CONV_2D	         5510.623	   90.645	   91.330	  0.672%	 41.195%	     0.000	        1	[densenet201/conv3_block10_1_relu/Relu;densenet201/conv3_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block10_1_conv/Conv2D]:85
	                 CONV_2D	         5601.964	   78.830	   78.559	  0.578%	 41.773%	     0.000	        1	[densenet201/conv3_block10_2_conv/Conv2D1]:86
	           CONCATENATION	         5680.534	    0.290	    0.289	  0.002%	 41.775%	     0.000	        1	[densenet201/conv3_block10_concat/concat]:87
	                     MUL	         5680.832	   24.612	   24.731	  0.182%	 41.957%	     0.000	        1	[densenet201/conv3_block11_0_bn/FusedBatchNormV3]:88
	                     ADD	         5705.573	   31.800	   31.954	  0.235%	 42.192%	     0.000	        1	[densenet201/conv3_block11_0_relu/Relu;densenet201/conv3_block11_0_bn/FusedBatchNormV3]:89
	                 CONV_2D	         5737.538	   97.399	   98.359	  0.723%	 42.915%	     0.000	        1	[densenet201/conv3_block11_1_relu/Relu;densenet201/conv3_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block11_1_conv/Conv2D]:90
	                 CONV_2D	         5835.907	   77.673	   79.457	  0.584%	 43.500%	     0.000	        1	[densenet201/conv3_block11_2_conv/Conv2D1]:91
	           CONCATENATION	         5915.376	    0.312	    0.341	  0.003%	 43.502%	     0.000	        1	[densenet201/conv3_block11_concat/concat]:92
	                     MUL	         5915.725	   26.487	   26.570	  0.195%	 43.698%	     0.000	        1	[densenet201/conv3_block12_0_bn/FusedBatchNormV3]:93
	                     ADD	         5942.307	   34.132	   34.401	  0.253%	 43.951%	     0.000	        1	[densenet201/conv3_block12_0_relu/Relu;densenet201/conv3_block12_0_bn/FusedBatchNormV3]:94
	                 CONV_2D	         5976.718	  103.816	  105.860	  0.779%	 44.729%	     0.000	        1	[densenet201/conv3_block12_1_relu/Relu;densenet201/conv3_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block12_1_conv/Conv2D]:95
	                 CONV_2D	         6082.589	   79.664	   80.282	  0.590%	 45.320%	     0.000	        1	[densenet201/conv3_block12_2_conv/Conv2D1]:96
	           CONCATENATION	         6162.882	    0.293	    0.379	  0.003%	 45.322%	     0.000	        1	[densenet201/conv3_block12_concat/concat]:97
	                     MUL	         6163.271	   28.170	   28.585	  0.210%	 45.533%	     0.000	        1	[densenet201/pool3_bn/FusedBatchNormV3]:98
	                     ADD	         6191.866	   36.344	   36.797	  0.271%	 45.803%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99
	                 CONV_2D	         6228.673	  213.866	  217.967	  1.603%	 47.406%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100
	         AVERAGE_POOL_2D	         6446.651	    5.098	    5.127	  0.038%	 47.444%	     0.000	        1	[densenet201/pool3_pool/AvgPool]:101
	                     MUL	         6451.787	    3.552	    3.583	  0.026%	 47.471%	     0.000	        1	[densenet201/conv4_block1_0_bn/FusedBatchNormV31]:102
	                     ADD	         6455.379	    4.577	    4.628	  0.034%	 47.505%	     0.000	        1	[densenet201/conv4_block1_0_relu/Relu;densenet201/conv4_block1_0_bn/FusedBatchNormV3]:103
	                 CONV_2D	         6460.015	   14.595	   14.628	  0.108%	 47.612%	     0.000	        1	[densenet201/conv4_block1_1_relu/Relu;densenet201/conv4_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block1_1_conv/Conv2D]:104
	                 CONV_2D	         6474.653	   18.782	   19.200	  0.141%	 47.753%	     0.000	        1	[densenet201/conv4_block1_2_conv/Conv2D1]:105
	           CONCATENATION	         6493.862	    0.083	    0.081	  0.001%	 47.754%	     0.000	        1	[densenet201/conv4_block1_concat/concat]:106
	                     MUL	         6493.950	    3.982	    4.019	  0.030%	 47.784%	     0.000	        1	[densenet201/conv4_block2_0_bn/FusedBatchNormV31]:107
	                     ADD	         6497.978	    5.123	    5.174	  0.038%	 47.822%	     0.000	        1	[densenet201/conv4_block2_0_relu/Relu;densenet201/conv4_block2_0_bn/FusedBatchNormV3]:108
	                 CONV_2D	         6503.160	   16.154	   16.298	  0.120%	 47.941%	     0.000	        1	[densenet201/conv4_block2_1_relu/Relu;densenet201/conv4_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block2_1_conv/Conv2D]:109
	                 CONV_2D	         6519.467	   18.721	   19.204	  0.141%	 48.083%	     0.000	        1	[densenet201/conv4_block2_2_conv/Conv2D1]:110
	           CONCATENATION	         6538.682	    0.076	    0.077	  0.001%	 48.083%	     0.000	        1	[densenet201/conv4_block2_concat/concat]:111
	                     MUL	         6538.766	    4.416	    4.447	  0.033%	 48.116%	     0.000	        1	[densenet201/conv4_block3_0_bn/FusedBatchNormV31]:112
	                     ADD	         6543.221	    5.680	    5.741	  0.042%	 48.158%	     0.000	        1	[densenet201/conv4_block3_0_relu/Relu;densenet201/conv4_block3_0_bn/FusedBatchNormV3]:113
	                 CONV_2D	         6548.971	   17.859	   18.037	  0.133%	 48.291%	     0.000	        1	[densenet201/conv4_block3_1_relu/Relu;densenet201/conv4_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block3_1_conv/Conv2D]:114
	                 CONV_2D	         6567.018	   19.286	   19.299	  0.142%	 48.433%	     0.000	        1	[densenet201/conv4_block3_2_conv/Conv2D1]:115
	           CONCATENATION	         6586.328	    0.077	    0.091	  0.001%	 48.433%	     0.000	        1	[densenet201/conv4_block3_concat/concat]:116
	                     MUL	         6586.426	    4.869	    4.887	  0.036%	 48.469%	     0.000	        1	[densenet201/conv4_block4_0_bn/FusedBatchNormV31]:117
	                     ADD	         6591.321	    6.243	    6.293	  0.046%	 48.516%	     0.000	        1	[densenet201/conv4_block4_0_relu/Relu;densenet201/conv4_block4_0_bn/FusedBatchNormV3]:118
	                 CONV_2D	         6597.622	   19.516	   19.685	  0.145%	 48.660%	     0.000	        1	[densenet201/conv4_block4_1_relu/Relu;densenet201/conv4_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block4_1_conv/Conv2D]:119
	                 CONV_2D	         6617.316	   18.717	   19.012	  0.140%	 48.800%	     0.000	        1	[densenet201/conv4_block4_2_conv/Conv2D1]:120
	           CONCATENATION	         6636.338	    0.087	    0.097	  0.001%	 48.801%	     0.000	        1	[densenet201/conv4_block4_concat/concat]:121
	                     MUL	         6636.442	    5.287	    5.348	  0.039%	 48.840%	     0.000	        1	[densenet201/conv4_block5_0_bn/FusedBatchNormV31]:122
	                     ADD	         6641.798	    6.890	    6.880	  0.051%	 48.891%	     0.000	        1	[densenet201/conv4_block5_0_relu/Relu;densenet201/conv4_block5_0_bn/FusedBatchNormV3]:123
	                 CONV_2D	         6648.686	   21.358	   21.394	  0.157%	 49.048%	     0.000	        1	[densenet201/conv4_block5_1_relu/Relu;densenet201/conv4_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block5_1_conv/Conv2D]:124
	                 CONV_2D	         6670.089	   18.836	   19.054	  0.140%	 49.188%	     0.000	        1	[densenet201/conv4_block5_2_conv/Conv2D1]:125
	           CONCATENATION	         6689.154	    0.091	    0.102	  0.001%	 49.189%	     0.000	        1	[densenet201/conv4_block5_concat/concat]:126
	                     MUL	         6689.263	    5.745	    5.758	  0.042%	 49.232%	     0.000	        1	[densenet201/conv4_block6_0_bn/FusedBatchNormV31]:127
	                     ADD	         6695.029	    7.378	    7.436	  0.055%	 49.286%	     0.000	        1	[densenet201/conv4_block6_0_relu/Relu;densenet201/conv4_block6_0_bn/FusedBatchNormV3]:128
	                 CONV_2D	         6702.474	   22.956	   23.094	  0.170%	 49.456%	     0.000	        1	[densenet201/conv4_block6_1_relu/Relu;densenet201/conv4_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block6_1_conv/Conv2D]:129
	                 CONV_2D	         6725.577	   19.386	   19.015	  0.140%	 49.596%	     0.000	        1	[densenet201/conv4_block6_2_conv/Conv2D1]:130
	           CONCATENATION	         6744.602	    0.122	    0.110	  0.001%	 49.597%	     0.000	        1	[densenet201/conv4_block6_concat/concat]:131
	                     MUL	         6744.720	    6.200	    6.204	  0.046%	 49.642%	     0.000	        1	[densenet201/conv4_block7_0_bn/FusedBatchNormV31]:132
	                     ADD	         6750.931	    8.023	    8.012	  0.059%	 49.701%	     0.000	        1	[densenet201/conv4_block7_0_relu/Relu;densenet201/conv4_block7_0_bn/FusedBatchNormV3]:133
	                 CONV_2D	         6758.954	   24.841	   24.773	  0.182%	 49.884%	     0.000	        1	[densenet201/conv4_block7_1_relu/Relu;densenet201/conv4_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block7_1_conv/Conv2D]:134
	                 CONV_2D	         6783.737	   18.907	   19.150	  0.141%	 50.024%	     0.000	        1	[densenet201/conv4_block7_2_conv/Conv2D1]:135
	           CONCATENATION	         6802.898	    0.135	    0.118	  0.001%	 50.025%	     0.000	        1	[densenet201/conv4_block7_concat/concat]:136
	                     MUL	         6803.023	    6.603	    6.641	  0.049%	 50.074%	     0.000	        1	[densenet201/conv4_block8_0_bn/FusedBatchNormV31]:137
	                     ADD	         6809.673	    8.518	    8.571	  0.063%	 50.137%	     0.000	        1	[densenet201/conv4_block8_0_relu/Relu;densenet201/conv4_block8_0_bn/FusedBatchNormV3]:138
	                 CONV_2D	         6818.253	   26.389	   26.453	  0.195%	 50.332%	     0.000	        1	[densenet201/conv4_block8_1_relu/Relu;densenet201/conv4_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block8_1_conv/Conv2D]:139
	                 CONV_2D	         6844.716	   19.240	   19.133	  0.141%	 50.472%	     0.000	        1	[densenet201/conv4_block8_2_conv/Conv2D1]:140
	           CONCATENATION	         6863.859	    0.115	    0.117	  0.001%	 50.473%	     0.000	        1	[densenet201/conv4_block8_concat/concat]:141
	                     MUL	         6863.984	    7.161	    7.074	  0.052%	 50.525%	     0.000	        1	[densenet201/conv4_block9_0_bn/FusedBatchNormV31]:142
	                     ADD	         6871.066	    9.195	    9.122	  0.067%	 50.592%	     0.000	        1	[densenet201/conv4_block9_0_relu/Relu;densenet201/conv4_block9_0_bn/FusedBatchNormV3]:143
	                 CONV_2D	         6880.197	   28.327	   28.124	  0.207%	 50.799%	     0.000	        1	[densenet201/conv4_block9_1_relu/Relu;densenet201/conv4_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block9_1_conv/Conv2D]:144
	                 CONV_2D	         6908.331	   19.911	   19.267	  0.142%	 50.941%	     0.000	        1	[densenet201/conv4_block9_2_conv/Conv2D1]:145
	           CONCATENATION	         6927.608	    0.106	    0.126	  0.001%	 50.942%	     0.000	        1	[densenet201/conv4_block9_concat/concat]:146
	                     MUL	         6927.741	    7.643	    7.528	  0.055%	 50.997%	     0.000	        1	[densenet201/conv4_block10_0_bn/FusedBatchNormV31]:147
	                     ADD	         6935.278	    9.849	    9.765	  0.072%	 51.069%	     0.000	        1	[densenet201/conv4_block10_0_relu/Relu;densenet201/conv4_block10_0_bn/FusedBatchNormV3]:148
	                 CONV_2D	         6945.052	   31.182	   30.626	  0.225%	 51.294%	     0.000	        1	[densenet201/conv4_block10_1_relu/Relu;densenet201/conv4_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block10_1_conv/Conv2D]:149
	                 CONV_2D	         6975.688	   19.782	   19.210	  0.141%	 51.436%	     0.000	        1	[densenet201/conv4_block10_2_conv/Conv2D1]:150
	           CONCATENATION	         6994.909	    0.123	    0.139	  0.001%	 51.437%	     0.000	        1	[densenet201/conv4_block10_concat/concat]:151
	                     MUL	         6995.055	    8.052	    7.967	  0.059%	 51.495%	     0.000	        1	[densenet201/conv4_block11_0_bn/FusedBatchNormV31]:152
	                     ADD	         7003.031	   10.506	   10.372	  0.076%	 51.572%	     0.000	        1	[densenet201/conv4_block11_0_relu/Relu;densenet201/conv4_block11_0_bn/FusedBatchNormV3]:153
	                 CONV_2D	         7013.412	   33.134	   32.370	  0.238%	 51.810%	     0.000	        1	[densenet201/conv4_block11_1_relu/Relu;densenet201/conv4_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block11_1_conv/Conv2D]:154
	                 CONV_2D	         7045.794	   19.790	   19.261	  0.142%	 51.951%	     0.000	        1	[densenet201/conv4_block11_2_conv/Conv2D1]:155
	           CONCATENATION	         7065.065	    0.165	    0.151	  0.001%	 51.952%	     0.000	        1	[densenet201/conv4_block11_concat/concat]:156
	                     MUL	         7065.223	    8.561	    8.495	  0.062%	 52.015%	     0.000	        1	[densenet201/conv4_block12_0_bn/FusedBatchNormV31]:157
	                     ADD	         7073.727	   11.063	   10.947	  0.081%	 52.095%	     0.000	        1	[densenet201/conv4_block12_0_relu/Relu;densenet201/conv4_block12_0_bn/FusedBatchNormV3]:158
	                 CONV_2D	         7084.685	   34.901	   34.169	  0.251%	 52.347%	     0.000	        1	[densenet201/conv4_block12_1_relu/Relu;densenet201/conv4_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block12_1_conv/Conv2D]:159
	                 CONV_2D	         7118.865	   19.519	   20.007	  0.147%	 52.494%	     0.000	        1	[densenet201/conv4_block12_2_conv/Conv2D1]:160
	           CONCATENATION	         7138.882	    0.107	    0.148	  0.001%	 52.495%	     0.000	        1	[densenet201/conv4_block12_concat/concat]:161
	                     MUL	         7139.037	    8.941	    8.886	  0.065%	 52.560%	     0.000	        1	[densenet201/conv4_block13_0_bn/FusedBatchNormV31]:162
	                     ADD	         7147.932	   11.532	   11.460	  0.084%	 52.645%	     0.000	        1	[densenet201/conv4_block13_0_relu/Relu;densenet201/conv4_block13_0_bn/FusedBatchNormV3]:163
	                 CONV_2D	         7159.401	   37.710	   35.988	  0.265%	 52.909%	     0.000	        1	[densenet201/conv4_block13_1_relu/Relu;densenet201/conv4_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block13_1_conv/Conv2D]:164
	                 CONV_2D	         7195.400	   20.290	   19.270	  0.142%	 53.051%	     0.000	        1	[densenet201/conv4_block13_2_conv/Conv2D1]:165
	           CONCATENATION	         7214.681	    0.169	    0.156	  0.001%	 53.052%	     0.000	        1	[densenet201/conv4_block13_concat/concat]:166
	                     MUL	         7214.843	    9.584	    9.341	  0.069%	 53.121%	     0.000	        1	[densenet201/conv4_block14_0_bn/FusedBatchNormV31]:167
	                     ADD	         7224.193	   12.808	   12.154	  0.089%	 53.210%	     0.000	        1	[densenet201/conv4_block14_0_relu/Relu;densenet201/conv4_block14_0_bn/FusedBatchNormV3]:168
	                 CONV_2D	         7236.361	   39.693	   37.843	  0.278%	 53.489%	     0.000	        1	[densenet201/conv4_block14_1_relu/Relu;densenet201/conv4_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block14_1_conv/Conv2D]:169
	                 CONV_2D	         7274.215	   19.395	   19.349	  0.142%	 53.631%	     0.000	        1	[densenet201/conv4_block14_2_conv/Conv2D1]:170
	           CONCATENATION	         7293.574	    0.219	    0.175	  0.001%	 53.632%	     0.000	        1	[densenet201/conv4_block14_concat/concat]:171
	                     MUL	         7293.757	    9.732	    9.729	  0.072%	 53.704%	     0.000	        1	[densenet201/conv4_block15_0_bn/FusedBatchNormV31]:172
	                     ADD	         7303.495	   12.709	   12.675	  0.093%	 53.797%	     0.000	        1	[densenet201/conv4_block15_0_relu/Relu;densenet201/conv4_block15_0_bn/FusedBatchNormV3]:173
	                 CONV_2D	         7316.181	   39.704	   39.402	  0.290%	 54.087%	     0.000	        1	[densenet201/conv4_block15_1_relu/Relu;densenet201/conv4_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block15_1_conv/Conv2D]:174
	                 CONV_2D	         7355.593	   19.567	   19.246	  0.142%	 54.228%	     0.000	        1	[densenet201/conv4_block15_2_conv/Conv2D1]:175
	           CONCATENATION	         7374.849	    0.178	    0.174	  0.001%	 54.230%	     0.000	        1	[densenet201/conv4_block15_concat/concat]:176
	                     MUL	         7375.031	   10.300	   10.213	  0.075%	 54.305%	     0.000	        1	[densenet201/conv4_block16_0_bn/FusedBatchNormV31]:177
	                     ADD	         7385.253	   13.246	   13.228	  0.097%	 54.402%	     0.000	        1	[densenet201/conv4_block16_0_relu/Relu;densenet201/conv4_block16_0_bn/FusedBatchNormV3]:178
	                 CONV_2D	         7398.490	   41.604	   41.035	  0.302%	 54.704%	     0.000	        1	[densenet201/conv4_block16_1_relu/Relu;densenet201/conv4_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block16_1_conv/Conv2D]:179
	                 CONV_2D	         7439.536	   19.500	   19.293	  0.142%	 54.846%	     0.000	        1	[densenet201/conv4_block16_2_conv/Conv2D1]:180
	           CONCATENATION	         7458.839	    0.168	    0.172	  0.001%	 54.847%	     0.000	        1	[densenet201/conv4_block16_concat/concat]:181
	                     MUL	         7459.018	   10.794	   10.604	  0.078%	 54.925%	     0.000	        1	[densenet201/conv4_block17_0_bn/FusedBatchNormV31]:182
	                     ADD	         7469.632	   13.851	   13.740	  0.101%	 55.026%	     0.000	        1	[densenet201/conv4_block17_0_relu/Relu;densenet201/conv4_block17_0_bn/FusedBatchNormV3]:183
	                 CONV_2D	         7483.383	   43.140	   42.633	  0.314%	 55.340%	     0.000	        1	[densenet201/conv4_block17_1_relu/Relu;densenet201/conv4_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block17_1_conv/Conv2D]:184
	                 CONV_2D	         7526.026	   19.359	   19.157	  0.141%	 55.480%	     0.000	        1	[densenet201/conv4_block17_2_conv/Conv2D1]:185
	           CONCATENATION	         7545.193	    0.148	    0.175	  0.001%	 55.482%	     0.000	        1	[densenet201/conv4_block17_concat/concat]:186
	                     MUL	         7545.377	   11.136	   11.034	  0.081%	 55.563%	     0.000	        1	[densenet201/conv4_block18_0_bn/FusedBatchNormV31]:187
	                     ADD	         7556.421	   14.435	   14.363	  0.106%	 55.669%	     0.000	        1	[densenet201/conv4_block18_0_relu/Relu;densenet201/conv4_block18_0_bn/FusedBatchNormV3]:188
	                 CONV_2D	         7570.793	   44.845	   44.361	  0.326%	 55.995%	     0.000	        1	[densenet201/conv4_block18_1_relu/Relu;densenet201/conv4_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block18_1_conv/Conv2D]:189
	                 CONV_2D	         7615.165	   19.430	   19.133	  0.141%	 56.135%	     0.000	        1	[densenet201/conv4_block18_2_conv/Conv2D1]:190
	           CONCATENATION	         7634.308	    0.166	    0.185	  0.001%	 56.137%	     0.000	        1	[densenet201/conv4_block18_concat/concat]:191
	                     MUL	         7634.501	   11.502	   11.491	  0.085%	 56.221%	     0.000	        1	[densenet201/conv4_block19_0_bn/FusedBatchNormV31]:192
	                     ADD	         7646.000	   15.140	   14.934	  0.110%	 56.331%	     0.000	        1	[densenet201/conv4_block19_0_relu/Relu;densenet201/conv4_block19_0_bn/FusedBatchNormV3]:193
	                 CONV_2D	         7660.944	   46.656	   46.094	  0.339%	 56.670%	     0.000	        1	[densenet201/conv4_block19_1_relu/Relu;densenet201/conv4_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block19_1_conv/Conv2D]:194
	                 CONV_2D	         7707.048	   19.413	   19.205	  0.141%	 56.811%	     0.000	        1	[densenet201/conv4_block19_2_conv/Conv2D1]:195
	           CONCATENATION	         7726.264	    0.164	    0.182	  0.001%	 56.813%	     0.000	        1	[densenet201/conv4_block19_concat/concat]:196
	                     MUL	         7726.453	   11.952	   11.923	  0.088%	 56.901%	     0.000	        1	[densenet201/conv4_block20_0_bn/FusedBatchNormV31]:197
	                     ADD	         7738.387	   15.713	   15.469	  0.114%	 57.014%	     0.000	        1	[densenet201/conv4_block20_0_relu/Relu;densenet201/conv4_block20_0_bn/FusedBatchNormV3]:198
	                 CONV_2D	         7753.865	   48.271	   47.574	  0.350%	 57.364%	     0.000	        1	[densenet201/conv4_block20_1_relu/Relu;densenet201/conv4_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block20_1_conv/Conv2D]:199
	                 CONV_2D	         7801.450	   19.405	   19.081	  0.140%	 57.505%	     0.000	        1	[densenet201/conv4_block20_2_conv/Conv2D1]:200
	           CONCATENATION	         7820.543	    0.208	    0.215	  0.002%	 57.506%	     0.000	        1	[densenet201/conv4_block20_concat/concat]:201
	                     MUL	         7820.765	   12.434	   12.328	  0.091%	 57.597%	     0.000	        1	[densenet201/conv4_block21_0_bn/FusedBatchNormV3]:202
	                     ADD	         7833.102	   16.166	   15.977	  0.118%	 57.714%	     0.000	        1	[densenet201/conv4_block21_0_relu/Relu;densenet201/conv4_block21_0_bn/FusedBatchNormV3]:203
	                 CONV_2D	         7849.087	   50.290	   49.014	  0.360%	 58.075%	     0.000	        1	[densenet201/conv4_block21_1_relu/Relu;densenet201/conv4_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block21_1_conv/Conv2D]:204
	                 CONV_2D	         7898.112	   19.347	   18.951	  0.139%	 58.214%	     0.000	        1	[densenet201/conv4_block21_2_conv/Conv2D1]:205
	           CONCATENATION	         7917.072	    0.212	    0.228	  0.002%	 58.216%	     0.000	        1	[densenet201/conv4_block21_concat/concat]:206
	                     MUL	         7917.308	   12.751	   12.734	  0.094%	 58.310%	     0.000	        1	[densenet201/conv4_block22_0_bn/FusedBatchNormV3]:207
	                     ADD	         7930.051	   16.524	   16.529	  0.122%	 58.431%	     0.000	        1	[densenet201/conv4_block22_0_relu/Relu;densenet201/conv4_block22_0_bn/FusedBatchNormV3]:208
	                 CONV_2D	         7946.588	   50.458	   50.523	  0.372%	 58.803%	     0.000	        1	[densenet201/conv4_block22_1_relu/Relu;densenet201/conv4_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block22_1_conv/Conv2D]:209
	                 CONV_2D	         7997.121	   18.802	   18.880	  0.139%	 58.942%	     0.000	        1	[densenet201/conv4_block22_2_conv/Conv2D1]:210
	           CONCATENATION	         8016.010	    0.229	    0.224	  0.002%	 58.943%	     0.000	        1	[densenet201/conv4_block22_concat/concat]:211
	                     MUL	         8016.241	   13.171	   13.186	  0.097%	 59.040%	     0.000	        1	[densenet201/conv4_block23_0_bn/FusedBatchNormV3]:212
	                     ADD	         8029.436	   17.080	   17.089	  0.126%	 59.166%	     0.000	        1	[densenet201/conv4_block23_0_relu/Relu;densenet201/conv4_block23_0_bn/FusedBatchNormV3]:213
	                 CONV_2D	         8046.534	   52.156	   52.195	  0.384%	 59.550%	     0.000	        1	[densenet201/conv4_block23_1_relu/Relu;densenet201/conv4_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block23_1_conv/Conv2D]:214
	                 CONV_2D	         8098.739	   18.951	   18.837	  0.139%	 59.688%	     0.000	        1	[densenet201/conv4_block23_2_conv/Conv2D1]:215
	           CONCATENATION	         8117.586	    0.257	    0.231	  0.002%	 59.690%	     0.000	        1	[densenet201/conv4_block23_concat/concat]:216
	                     MUL	         8117.825	   13.632	   13.621	  0.100%	 59.790%	     0.000	        1	[densenet201/conv4_block24_0_bn/FusedBatchNormV3]:217
	                     ADD	         8131.455	   17.741	   17.700	  0.130%	 59.920%	     0.000	        1	[densenet201/conv4_block24_0_relu/Relu;densenet201/conv4_block24_0_bn/FusedBatchNormV3]:218
	                 CONV_2D	         8149.165	   53.812	   54.380	  0.400%	 60.320%	     0.000	        1	[densenet201/conv4_block24_1_relu/Relu;densenet201/conv4_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block24_1_conv/Conv2D]:219
	                 CONV_2D	         8203.558	   18.755	   19.075	  0.140%	 60.461%	     0.000	        1	[densenet201/conv4_block24_2_conv/Conv2D1]:220
	           CONCATENATION	         8222.645	    0.232	    0.233	  0.002%	 60.462%	     0.000	        1	[densenet201/conv4_block24_concat/concat]:221
	                     MUL	         8222.886	   14.034	   14.074	  0.104%	 60.566%	     0.000	        1	[densenet201/conv4_block25_0_bn/FusedBatchNormV3]:222
	                     ADD	         8236.968	   18.194	   18.289	  0.135%	 60.700%	     0.000	        1	[densenet201/conv4_block25_0_relu/Relu;densenet201/conv4_block25_0_bn/FusedBatchNormV3]:223
	                 CONV_2D	         8255.267	   55.344	   56.147	  0.413%	 61.113%	     0.000	        1	[densenet201/conv4_block25_1_relu/Relu;densenet201/conv4_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block25_1_conv/Conv2D]:224
	                 CONV_2D	         8311.424	   18.798	   19.179	  0.141%	 61.254%	     0.000	        1	[densenet201/conv4_block25_2_conv/Conv2D1]:225
	           CONCATENATION	         8330.613	    0.241	    0.241	  0.002%	 61.256%	     0.000	        1	[densenet201/conv4_block25_concat/concat]:226
	                     MUL	         8330.862	   14.484	   14.584	  0.107%	 61.363%	     0.000	        1	[densenet201/conv4_block26_0_bn/FusedBatchNormV3]:227
	                     ADD	         8345.455	   18.730	   18.925	  0.139%	 61.503%	     0.000	        1	[densenet201/conv4_block26_0_relu/Relu;densenet201/conv4_block26_0_bn/FusedBatchNormV3]:228
	                 CONV_2D	         8364.391	   58.418	   59.253	  0.436%	 61.938%	     0.000	        1	[densenet201/conv4_block26_1_relu/Relu;densenet201/conv4_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block26_1_conv/Conv2D]:229
	                 CONV_2D	         8423.654	   18.757	   19.225	  0.141%	 62.080%	     0.000	        1	[densenet201/conv4_block26_2_conv/Conv2D1]:230
	           CONCATENATION	         8442.889	    0.267	    0.233	  0.002%	 62.082%	     0.000	        1	[densenet201/conv4_block26_concat/concat]:231
	                     MUL	         8443.131	   14.928	   15.021	  0.110%	 62.192%	     0.000	        1	[densenet201/conv4_block27_0_bn/FusedBatchNormV3]:232
	                     ADD	         8458.161	   19.330	   19.547	  0.144%	 62.336%	     0.000	        1	[densenet201/conv4_block27_0_relu/Relu;densenet201/conv4_block27_0_bn/FusedBatchNormV3]:233
	                 CONV_2D	         8477.717	   60.024	   60.833	  0.447%	 62.783%	     0.000	        1	[densenet201/conv4_block27_1_relu/Relu;densenet201/conv4_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block27_1_conv/Conv2D]:234
	                 CONV_2D	         8538.562	   19.277	   19.287	  0.142%	 62.925%	     0.000	        1	[densenet201/conv4_block27_2_conv/Conv2D1]:235
	           CONCATENATION	         8557.862	    0.277	    0.234	  0.002%	 62.927%	     0.000	        1	[densenet201/conv4_block27_concat/concat]:236
	                     MUL	         8558.104	   15.360	   15.482	  0.114%	 63.041%	     0.000	        1	[densenet201/conv4_block28_0_bn/FusedBatchNormV3]:237
	                     ADD	         8573.596	   19.880	   20.151	  0.148%	 63.189%	     0.000	        1	[densenet201/conv4_block28_0_relu/Relu;densenet201/conv4_block28_0_bn/FusedBatchNormV3]:238
	                 CONV_2D	         8593.758	   61.804	   62.778	  0.462%	 63.651%	     0.000	        1	[densenet201/conv4_block28_1_relu/Relu;densenet201/conv4_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block28_1_conv/Conv2D]:239
	                 CONV_2D	         8656.546	   18.695	   19.242	  0.142%	 63.792%	     0.000	        1	[densenet201/conv4_block28_2_conv/Conv2D1]:240
	           CONCATENATION	         8675.799	    0.294	    0.245	  0.002%	 63.794%	     0.000	        1	[densenet201/conv4_block28_concat/concat]:241
	                     MUL	         8676.051	   15.805	   15.908	  0.117%	 63.911%	     0.000	        1	[densenet201/conv4_block29_0_bn/FusedBatchNormV3]:242
	                     ADD	         8691.968	   20.459	   20.677	  0.152%	 64.063%	     0.000	        1	[densenet201/conv4_block29_0_relu/Relu;densenet201/conv4_block29_0_bn/FusedBatchNormV3]:243
	                 CONV_2D	         8712.658	   63.226	   64.109	  0.472%	 64.534%	     0.000	        1	[densenet201/conv4_block29_1_relu/Relu;densenet201/conv4_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block29_1_conv/Conv2D]:244
	                 CONV_2D	         8776.777	   19.153	   19.179	  0.141%	 64.676%	     0.000	        1	[densenet201/conv4_block29_2_conv/Conv2D1]:245
	           CONCATENATION	         8795.968	    0.262	    0.254	  0.002%	 64.677%	     0.000	        1	[densenet201/conv4_block29_concat/concat]:246
	                     MUL	         8796.230	   16.205	   16.504	  0.121%	 64.799%	     0.000	        1	[densenet201/conv4_block30_0_bn/FusedBatchNormV3]:247
	                     ADD	         8812.744	   21.142	   21.259	  0.156%	 64.955%	     0.000	        1	[densenet201/conv4_block30_0_relu/Relu;densenet201/conv4_block30_0_bn/FusedBatchNormV3]:248
	                 CONV_2D	         8834.013	   64.964	   66.103	  0.486%	 65.441%	     0.000	        1	[densenet201/conv4_block30_1_relu/Relu;densenet201/conv4_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block30_1_conv/Conv2D]:249
	                 CONV_2D	         8900.127	   18.702	   19.095	  0.140%	 65.582%	     0.000	        1	[densenet201/conv4_block30_2_conv/Conv2D1]:250
	           CONCATENATION	         8919.232	    0.200	    0.210	  0.002%	 65.583%	     0.000	        1	[densenet201/conv4_block30_concat/concat]:251
	                     MUL	         8919.450	   16.674	   16.754	  0.123%	 65.707%	     0.000	        1	[densenet201/conv4_block31_0_bn/FusedBatchNormV3]:252
	                     ADD	         8936.214	   21.625	   21.783	  0.160%	 65.867%	     0.000	        1	[densenet201/conv4_block31_0_relu/Relu;densenet201/conv4_block31_0_bn/FusedBatchNormV3]:253
	                 CONV_2D	         8958.007	   66.299	   67.398	  0.496%	 66.362%	     0.000	        1	[densenet201/conv4_block31_1_relu/Relu;densenet201/conv4_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block31_1_conv/Conv2D]:254
	                 CONV_2D	         9025.417	   18.827	   19.075	  0.140%	 66.503%	     0.000	        1	[densenet201/conv4_block31_2_conv/Conv2D1]:255
	           CONCATENATION	         9044.503	    0.231	    0.236	  0.002%	 66.504%	     0.000	        1	[densenet201/conv4_block31_concat/concat]:256
	                     MUL	         9044.746	   17.137	   17.175	  0.126%	 66.631%	     0.000	        1	[densenet201/conv4_block32_0_bn/FusedBatchNormV3]:257
	                     ADD	         9061.932	   22.225	   22.302	  0.164%	 66.795%	     0.000	        1	[densenet201/conv4_block32_0_relu/Relu;densenet201/conv4_block32_0_bn/FusedBatchNormV3]:258
	                 CONV_2D	         9084.244	   68.078	   68.763	  0.506%	 67.301%	     0.000	        1	[densenet201/conv4_block32_1_relu/Relu;densenet201/conv4_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block32_1_conv/Conv2D]:259
	                 CONV_2D	         9153.018	   18.825	   19.094	  0.140%	 67.441%	     0.000	        1	[densenet201/conv4_block32_2_conv/Conv2D1]:260
	           CONCATENATION	         9172.121	    0.234	    0.233	  0.002%	 67.443%	     0.000	        1	[densenet201/conv4_block32_concat/concat]:261
	                     MUL	         9172.362	   17.507	   17.595	  0.129%	 67.572%	     0.000	        1	[densenet201/conv4_block33_0_bn/FusedBatchNormV3]:262
	                     ADD	         9189.966	   22.769	   22.891	  0.168%	 67.741%	     0.000	        1	[densenet201/conv4_block33_0_relu/Relu;densenet201/conv4_block33_0_bn/FusedBatchNormV3]:263
	                 CONV_2D	         9212.866	   71.144	   70.567	  0.519%	 68.260%	     0.000	        1	[densenet201/conv4_block33_1_relu/Relu;densenet201/conv4_block33_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block33_1_conv/Conv2D]:264
	                 CONV_2D	         9283.444	   18.885	   19.076	  0.140%	 68.400%	     0.000	        1	[densenet201/conv4_block33_2_conv/Conv2D1]:265
	           CONCATENATION	         9302.530	    0.269	    0.256	  0.002%	 68.402%	     0.000	        1	[densenet201/conv4_block33_concat/concat]:266
	                     MUL	         9302.794	   17.989	   18.035	  0.133%	 68.534%	     0.000	        1	[densenet201/conv4_block34_0_bn/FusedBatchNormV3]:267
	                     ADD	         9320.839	   23.334	   23.397	  0.172%	 68.706%	     0.000	        1	[densenet201/conv4_block34_0_relu/Relu;densenet201/conv4_block34_0_bn/FusedBatchNormV3]:268
	                 CONV_2D	         9344.246	   73.519	   72.070	  0.530%	 69.237%	     0.000	        1	[densenet201/conv4_block34_1_relu/Relu;densenet201/conv4_block34_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block34_1_conv/Conv2D]:269
	                 CONV_2D	         9416.327	   19.767	   19.004	  0.140%	 69.376%	     0.000	        1	[densenet201/conv4_block34_2_conv/Conv2D1]:270
	           CONCATENATION	         9435.344	    0.297	    0.270	  0.002%	 69.378%	     0.000	        1	[densenet201/conv4_block34_concat/concat]:271
	                     MUL	         9435.622	   18.699	   18.511	  0.136%	 69.514%	     0.000	        1	[densenet201/conv4_block35_0_bn/FusedBatchNormV3]:272
	                     ADD	         9454.143	   24.391	   24.039	  0.177%	 69.691%	     0.000	        1	[densenet201/conv4_block35_0_relu/Relu;densenet201/conv4_block35_0_bn/FusedBatchNormV3]:273
	                 CONV_2D	         9478.192	   76.423	   74.295	  0.546%	 70.238%	     0.000	        1	[densenet201/conv4_block35_1_relu/Relu;densenet201/conv4_block35_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block35_1_conv/Conv2D]:274
	                 CONV_2D	         9552.499	   19.757	   19.290	  0.142%	 70.380%	     0.000	        1	[densenet201/conv4_block35_2_conv/Conv2D1]:275
	           CONCATENATION	         9571.800	    0.266	    0.264	  0.002%	 70.381%	     0.000	        1	[densenet201/conv4_block35_concat/concat]:276
	                     MUL	         9572.073	   19.141	   18.961	  0.139%	 70.521%	     0.000	        1	[densenet201/conv4_block36_0_bn/FusedBatchNormV3]:277
	                     ADD	         9591.045	   24.812	   24.684	  0.182%	 70.702%	     0.000	        1	[densenet201/conv4_block36_0_relu/Relu;densenet201/conv4_block36_0_bn/FusedBatchNormV3]:278
	                 CONV_2D	         9615.739	   79.101	   76.773	  0.565%	 71.267%	     0.000	        1	[densenet201/conv4_block36_1_relu/Relu;densenet201/conv4_block36_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block36_1_conv/Conv2D]:279
	                 CONV_2D	         9692.524	   19.978	   19.227	  0.141%	 71.409%	     0.000	        1	[densenet201/conv4_block36_2_conv/Conv2D1]:280
	           CONCATENATION	         9711.761	    0.314	    0.274	  0.002%	 71.411%	     0.000	        1	[densenet201/conv4_block36_concat/concat]:281
	                     MUL	         9712.046	   19.558	   19.482	  0.143%	 71.554%	     0.000	        1	[densenet201/conv4_block37_0_bn/FusedBatchNormV3]:282
	                     ADD	         9731.537	   25.457	   25.378	  0.187%	 71.741%	     0.000	        1	[densenet201/conv4_block37_0_relu/Relu;densenet201/conv4_block37_0_bn/FusedBatchNormV3]:283
	                 CONV_2D	         9756.925	   81.250	   78.356	  0.576%	 72.317%	     0.000	        1	[densenet201/conv4_block37_1_relu/Relu;densenet201/conv4_block37_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block37_1_conv/Conv2D]:284
	                 CONV_2D	         9835.294	   19.541	   19.215	  0.141%	 72.458%	     0.000	        1	[densenet201/conv4_block37_2_conv/Conv2D1]:285
	           CONCATENATION	         9854.519	    0.246	    0.273	  0.002%	 72.460%	     0.000	        1	[densenet201/conv4_block37_concat/concat]:286
	                     MUL	         9854.800	   20.003	   19.863	  0.146%	 72.606%	     0.000	        1	[densenet201/conv4_block38_0_bn/FusedBatchNormV3]:287
	                     ADD	         9874.673	   26.095	   25.826	  0.190%	 72.796%	     0.000	        1	[densenet201/conv4_block38_0_relu/Relu;densenet201/conv4_block38_0_bn/FusedBatchNormV3]:288
	                 CONV_2D	         9900.509	   81.062	   79.983	  0.588%	 73.384%	     0.000	        1	[densenet201/conv4_block38_1_relu/Relu;densenet201/conv4_block38_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block38_1_conv/Conv2D]:289
	                 CONV_2D	         9980.504	   19.427	   19.082	  0.140%	 73.525%	     0.000	        1	[densenet201/conv4_block38_2_conv/Conv2D1]:290
	           CONCATENATION	         9999.595	    0.229	    0.267	  0.002%	 73.527%	     0.000	        1	[densenet201/conv4_block38_concat/concat]:291
	                     MUL	         9999.870	   20.352	   20.277	  0.149%	 73.676%	     0.000	        1	[densenet201/conv4_block39_0_bn/FusedBatchNormV3]:292
	                     ADD	        10020.158	   26.514	   26.342	  0.194%	 73.870%	     0.000	        1	[densenet201/conv4_block39_0_relu/Relu;densenet201/conv4_block39_0_bn/FusedBatchNormV3]:293
	                 CONV_2D	        10046.510	   82.381	   81.235	  0.597%	 74.467%	     0.000	        1	[densenet201/conv4_block39_1_relu/Relu;densenet201/conv4_block39_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block39_1_conv/Conv2D]:294
	                 CONV_2D	        10127.755	   19.423	   19.134	  0.141%	 74.608%	     0.000	        1	[densenet201/conv4_block39_2_conv/Conv2D1]:295
	           CONCATENATION	        10146.906	    0.254	    0.273	  0.002%	 74.610%	     0.000	        1	[densenet201/conv4_block39_concat/concat]:296
	                     MUL	        10147.186	   20.806	   20.726	  0.152%	 74.762%	     0.000	        1	[densenet201/conv4_block40_0_bn/FusedBatchNormV3]:297
	                     ADD	        10167.922	   26.959	   26.909	  0.198%	 74.960%	     0.000	        1	[densenet201/conv4_block40_0_relu/Relu;densenet201/conv4_block40_0_bn/FusedBatchNormV3]:298
	                 CONV_2D	        10194.841	   82.328	   82.774	  0.609%	 75.569%	     0.000	        1	[densenet201/conv4_block40_1_relu/Relu;densenet201/conv4_block40_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block40_1_conv/Conv2D]:299
	                 CONV_2D	        10277.625	   18.824	   18.981	  0.140%	 75.709%	     0.000	        1	[densenet201/conv4_block40_2_conv/Conv2D1]:300
	           CONCATENATION	        10296.616	    0.290	    0.292	  0.002%	 75.711%	     0.000	        1	[densenet201/conv4_block40_concat/concat]:301
	                     MUL	        10296.915	   20.999	   21.093	  0.155%	 75.866%	     0.000	        1	[densenet201/conv4_block41_0_bn/FusedBatchNormV3]:302
	                     ADD	        10318.018	   27.392	   27.474	  0.202%	 76.068%	     0.000	        1	[densenet201/conv4_block41_0_relu/Relu;densenet201/conv4_block41_0_bn/FusedBatchNormV3]:303
	                 CONV_2D	        10345.502	   83.571	   84.009	  0.618%	 76.686%	     0.000	        1	[densenet201/conv4_block41_1_relu/Relu;densenet201/conv4_block41_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block41_1_conv/Conv2D]:304
	                 CONV_2D	        10429.522	   18.647	   18.956	  0.139%	 76.825%	     0.000	        1	[densenet201/conv4_block41_2_conv/Conv2D1]:305
	           CONCATENATION	        10448.489	    0.318	    0.317	  0.002%	 76.828%	     0.000	        1	[densenet201/conv4_block41_concat/concat]:306
	                     MUL	        10448.813	   21.518	   21.559	  0.159%	 76.986%	     0.000	        1	[densenet201/conv4_block42_0_bn/FusedBatchNormV3]:307
	                     ADD	        10470.382	   27.912	   28.019	  0.206%	 77.192%	     0.000	        1	[densenet201/conv4_block42_0_relu/Relu;densenet201/conv4_block42_0_bn/FusedBatchNormV3]:308
	                 CONV_2D	        10498.416	   86.680	   86.897	  0.639%	 77.831%	     0.000	        1	[densenet201/conv4_block42_1_relu/Relu;densenet201/conv4_block42_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block42_1_conv/Conv2D]:309
	                 CONV_2D	        10585.323	   18.890	   18.979	  0.140%	 77.971%	     0.000	        1	[densenet201/conv4_block42_2_conv/Conv2D1]:310
	           CONCATENATION	        10604.312	    0.303	    0.288	  0.002%	 77.973%	     0.000	        1	[densenet201/conv4_block42_concat/concat]:311
	                     MUL	        10604.607	   21.935	   22.016	  0.162%	 78.135%	     0.000	        1	[densenet201/conv4_block43_0_bn/FusedBatchNormV3]:312
	                     ADD	        10626.634	   28.642	   28.651	  0.211%	 78.346%	     0.000	        1	[densenet201/conv4_block43_0_relu/Relu;densenet201/conv4_block43_0_bn/FusedBatchNormV3]:313
	                 CONV_2D	        10655.297	   88.015	   88.813	  0.653%	 78.999%	     0.000	        1	[densenet201/conv4_block43_1_relu/Relu;densenet201/conv4_block43_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block43_1_conv/Conv2D]:314
	                 CONV_2D	        10744.120	   19.282	   19.257	  0.142%	 79.141%	     0.000	        1	[densenet201/conv4_block43_2_conv/Conv2D1]:315
	           CONCATENATION	        10763.388	    0.332	    0.310	  0.002%	 79.143%	     0.000	        1	[densenet201/conv4_block43_concat/concat]:316
	                     MUL	        10763.706	   22.397	   22.558	  0.166%	 79.309%	     0.000	        1	[densenet201/conv4_block44_0_bn/FusedBatchNormV3]:317
	                     ADD	        10786.274	   29.081	   29.265	  0.215%	 79.524%	     0.000	        1	[densenet201/conv4_block44_0_relu/Relu;densenet201/conv4_block44_0_bn/FusedBatchNormV3]:318
	                 CONV_2D	        10815.550	   89.327	   91.142	  0.670%	 80.194%	     0.000	        1	[densenet201/conv4_block44_1_relu/Relu;densenet201/conv4_block44_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block44_1_conv/Conv2D]:319
	                 CONV_2D	        10906.703	   18.889	   19.358	  0.142%	 80.337%	     0.000	        1	[densenet201/conv4_block44_2_conv/Conv2D1]:320
	           CONCATENATION	        10926.072	    0.293	    0.320	  0.002%	 80.339%	     0.000	        1	[densenet201/conv4_block44_concat/concat]:321
	                     MUL	        10926.401	   22.833	   22.977	  0.169%	 80.508%	     0.000	        1	[densenet201/conv4_block45_0_bn/FusedBatchNormV3]:322
	                     ADD	        10949.389	   29.668	   29.914	  0.220%	 80.728%	     0.000	        1	[densenet201/conv4_block45_0_relu/Relu;densenet201/conv4_block45_0_bn/FusedBatchNormV3]:323
	                 CONV_2D	        10979.314	   91.294	   92.915	  0.683%	 81.411%	     0.000	        1	[densenet201/conv4_block45_1_relu/Relu;densenet201/conv4_block45_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block45_1_conv/Conv2D]:324
	                 CONV_2D	        11072.246	   19.203	   19.242	  0.142%	 81.553%	     0.000	        1	[densenet201/conv4_block45_2_conv/Conv2D1]:325
	           CONCATENATION	        11091.497	    0.341	    0.323	  0.002%	 81.555%	     0.000	        1	[densenet201/conv4_block45_concat/concat]:326
	                     MUL	        11091.828	   23.208	   23.416	  0.172%	 81.728%	     0.000	        1	[densenet201/conv4_block46_0_bn/FusedBatchNormV3]:327
	                     ADD	        11115.255	   30.214	   30.427	  0.224%	 81.951%	     0.000	        1	[densenet201/conv4_block46_0_relu/Relu;densenet201/conv4_block46_0_bn/FusedBatchNormV3]:328
	                 CONV_2D	        11145.692	   93.074	   94.680	  0.696%	 82.648%	     0.000	        1	[densenet201/conv4_block46_1_relu/Relu;densenet201/conv4_block46_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block46_1_conv/Conv2D]:329
	                 CONV_2D	        11240.383	   19.022	   19.097	  0.140%	 82.788%	     0.000	        1	[densenet201/conv4_block46_2_conv/Conv2D1]:330
	           CONCATENATION	        11259.491	    0.323	    0.307	  0.002%	 82.790%	     0.000	        1	[densenet201/conv4_block46_concat/concat]:331
	                     MUL	        11259.807	   23.687	   23.829	  0.175%	 82.966%	     0.000	        1	[densenet201/conv4_block47_0_bn/FusedBatchNormV3]:332
	                     ADD	        11283.646	   30.758	   30.956	  0.228%	 83.193%	     0.000	        1	[densenet201/conv4_block47_0_relu/Relu;densenet201/conv4_block47_0_bn/FusedBatchNormV3]:333
	                 CONV_2D	        11314.613	   94.453	   96.180	  0.707%	 83.901%	     0.000	        1	[densenet201/conv4_block47_1_relu/Relu;densenet201/conv4_block47_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block47_1_conv/Conv2D]:334
	                 CONV_2D	        11410.804	   18.930	   19.376	  0.143%	 84.043%	     0.000	        1	[densenet201/conv4_block47_2_conv/Conv2D1]:335
	           CONCATENATION	        11430.191	    0.336	    0.317	  0.002%	 84.046%	     0.000	        1	[densenet201/conv4_block47_concat/concat]:336
	                     MUL	        11430.515	   24.151	   24.309	  0.179%	 84.224%	     0.000	        1	[densenet201/conv4_block48_0_bn/FusedBatchNormV3]:337
	                     ADD	        11454.835	   31.405	   31.483	  0.232%	 84.456%	     0.000	        1	[densenet201/conv4_block48_0_relu/Relu;densenet201/conv4_block48_0_bn/FusedBatchNormV3]:338
	                 CONV_2D	        11486.329	   96.382	   97.437	  0.717%	 85.173%	     0.000	        1	[densenet201/conv4_block48_1_relu/Relu;densenet201/conv4_block48_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block48_1_conv/Conv2D]:339
	                 CONV_2D	        11583.777	   19.168	   18.942	  0.139%	 85.312%	     0.000	        1	[densenet201/conv4_block48_2_conv/Conv2D1]:340
	           CONCATENATION	        11602.728	    0.299	    0.291	  0.002%	 85.314%	     0.000	        1	[densenet201/conv4_block48_concat/concat]:341
	                     MUL	        11603.028	   24.494	   24.593	  0.181%	 85.495%	     0.000	        1	[densenet201/pool4_bn/FusedBatchNormV3]:342
	                     ADD	        11627.631	   31.956	   32.026	  0.236%	 85.731%	     0.000	        1	[densenet201/pool4_relu/Relu;densenet201/pool4_bn/FusedBatchNormV3]:343
	                 CONV_2D	        11659.667	  674.835	  658.562	  4.844%	 90.574%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	         AVERAGE_POOL_2D	        12318.242	    4.567	    4.535	  0.033%	 90.608%	     0.000	        1	[densenet201/pool4_pool/AvgPool]:345
	                     MUL	        12322.786	    3.130	    3.110	  0.023%	 90.630%	     0.000	        1	[densenet201/conv5_block1_0_bn/FusedBatchNormV31]:346
	                     ADD	        12325.904	    4.057	    4.029	  0.030%	 90.660%	     0.000	        1	[densenet201/conv5_block1_0_relu/Relu;densenet201/conv5_block1_0_bn/FusedBatchNormV3]:347
	                 CONV_2D	        12329.942	   14.153	   13.904	  0.102%	 90.762%	     0.000	        1	[densenet201/conv5_block1_1_relu/Relu;densenet201/conv5_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block1_1_conv/Conv2D]:348
	                 CONV_2D	        12343.855	    5.399	    5.374	  0.040%	 90.802%	     0.000	        1	[densenet201/conv5_block1_2_conv/Conv2D1]:349
	           CONCATENATION	        12349.239	    0.061	    0.060	  0.000%	 90.802%	     0.000	        1	[densenet201/conv5_block1_concat/concat]:350
	                     MUL	        12349.305	    3.250	    3.228	  0.024%	 90.826%	     0.000	        1	[densenet201/conv5_block2_0_bn/FusedBatchNormV31]:351
	                     ADD	        12352.542	    4.213	    4.189	  0.031%	 90.857%	     0.000	        1	[densenet201/conv5_block2_0_relu/Relu;densenet201/conv5_block2_0_bn/FusedBatchNormV3]:352
	                 CONV_2D	        12356.740	   14.380	   14.343	  0.105%	 90.962%	     0.000	        1	[densenet201/conv5_block2_1_relu/Relu;densenet201/conv5_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block2_1_conv/Conv2D]:353
	                 CONV_2D	        12371.092	    5.372	    5.324	  0.039%	 91.001%	     0.000	        1	[densenet201/conv5_block2_2_conv/Conv2D1]:354
	           CONCATENATION	        12376.425	    0.088	    0.056	  0.000%	 91.002%	     0.000	        1	[densenet201/conv5_block2_concat/concat]:355
	                     MUL	        12376.487	    3.361	    3.339	  0.025%	 91.026%	     0.000	        1	[densenet201/conv5_block3_0_bn/FusedBatchNormV31]:356
	                     ADD	        12379.835	    4.356	    4.316	  0.032%	 91.058%	     0.000	        1	[densenet201/conv5_block3_0_relu/Relu;densenet201/conv5_block3_0_bn/FusedBatchNormV3]:357
	                 CONV_2D	        12384.161	   14.805	   14.745	  0.108%	 91.167%	     0.000	        1	[densenet201/conv5_block3_1_relu/Relu;densenet201/conv5_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block3_1_conv/Conv2D]:358
	                 CONV_2D	        12398.916	    5.411	    5.331	  0.039%	 91.206%	     0.000	        1	[densenet201/conv5_block3_2_conv/Conv2D1]:359
	           CONCATENATION	        12404.256	    0.080	    0.060	  0.000%	 91.206%	     0.000	        1	[densenet201/conv5_block3_concat/concat]:360
	                     MUL	        12404.323	    3.453	    3.447	  0.025%	 91.232%	     0.000	        1	[densenet201/conv5_block4_0_bn/FusedBatchNormV31]:361
	                     ADD	        12407.778	    4.484	    4.470	  0.033%	 91.265%	     0.000	        1	[densenet201/conv5_block4_0_relu/Relu;densenet201/conv5_block4_0_bn/FusedBatchNormV3]:362
	                 CONV_2D	        12412.257	   15.570	   15.262	  0.112%	 91.377%	     0.000	        1	[densenet201/conv5_block4_1_relu/Relu;densenet201/conv5_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block4_1_conv/Conv2D]:363
	                 CONV_2D	        12427.529	    5.429	    5.346	  0.039%	 91.416%	     0.000	        1	[densenet201/conv5_block4_2_conv/Conv2D1]:364
	           CONCATENATION	        12432.885	    0.077	    0.057	  0.000%	 91.417%	     0.000	        1	[densenet201/conv5_block4_concat/concat]:365
	                     MUL	        12432.948	    3.595	    3.550	  0.026%	 91.443%	     0.000	        1	[densenet201/conv5_block5_0_bn/FusedBatchNormV31]:366
	                     ADD	        12436.506	    4.651	    4.608	  0.034%	 91.477%	     0.000	        1	[densenet201/conv5_block5_0_relu/Relu;densenet201/conv5_block5_0_bn/FusedBatchNormV3]:367
	                 CONV_2D	        12441.123	   15.789	   15.722	  0.116%	 91.592%	     0.000	        1	[densenet201/conv5_block5_1_relu/Relu;densenet201/conv5_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block5_1_conv/Conv2D]:368
	                 CONV_2D	        12456.855	    5.371	    5.340	  0.039%	 91.631%	     0.000	        1	[densenet201/conv5_block5_2_conv/Conv2D1]:369
	           CONCATENATION	        12462.203	    0.066	    0.056	  0.000%	 91.632%	     0.000	        1	[densenet201/conv5_block5_concat/concat]:370
	                     MUL	        12462.267	    3.686	    3.668	  0.027%	 91.659%	     0.000	        1	[densenet201/conv5_block6_0_bn/FusedBatchNormV31]:371
	                     ADD	        12465.947	    4.794	    4.744	  0.035%	 91.694%	     0.000	        1	[densenet201/conv5_block6_0_relu/Relu;densenet201/conv5_block6_0_bn/FusedBatchNormV3]:372
	                 CONV_2D	        12470.700	   16.476	   16.402	  0.121%	 91.814%	     0.000	        1	[densenet201/conv5_block6_1_relu/Relu;densenet201/conv5_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block6_1_conv/Conv2D]:373
	                 CONV_2D	        12487.111	    5.568	    5.329	  0.039%	 91.854%	     0.000	        1	[densenet201/conv5_block6_2_conv/Conv2D1]:374
	           CONCATENATION	        12492.449	    0.074	    0.056	  0.000%	 91.854%	     0.000	        1	[densenet201/conv5_block6_concat/concat]:375
	                     MUL	        12492.512	    3.807	    3.775	  0.028%	 91.882%	     0.000	        1	[densenet201/conv5_block7_0_bn/FusedBatchNormV31]:376
	                     ADD	        12496.296	    4.914	    4.891	  0.036%	 91.918%	     0.000	        1	[densenet201/conv5_block7_0_relu/Relu;densenet201/conv5_block7_0_bn/FusedBatchNormV3]:377
	                 CONV_2D	        12501.196	   17.071	   16.884	  0.124%	 92.042%	     0.000	        1	[densenet201/conv5_block7_1_relu/Relu;densenet201/conv5_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block7_1_conv/Conv2D]:378
	                 CONV_2D	        12518.091	    5.514	    5.358	  0.039%	 92.081%	     0.000	        1	[densenet201/conv5_block7_2_conv/Conv2D1]:379
	           CONCATENATION	        12523.459	    0.069	    0.061	  0.000%	 92.082%	     0.000	        1	[densenet201/conv5_block7_concat/concat]:380
	                     MUL	        12523.526	    3.915	    3.877	  0.029%	 92.110%	     0.000	        1	[densenet201/conv5_block8_0_bn/FusedBatchNormV31]:381
	                     ADD	        12527.414	    5.050	    5.036	  0.037%	 92.147%	     0.000	        1	[densenet201/conv5_block8_0_relu/Relu;densenet201/conv5_block8_0_bn/FusedBatchNormV3]:382
	                 CONV_2D	        12532.461	   17.509	   17.416	  0.128%	 92.275%	     0.000	        1	[densenet201/conv5_block8_1_relu/Relu;densenet201/conv5_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block8_1_conv/Conv2D]:383
	                 CONV_2D	        12549.886	    5.352	    5.346	  0.039%	 92.315%	     0.000	        1	[densenet201/conv5_block8_2_conv/Conv2D1]:384
	           CONCATENATION	        12555.242	    0.095	    0.067	  0.000%	 92.315%	     0.000	        1	[densenet201/conv5_block8_concat/concat]:385
	                     MUL	        12555.316	    4.020	    3.997	  0.029%	 92.345%	     0.000	        1	[densenet201/conv5_block9_0_bn/FusedBatchNormV31]:386
	                     ADD	        12559.321	    5.209	    5.178	  0.038%	 92.383%	     0.000	        1	[densenet201/conv5_block9_0_relu/Relu;densenet201/conv5_block9_0_bn/FusedBatchNormV3]:387
	                 CONV_2D	        12564.507	   18.108	   17.812	  0.131%	 92.514%	     0.000	        1	[densenet201/conv5_block9_1_relu/Relu;densenet201/conv5_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block9_1_conv/Conv2D]:388
	                 CONV_2D	        12582.328	    5.369	    5.346	  0.039%	 92.553%	     0.000	        1	[densenet201/conv5_block9_2_conv/Conv2D1]:389
	           CONCATENATION	        12587.683	    0.067	    0.056	  0.000%	 92.553%	     0.000	        1	[densenet201/conv5_block9_concat/concat]:390
	                     MUL	        12587.746	    4.143	    4.098	  0.030%	 92.584%	     0.000	        1	[densenet201/conv5_block10_0_bn/FusedBatchNormV31]:391
	                     ADD	        12591.852	    5.374	    5.318	  0.039%	 92.623%	     0.000	        1	[densenet201/conv5_block10_0_relu/Relu;densenet201/conv5_block10_0_bn/FusedBatchNormV3]:392
	                 CONV_2D	        12597.179	   18.484	   18.310	  0.135%	 92.757%	     0.000	        1	[densenet201/conv5_block10_1_relu/Relu;densenet201/conv5_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block10_1_conv/Conv2D]:393
	                 CONV_2D	        12615.498	    5.419	    5.323	  0.039%	 92.796%	     0.000	        1	[densenet201/conv5_block10_2_conv/Conv2D1]:394
	           CONCATENATION	        12620.830	    0.068	    0.055	  0.000%	 92.797%	     0.000	        1	[densenet201/conv5_block10_concat/concat]:395
	                     MUL	        12620.892	    4.229	    4.210	  0.031%	 92.828%	     0.000	        1	[densenet201/conv5_block11_0_bn/FusedBatchNormV31]:396
	                     ADD	        12625.110	    5.525	    5.464	  0.040%	 92.868%	     0.000	        1	[densenet201/conv5_block11_0_relu/Relu;densenet201/conv5_block11_0_bn/FusedBatchNormV3]:397
	                 CONV_2D	        12630.583	   19.024	   18.771	  0.138%	 93.006%	     0.000	        1	[densenet201/conv5_block11_1_relu/Relu;densenet201/conv5_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block11_1_conv/Conv2D]:398
	                 CONV_2D	        12649.363	    5.380	    5.320	  0.039%	 93.045%	     0.000	        1	[densenet201/conv5_block11_2_conv/Conv2D1]:399
	           CONCATENATION	        12654.692	    0.077	    0.062	  0.000%	 93.046%	     0.000	        1	[densenet201/conv5_block11_concat/concat]:400
	                     MUL	        12654.763	    4.339	    4.313	  0.032%	 93.077%	     0.000	        1	[densenet201/conv5_block12_0_bn/FusedBatchNormV31]:401
	                     ADD	        12659.084	    5.669	    5.607	  0.041%	 93.119%	     0.000	        1	[densenet201/conv5_block12_0_relu/Relu;densenet201/conv5_block12_0_bn/FusedBatchNormV3]:402
	                 CONV_2D	        12664.699	   19.480	   19.321	  0.142%	 93.261%	     0.000	        1	[densenet201/conv5_block12_1_relu/Relu;densenet201/conv5_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block12_1_conv/Conv2D]:403
	                 CONV_2D	        12684.029	    5.526	    5.370	  0.039%	 93.300%	     0.000	        1	[densenet201/conv5_block12_2_conv/Conv2D1]:404
	           CONCATENATION	        12689.408	    0.079	    0.059	  0.000%	 93.301%	     0.000	        1	[densenet201/conv5_block12_concat/concat]:405
	                     MUL	        12689.473	    4.450	    4.433	  0.033%	 93.333%	     0.000	        1	[densenet201/conv5_block13_0_bn/FusedBatchNormV31]:406
	                     ADD	        12693.915	    5.789	    5.748	  0.042%	 93.376%	     0.000	        1	[densenet201/conv5_block13_0_relu/Relu;densenet201/conv5_block13_0_bn/FusedBatchNormV3]:407
	                 CONV_2D	        12699.671	   19.869	   19.668	  0.145%	 93.520%	     0.000	        1	[densenet201/conv5_block13_1_relu/Relu;densenet201/conv5_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block13_1_conv/Conv2D]:408
	                 CONV_2D	        12719.349	    5.447	    5.318	  0.039%	 93.559%	     0.000	        1	[densenet201/conv5_block13_2_conv/Conv2D1]:409
	           CONCATENATION	        12724.676	    0.074	    0.062	  0.000%	 93.560%	     0.000	        1	[densenet201/conv5_block13_concat/concat]:410
	                     MUL	        12724.745	    4.567	    4.531	  0.033%	 93.593%	     0.000	        1	[densenet201/conv5_block14_0_bn/FusedBatchNormV31]:411
	                     ADD	        12729.284	    5.945	    5.895	  0.043%	 93.636%	     0.000	        1	[densenet201/conv5_block14_0_relu/Relu;densenet201/conv5_block14_0_bn/FusedBatchNormV3]:412
	                 CONV_2D	        12735.188	   20.340	   20.187	  0.148%	 93.785%	     0.000	        1	[densenet201/conv5_block14_1_relu/Relu;densenet201/conv5_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block14_1_conv/Conv2D]:413
	                 CONV_2D	        12755.385	    5.377	    5.321	  0.039%	 93.824%	     0.000	        1	[densenet201/conv5_block14_2_conv/Conv2D1]:414
	           CONCATENATION	        12760.715	    0.076	    0.063	  0.000%	 93.825%	     0.000	        1	[densenet201/conv5_block14_concat/concat]:415
	                     MUL	        12760.784	    4.676	    4.641	  0.034%	 93.859%	     0.000	        1	[densenet201/conv5_block15_0_bn/FusedBatchNormV31]:416
	                     ADD	        12765.434	    6.077	    6.030	  0.044%	 93.903%	     0.000	        1	[densenet201/conv5_block15_0_relu/Relu;densenet201/conv5_block15_0_bn/FusedBatchNormV3]:417
	                 CONV_2D	        12771.473	   20.985	   20.699	  0.152%	 94.055%	     0.000	        1	[densenet201/conv5_block15_1_relu/Relu;densenet201/conv5_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block15_1_conv/Conv2D]:418
	                 CONV_2D	        12792.181	    5.330	    5.299	  0.039%	 94.094%	     0.000	        1	[densenet201/conv5_block15_2_conv/Conv2D1]:419
	           CONCATENATION	        12797.488	    0.075	    0.060	  0.000%	 94.095%	     0.000	        1	[densenet201/conv5_block15_concat/concat]:420
	                     MUL	        12797.555	    4.773	    4.736	  0.035%	 94.129%	     0.000	        1	[densenet201/conv5_block16_0_bn/FusedBatchNormV31]:421
	                     ADD	        12802.299	    6.278	    6.167	  0.045%	 94.175%	     0.000	        1	[densenet201/conv5_block16_0_relu/Relu;densenet201/conv5_block16_0_bn/FusedBatchNormV3]:422
	                 CONV_2D	        12808.474	   21.438	   21.161	  0.156%	 94.330%	     0.000	        1	[densenet201/conv5_block16_1_relu/Relu;densenet201/conv5_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block16_1_conv/Conv2D]:423
	                 CONV_2D	        12829.645	    5.366	    5.332	  0.039%	 94.370%	     0.000	        1	[densenet201/conv5_block16_2_conv/Conv2D1]:424
	           CONCATENATION	        12834.985	    0.093	    0.057	  0.000%	 94.370%	     0.000	        1	[densenet201/conv5_block16_concat/concat]:425
	                     MUL	        12835.048	    4.880	    4.846	  0.036%	 94.406%	     0.000	        1	[densenet201/conv5_block17_0_bn/FusedBatchNormV31]:426
	                     ADD	        12839.903	    6.402	    6.304	  0.046%	 94.452%	     0.000	        1	[densenet201/conv5_block17_0_relu/Relu;densenet201/conv5_block17_0_bn/FusedBatchNormV3]:427
	                 CONV_2D	        12846.215	   21.843	   21.574	  0.159%	 94.611%	     0.000	        1	[densenet201/conv5_block17_1_relu/Relu;densenet201/conv5_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block17_1_conv/Conv2D]:428
	                 CONV_2D	        12867.799	    5.361	    5.290	  0.039%	 94.650%	     0.000	        1	[densenet201/conv5_block17_2_conv/Conv2D1]:429
	           CONCATENATION	        12873.097	    0.072	    0.064	  0.000%	 94.650%	     0.000	        1	[densenet201/conv5_block17_concat/concat]:430
	                     MUL	        12873.167	    4.961	    4.951	  0.036%	 94.687%	     0.000	        1	[densenet201/conv5_block18_0_bn/FusedBatchNormV31]:431
	                     ADD	        12878.127	    6.464	    6.442	  0.047%	 94.734%	     0.000	        1	[densenet201/conv5_block18_0_relu/Relu;densenet201/conv5_block18_0_bn/FusedBatchNormV3]:432
	                 CONV_2D	        12884.577	   22.031	   22.087	  0.162%	 94.896%	     0.000	        1	[densenet201/conv5_block18_1_relu/Relu;densenet201/conv5_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block18_1_conv/Conv2D]:433
	                 CONV_2D	        12906.673	    5.226	    5.300	  0.039%	 94.935%	     0.000	        1	[densenet201/conv5_block18_2_conv/Conv2D1]:434
	           CONCATENATION	        12911.982	    0.050	    0.056	  0.000%	 94.936%	     0.000	        1	[densenet201/conv5_block18_concat/concat]:435
	                     MUL	        12912.044	    5.038	    5.064	  0.037%	 94.973%	     0.000	        1	[densenet201/conv5_block19_0_bn/FusedBatchNormV31]:436
	                     ADD	        12917.116	    6.549	    6.566	  0.048%	 95.021%	     0.000	        1	[densenet201/conv5_block19_0_relu/Relu;densenet201/conv5_block19_0_bn/FusedBatchNormV3]:437
	                 CONV_2D	        12923.690	   22.453	   22.490	  0.165%	 95.187%	     0.000	        1	[densenet201/conv5_block19_1_relu/Relu;densenet201/conv5_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block19_1_conv/Conv2D]:438
	                 CONV_2D	        12946.190	    5.247	    5.329	  0.039%	 95.226%	     0.000	        1	[densenet201/conv5_block19_2_conv/Conv2D1]:439
	           CONCATENATION	        12951.528	    0.102	    0.073	  0.001%	 95.227%	     0.000	        1	[densenet201/conv5_block19_concat/concat]:440
	                     MUL	        12951.609	    5.171	    5.197	  0.038%	 95.265%	     0.000	        1	[densenet201/conv5_block20_0_bn/FusedBatchNormV31]:441
	                     ADD	        12956.814	    6.678	    6.723	  0.049%	 95.314%	     0.000	        1	[densenet201/conv5_block20_0_relu/Relu;densenet201/conv5_block20_0_bn/FusedBatchNormV3]:442
	                 CONV_2D	        12963.545	   22.922	   23.048	  0.170%	 95.484%	     0.000	        1	[densenet201/conv5_block20_1_relu/Relu;densenet201/conv5_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block20_1_conv/Conv2D]:443
	                 CONV_2D	        12986.602	    5.241	    5.306	  0.039%	 95.523%	     0.000	        1	[densenet201/conv5_block20_2_conv/Conv2D1]:444
	           CONCATENATION	        12991.916	    0.042	    0.060	  0.000%	 95.523%	     0.000	        1	[densenet201/conv5_block20_concat/concat]:445
	                     MUL	        12991.983	    5.251	    5.272	  0.039%	 95.562%	     0.000	        1	[densenet201/conv5_block21_0_bn/FusedBatchNormV31]:446
	                     ADD	        12997.263	    6.842	    6.864	  0.050%	 95.612%	     0.000	        1	[densenet201/conv5_block21_0_relu/Relu;densenet201/conv5_block21_0_bn/FusedBatchNormV3]:447
	                 CONV_2D	        13004.135	   23.278	   23.414	  0.172%	 95.785%	     0.000	        1	[densenet201/conv5_block21_1_relu/Relu;densenet201/conv5_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block21_1_conv/Conv2D]:448
	                 CONV_2D	        13027.560	    5.269	    5.281	  0.039%	 95.823%	     0.000	        1	[densenet201/conv5_block21_2_conv/Conv2D1]:449
	           CONCATENATION	        13032.849	    0.052	    0.060	  0.000%	 95.824%	     0.000	        1	[densenet201/conv5_block21_concat/concat]:450
	                     MUL	        13032.916	    5.382	    5.387	  0.040%	 95.864%	     0.000	        1	[densenet201/conv5_block22_0_bn/FusedBatchNormV31]:451
	                     ADD	        13038.310	    6.972	    6.983	  0.051%	 95.915%	     0.000	        1	[densenet201/conv5_block22_0_relu/Relu;densenet201/conv5_block22_0_bn/FusedBatchNormV3]:452
	                 CONV_2D	        13045.301	   24.012	   24.118	  0.177%	 96.092%	     0.000	        1	[densenet201/conv5_block22_1_relu/Relu;densenet201/conv5_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block22_1_conv/Conv2D]:453
	                 CONV_2D	        13069.429	    5.255	    5.292	  0.039%	 96.131%	     0.000	        1	[densenet201/conv5_block22_2_conv/Conv2D1]:454
	           CONCATENATION	        13074.731	    0.040	    0.061	  0.000%	 96.132%	     0.000	        1	[densenet201/conv5_block22_concat/concat]:455
	                     MUL	        13074.800	    5.468	    5.504	  0.040%	 96.172%	     0.000	        1	[densenet201/conv5_block23_0_bn/FusedBatchNormV31]:456
	                     ADD	        13080.312	    7.098	    7.125	  0.052%	 96.225%	     0.000	        1	[densenet201/conv5_block23_0_relu/Relu;densenet201/conv5_block23_0_bn/FusedBatchNormV3]:457
	                 CONV_2D	        13087.445	   24.460	   24.618	  0.181%	 96.406%	     0.000	        1	[densenet201/conv5_block23_1_relu/Relu;densenet201/conv5_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block23_1_conv/Conv2D]:458
	                 CONV_2D	        13112.073	    5.256	    5.295	  0.039%	 96.445%	     0.000	        1	[densenet201/conv5_block23_2_conv/Conv2D1]:459
	           CONCATENATION	        13117.377	    0.092	    0.070	  0.001%	 96.445%	     0.000	        1	[densenet201/conv5_block23_concat/concat]:460
	                     MUL	        13117.454	    5.604	    5.612	  0.041%	 96.486%	     0.000	        1	[densenet201/conv5_block24_0_bn/FusedBatchNormV31]:461
	                     ADD	        13123.075	    7.250	    7.283	  0.054%	 96.540%	     0.000	        1	[densenet201/conv5_block24_0_relu/Relu;densenet201/conv5_block24_0_bn/FusedBatchNormV3]:462
	                 CONV_2D	        13130.366	   25.264	   25.190	  0.185%	 96.725%	     0.000	        1	[densenet201/conv5_block24_1_relu/Relu;densenet201/conv5_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block24_1_conv/Conv2D]:463
	                 CONV_2D	        13155.565	    5.257	    5.310	  0.039%	 96.764%	     0.000	        1	[densenet201/conv5_block24_2_conv/Conv2D1]:464
	           CONCATENATION	        13160.883	    0.052	    0.066	  0.000%	 96.765%	     0.000	        1	[densenet201/conv5_block24_concat/concat]:465
	                     MUL	        13160.956	    5.698	    5.725	  0.042%	 96.807%	     0.000	        1	[densenet201/conv5_block25_0_bn/FusedBatchNormV31]:466
	                     ADD	        13166.689	    7.414	    7.428	  0.055%	 96.861%	     0.000	        1	[densenet201/conv5_block25_0_relu/Relu;densenet201/conv5_block25_0_bn/FusedBatchNormV3]:467
	                 CONV_2D	        13174.124	   25.474	   25.618	  0.188%	 97.050%	     0.000	        1	[densenet201/conv5_block25_1_relu/Relu;densenet201/conv5_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block25_1_conv/Conv2D]:468
	                 CONV_2D	        13199.752	    5.240	    5.308	  0.039%	 97.089%	     0.000	        1	[densenet201/conv5_block25_2_conv/Conv2D1]:469
	           CONCATENATION	        13205.069	    0.046	    0.070	  0.001%	 97.089%	     0.000	        1	[densenet201/conv5_block25_concat/concat]:470
	                     MUL	        13205.146	    5.798	    5.848	  0.043%	 97.132%	     0.000	        1	[densenet201/conv5_block26_0_bn/FusedBatchNormV31]:471
	                     ADD	        13211.002	    7.533	    7.593	  0.056%	 97.188%	     0.000	        1	[densenet201/conv5_block26_0_relu/Relu;densenet201/conv5_block26_0_bn/FusedBatchNormV3]:472
	                 CONV_2D	        13218.603	   26.012	   26.195	  0.193%	 97.381%	     0.000	        1	[densenet201/conv5_block26_1_relu/Relu;densenet201/conv5_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block26_1_conv/Conv2D]:473
	                 CONV_2D	        13244.812	    5.215	    5.307	  0.039%	 97.420%	     0.000	        1	[densenet201/conv5_block26_2_conv/Conv2D1]:474
	           CONCATENATION	        13250.128	    0.049	    0.069	  0.001%	 97.420%	     0.000	        1	[densenet201/conv5_block26_concat/concat]:475
	                     MUL	        13250.203	    5.953	    5.960	  0.044%	 97.464%	     0.000	        1	[densenet201/conv5_block27_0_bn/FusedBatchNormV31]:476
	                     ADD	        13256.174	    7.671	    7.723	  0.057%	 97.521%	     0.000	        1	[densenet201/conv5_block27_0_relu/Relu;densenet201/conv5_block27_0_bn/FusedBatchNormV3]:477
	                 CONV_2D	        13263.906	   26.401	   26.652	  0.196%	 97.717%	     0.000	        1	[densenet201/conv5_block27_1_relu/Relu;densenet201/conv5_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block27_1_conv/Conv2D]:478
	                 CONV_2D	        13290.568	    5.230	    5.303	  0.039%	 97.756%	     0.000	        1	[densenet201/conv5_block27_2_conv/Conv2D1]:479
	           CONCATENATION	        13295.879	    0.087	    0.079	  0.001%	 97.757%	     0.000	        1	[densenet201/conv5_block27_concat/concat]:480
	                     MUL	        13295.965	    6.025	    6.079	  0.045%	 97.801%	     0.000	        1	[densenet201/conv5_block28_0_bn/FusedBatchNormV31]:481
	                     ADD	        13302.051	    7.819	    7.875	  0.058%	 97.859%	     0.000	        1	[densenet201/conv5_block28_0_relu/Relu;densenet201/conv5_block28_0_bn/FusedBatchNormV3]:482
	                 CONV_2D	        13309.935	   26.899	   27.274	  0.201%	 98.060%	     0.000	        1	[densenet201/conv5_block28_1_relu/Relu;densenet201/conv5_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block28_1_conv/Conv2D]:483
	                 CONV_2D	        13337.220	    5.271	    5.333	  0.039%	 98.099%	     0.000	        1	[densenet201/conv5_block28_2_conv/Conv2D1]:484
	           CONCATENATION	        13342.564	    0.054	    0.080	  0.001%	 98.100%	     0.000	        1	[densenet201/conv5_block28_concat/concat]:485
	                     MUL	        13342.651	    6.152	    6.186	  0.045%	 98.145%	     0.000	        1	[densenet201/conv5_block29_0_bn/FusedBatchNormV31]:486
	                     ADD	        13348.845	    7.962	    8.039	  0.059%	 98.204%	     0.000	        1	[densenet201/conv5_block29_0_relu/Relu;densenet201/conv5_block29_0_bn/FusedBatchNormV3]:487
	                 CONV_2D	        13356.893	   27.271	   28.110	  0.207%	 98.411%	     0.000	        1	[densenet201/conv5_block29_1_relu/Relu;densenet201/conv5_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block29_1_conv/Conv2D]:488
	                 CONV_2D	        13385.014	    5.282	    5.532	  0.041%	 98.452%	     0.000	        1	[densenet201/conv5_block29_2_conv/Conv2D1]:489
	           CONCATENATION	        13390.555	    0.062	    0.086	  0.001%	 98.452%	     0.000	        1	[densenet201/conv5_block29_concat/concat]:490
	                     MUL	        13390.650	    6.252	    6.449	  0.047%	 98.500%	     0.000	        1	[densenet201/conv5_block30_0_bn/FusedBatchNormV31]:491
	                     ADD	        13397.113	    8.097	    8.356	  0.061%	 98.561%	     0.000	        1	[densenet201/conv5_block30_0_relu/Relu;densenet201/conv5_block30_0_bn/FusedBatchNormV3]:492
	                 CONV_2D	        13405.478	   27.819	   28.354	  0.209%	 98.770%	     0.000	        1	[densenet201/conv5_block30_1_relu/Relu;densenet201/conv5_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block30_1_conv/Conv2D]:493
	                 CONV_2D	        13433.842	    5.312	    5.337	  0.039%	 98.809%	     0.000	        1	[densenet201/conv5_block30_2_conv/Conv2D1]:494
	           CONCATENATION	        13439.189	    0.063	    0.080	  0.001%	 98.810%	     0.000	        1	[densenet201/conv5_block30_concat/concat]:495
	                     MUL	        13439.275	    6.352	    6.402	  0.047%	 98.857%	     0.000	        1	[densenet201/conv5_block31_0_bn/FusedBatchNormV31]:496
	                     ADD	        13445.685	    8.309	    8.332	  0.061%	 98.918%	     0.000	        1	[densenet201/conv5_block31_0_relu/Relu;densenet201/conv5_block31_0_bn/FusedBatchNormV3]:497
	                 CONV_2D	        13454.026	   28.309	   28.672	  0.211%	 99.129%	     0.000	        1	[densenet201/conv5_block31_1_relu/Relu;densenet201/conv5_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block31_1_conv/Conv2D]:498
	                 CONV_2D	        13482.708	    5.286	    5.328	  0.039%	 99.168%	     0.000	        1	[densenet201/conv5_block31_2_conv/Conv2D1]:499
	           CONCATENATION	        13488.044	    0.056	    0.085	  0.001%	 99.169%	     0.000	        1	[densenet201/conv5_block31_concat/concat]:500
	                     MUL	        13488.136	    6.456	    6.531	  0.048%	 99.217%	     0.000	        1	[densenet201/conv5_block32_0_bn/FusedBatchNormV31]:501
	                     ADD	        13494.675	    8.385	    8.478	  0.062%	 99.279%	     0.000	        1	[densenet201/conv5_block32_0_relu/Relu;densenet201/conv5_block32_0_bn/FusedBatchNormV3]:502
	                 CONV_2D	        13503.163	   28.826	   29.222	  0.215%	 99.494%	     0.000	        1	[densenet201/conv5_block32_1_relu/Relu;densenet201/conv5_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block32_1_conv/Conv2D]:503
	                 CONV_2D	        13532.396	    5.280	    5.384	  0.040%	 99.534%	     0.000	        1	[densenet201/conv5_block32_2_conv/Conv2D210]:504
	           CONCATENATION	        13537.789	    0.049	    0.073	  0.001%	 99.534%	     0.000	        1	[densenet201/conv5_block32_concat/concat]:505
	                     MUL	        13537.869	    6.580	    6.685	  0.049%	 99.583%	     0.000	        1	[densenet201/bn/FusedBatchNormV31]:506
	                     ADD	        13544.563	    8.506	    8.609	  0.063%	 99.647%	     0.000	        1	[densenet201/relu/Relu;densenet201/bn/FusedBatchNormV3]:507
	                    MEAN	        13553.182	   16.046	   16.227	  0.119%	 99.766%	     0.000	        1	[densenet201/avg_pool/Mean]:508
	         FULLY_CONNECTED	        13569.418	   31.464	   31.715	  0.233%	 99.999%	     0.000	        1	[densenet201/predictions/MatMul;densenet201/predictions/BiasAdd]:509
	                 SOFTMAX	        13601.146	    0.085	    0.090	  0.001%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:510

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	        11659.667	  674.835	  658.562	  4.844%	  4.844%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	                 CONV_2D	            3.753	  331.643	  331.413	  2.438%	  7.281%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                 CONV_2D	          927.170	  317.855	  320.828	  2.360%	  9.641%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	                 CONV_2D	         1991.570	  329.014	  319.671	  2.351%	 11.992%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	                 CONV_2D	         2590.791	  319.043	  318.670	  2.344%	 14.336%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	                 CONV_2D	         3230.943	  316.730	  318.556	  2.343%	 16.679%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	                 CONV_2D	         1439.751	  315.754	  317.794	  2.337%	 19.016%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	                 CONV_2D	          462.683	  315.934	  316.844	  2.330%	 21.346%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	                 CONV_2D	         3681.578	  233.570	  235.193	  1.730%	 23.076%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	                 CONV_2D	         6228.673	  213.866	  217.967	  1.603%	 24.679%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100

Number of nodes executed: 511
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	      200	 10417.078	    76.618%	    76.618%	     0.000	      200
	                     ADD	      102	  1730.077	    12.725%	    89.343%	     0.000	      102
	                     MUL	      102	  1335.160	     9.820%	    99.163%	     0.000	      102
	         FULLY_CONNECTED	        1	    31.715	     0.233%	    99.396%	     0.000	        1
	                     PAD	        2	    21.924	     0.161%	    99.557%	     0.000	        2
	         AVERAGE_POOL_2D	        3	    20.041	     0.147%	    99.705%	     0.000	        3
	           CONCATENATION	       98	    18.600	     0.137%	    99.842%	     0.000	       98
	                    MEAN	        1	    16.226	     0.119%	    99.961%	     0.000	        1
	             MAX_POOL_2D	        1	     5.219	     0.038%	    99.999%	     0.000	        1
	                 SOFTMAX	        1	     0.090	     0.001%	   100.000%	     0.000	        1

Timings (microseconds): count=12 first=13615891 curr=13602663 min=13575416 max=13618428 avg=1.35964e+07 std=13435
Memory (bytes): count=0
511 nodes observed



munmap_chunk(): invalid pointer
[ perf record: Woken up 921 times to write data ]
[ perf record: Captured and wrote 230.608 MB /tmp/data.record (1197061 samples) ]

302.339

