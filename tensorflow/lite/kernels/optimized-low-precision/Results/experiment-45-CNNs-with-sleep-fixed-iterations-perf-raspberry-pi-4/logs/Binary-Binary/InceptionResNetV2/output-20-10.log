STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/InceptionResNetV2.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/InceptionResNetV2.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (32, 27, ), Input shape (89401, 3, ), and Output shape (22201, 32, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 16)
	Allocating LowPrecision Activations Tensors with Shape of (22204, 16)
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (22201, 32, ), and Output shape (21609, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
, and the ID is 1
	Allocating LowPrecision Activations Tensors with Shape of (21612, 48)
Applying Conv Low-Precision for Kernel shape (64, 288, ), Input shape (21609, 32, ), and Output shape (21609, 64, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 48)
	Allocating LowPrecision Activations Tensors with Shape of (21612, 48)
2
Applying Conv Low-Precision for Kernel shape (80, 64, ), Input shape (5329, 64, ), and Output shape (5329, 80, ), and the ID is 3
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (80, 16)
	Allocating LowPrecision Activations Tensors with Shape of (5332, 16)
Applying Conv Low-Precision for Kernel shape (192, 720, ), Input shape (5329, 80, ), and Output shape (5041, 192, ), and the ID is 4	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 96)

	Allocating LowPrecision Activations Tensors with Shape of (5044, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 192, ), Input shape (1225, 192, ), and Output shape (1225, 64, ), and the ID is 5
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 32)
Applying Conv Low-Precision for Kernel shape (48, 192, ), Input shape (1225, 192, ), and Output shape (1225, 48, ), and the ID is 6
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 32)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 32)
Applying Conv Low-Precision for Kernel shape (64, 1200, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 160)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 160)
Applying Conv Low-Precision for Kernel shape (64, 192, ), Input shape (1225, 192, ), and Output shape (1225, 64, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 32)
Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (96, 80)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 80)
Applying Conv Low-Precision for Kernel shape (96, 864, ), Input shape (1225, 96, ), and Output shape (1225, 96, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (96, 112)
, and the ID is 10
	Allocating LowPrecision Activations Tensors with Shape of (1228, 112)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (96, 32)
Applying Conv Low-Precision for Kernel shape (96, 192, ), Input shape (1225, 192, ), and Output shape (1225, 96, ), and the ID is 11
	Allocating LowPrecision Activations Tensors with Shape of (1228, 32)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
, and the ID is 13
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 14
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 15
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
, and the ID is 16
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
(1225, 48, ), and Output shape (1225, 64, ), and the ID is 17
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 19
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
(1225, 32, ), and the ID is 20
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 21
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 22
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
, and the ID is 23
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 24
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 25
Applying Conv Low-Precision for Kernel shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
(32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 26
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 27
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 28
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 29
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
31
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
	Changing Input Shape
(1225, 320, ), and the ID is 32
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 33
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
(1225, 32, ), and the ID is 34
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 36
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 37
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 39
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 40
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 41
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 42	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)

	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 43
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 44
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
, and the ID is 45
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 46
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 47
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 48
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 49	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)

	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 50
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 51
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 52
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 53	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)

Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 54
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 55	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)

	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 56
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 57
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 58
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 59
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 60
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 61
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
62
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 63
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 64
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 65
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 66	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)

	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 67	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 68
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 69
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 70
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
71
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 72
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 73
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 74
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 75
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 76
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 288, ), Input shape (1225, 32, ), and Output shape (1225, 32, ), and the ID is 77
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
Applying Conv Low-Precision for Kernel shape (32, 320, ), Input shape (1225, 320, ), and Output shape (1225, 32, ), and the ID is 78
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (48, 48)
Applying Conv Low-Precision for Kernel shape (48, 288, ), Input shape (1225, 32, ), and Output shape (1225, 48, ), and the ID is 79
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 432, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 80
	Allocating LowPrecision Weight Tensors with Shape of (64, 64)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 64)
Applying Conv Low-Precision for Kernel shape (320, 128, ), Input shape (1225, 128, ), and Output shape (1225, 320, ), and the ID is 81
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 16)
Applying Conv Low-Precision for Kernel shape (384, 2880, ), Input shape (1225, 320, ), and Output shape (289, 384, ), and the ID is 82	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (384, 368)

	Allocating LowPrecision Activations Tensors with Shape of (292, 368)
Applying Conv Low-Precision for Kernel shape (256, 320, ), Input shape (1225, 320, ), and Output shape (1225, 256, ), and the ID is 83
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 48)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (1225, 256, ), and Output shape (1225, 256, ), and the ID is 84
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (1228, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (384, 288)
Applying Conv Low-Precision for Kernel shape (384, 2304, ), Input shape (1225, 256, ), and Output shape (289, 384, ), and the ID is 85
	Allocating LowPrecision Activations Tensors with Shape of (292, 288)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
, and the ID is 86
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 87
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 88
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 89
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 90
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 91
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 92
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 93
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 95
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 96
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 97
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 98
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 99
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
100
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 101
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 102
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 103
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 104
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 105
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 106
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 107
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 108
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
, and the ID is 109
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and Output shape (289, 1088, ), and the ID is 110
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 111
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 112
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 113
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
, and the ID is 114
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
115
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 117
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 118
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 119
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
120
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 121
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
122
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
123
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 124
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(289, 1088, ), and the ID is 125
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 126	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 127
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 128
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 129
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 130
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 131
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 132
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 133
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 134	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 135
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 136
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 137
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 138	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 140
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 141
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 142
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
143
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 144
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
145
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 146
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 147
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 148
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and Output shape (289, 1088, ), and the ID is 150
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 151
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 152
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 153
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 154
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(289, 1088, ), and the ID is 155
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 156	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 157
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 158
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 159
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(289, 1088, ), and the ID is 160
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 161	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 162
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 163
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 164
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 165
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 166
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 167
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 168
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 169
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 170
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 171	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 172
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 173	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
(289, 192, ), and the ID is 174
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape (289, 1088, ), and the ID is 175
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 176	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 177
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 178
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 179
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and Output shape (289, 1088, ), and the ID is 180
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (192, 1088, ), Input shape (289, 1088, ), and Output shape (289, 192, ), and the ID is 181	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (289, 1088, ), and Output shape (289, 128, ), and the ID is 182
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (160, 896, ), Input shape (289, 128, ), and Output shape (289, 160, ), and the ID is 183
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (160, 112)
	Allocating LowPrecision Activations Tensors with Shape of (292, 112)
Applying Conv Low-Precision for Kernel shape (192, 1120, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 184	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (1088, 384, ), Input shape (289, 384, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(289, 1088, ), and the ID is 185
	Allocating LowPrecision Weight Tensors with Shape of (1088, 48)
	Allocating LowPrecision Activations Tensors with Shape of (292, 48)
Applying Conv Low-Precision for Kernel shape (256, 1088, ), Input shape (289, 1088, ), and Output shape (289, 256, ), and the ID is 186	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 144)

	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (384, 2304, ), Input shape (289, 256, ), and Output shape (64, 384, ), and the ID is 187
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (384, 288)
	Allocating LowPrecision Activations Tensors with Shape of (64, 288)
Applying Conv Low-Precision for Kernel shape (256, 1088, ), Input shape (289, 1088, ), and Output shape (289, 256, ), and the ID is 188
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (288, 2304, ), Input shape (289, 256, ), and Output shape (64, 288, ), and the ID is 189
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (288, 288)
	Allocating LowPrecision Activations Tensors with Shape of (64, 288)
Applying Conv Low-Precision for Kernel shape (256, 1088, ), Input shape (289, 1088, ), and Output shape (289, 256, ), and the ID is 190
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 144)
	Allocating LowPrecision Activations Tensors with Shape of (292, 144)
Applying Conv Low-Precision for Kernel shape (288, 2304, ), Input shape (289, 256, ), and Output shape (289, 288, ), and the ID is 191
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (288, 288)
	Allocating LowPrecision Activations Tensors with Shape of (292, 288)
Applying Conv Low-Precision for Kernel shape (320, 2592, ), Input shape (289, 288, ), and Output shape (64, 320, ), and the ID is 192
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (320, 336)
	Allocating LowPrecision Activations Tensors with Shape of (64, 336)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 193
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 194
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 195
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 196
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 197
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 198
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 199
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 200
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 201
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(64, 448, ), and Output shape (64, 2080, ), and the ID is 202
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 203
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 204
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 205
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 206
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
207
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 208
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 209
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 210
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 211
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, ), and the ID is 212	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors

	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 213
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 214
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 215
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 216
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
217
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 218
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 219
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 220
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 221
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(64, 2080, ), and the ID is 222
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 223
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 224
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 225
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 226
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and the ID is 227
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 228
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 229
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 230
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 231
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
232
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 233
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 234
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 235
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 236
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, ), and Output shape (64, 2080, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
237
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 238
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (192, 272)
Applying Conv Low-Precision for Kernel shape (192, 2080, ), Input shape (64, 2080, ), and Output shape (64, 192, ), and the ID is 239
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (224, 576, ), Input shape (64, 192, ), and Output shape (64, 224, ), and the ID is 240
	Allocating LowPrecision Weight Tensors with Shape of (224, 80)
	Allocating LowPrecision Activations Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (256, 672, ), Input shape (64, 224, ), and Output shape (64, 256, ), and the ID is 241
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 96)
	Allocating LowPrecision Activations Tensors with Shape of (64, 96)
Applying Conv Low-Precision for Kernel shape (2080, 448, ), Input shape (64, 448, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and Output shape (64, 2080, ), and the ID is 242
	Allocating LowPrecision Weight Tensors with Shape of (2080, 64)
	Allocating LowPrecision Activations Tensors with Shape of (64, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1536, 2080, ), Input shape (64, 2080, ), and Output shape (64, 1536, ), and the ID is 243
	Allocating LowPrecision Weight Tensors with Shape of (1536, 272)
	Allocating LowPrecision Activations Tensors with Shape of (64, 272)
Applying Low-Precision for shape (1000, 1536, ) and Input shape (1, 1536, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 192)
	Transformed Activation Shape From: (1, 1536) To: (1, 192)
The input model file size (MB): 57.7798
Initialized session in 254.436ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=2215526 curr=2197637 min=2188572 max=2215526 avg=2.19822e+06 std=6575

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=2220482 curr=2199156 min=2196428 max=2220482 avg=2.20477e+06 std=5461

Inference timings in us: Init: 254436, First inference: 2215526, Warmup (avg): 2.19822e+06, Inference (avg): 2.20477e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=69.4648 overall=254
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  205.740	  205.740	100.000%	100.000%	 52660.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  205.740	  205.740	100.000%	100.000%	 52660.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   205.740	   100.000%	   100.000%	 52660.000	        1

Timings (microseconds): count=1 curr=205740
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.060	   63.806	   64.528	  2.931%	  2.931%	     0.000	        1	[inception_resnet_v2/activation_94/Relu;inception_resnet_v2/batch_normalization_94/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_94/Conv2D]:0
	                 CONV_2D	           64.600	   50.549	   50.573	  2.297%	  5.228%	     0.000	        1	[inception_resnet_v2/activation_95/Relu;inception_resnet_v2/batch_normalization_95/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_95/Conv2D]:1
	                 CONV_2D	          115.185	   65.694	   65.960	  2.996%	  8.224%	     0.000	        1	[inception_resnet_v2/activation_96/Relu;inception_resnet_v2/batch_normalization_96/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_96/Conv2D]:2
	             MAX_POOL_2D	          181.158	    8.903	    8.955	  0.407%	  8.631%	     0.000	        1	[inception_resnet_v2/max_pooling2d_4/MaxPool]:3
	                 CONV_2D	          190.123	   18.285	   18.539	  0.842%	  9.473%	     0.000	        1	[inception_resnet_v2/activation_97/Relu;inception_resnet_v2/batch_normalization_97/FusedBatchNormV3;inception_resnet_v2/batch_normalization_97/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_97/Conv2D]:4
	                 CONV_2D	          208.674	   28.725	   28.481	  1.294%	 10.767%	     0.000	        1	[inception_resnet_v2/activation_98/Relu;inception_resnet_v2/batch_normalization_98/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_98/Conv2D]:5
	             MAX_POOL_2D	          237.167	    5.732	    5.726	  0.260%	 11.027%	     0.000	        1	[inception_resnet_v2/max_pooling2d_5/MaxPool]:6
	         AVERAGE_POOL_2D	          242.904	   45.209	   45.414	  2.063%	 13.090%	     0.000	        1	[inception_resnet_v2/average_pooling2d_9/AvgPool]:7
	                 CONV_2D	          288.329	    3.548	    3.591	  0.163%	 13.253%	     0.000	        1	[inception_resnet_v2/activation_105/Relu;inception_resnet_v2/batch_normalization_105/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_105/Conv2D]:8
	                 CONV_2D	          291.929	    3.107	    3.146	  0.143%	 13.396%	     0.000	        1	[inception_resnet_v2/activation_100/Relu;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_100/Conv2D]:9
	                 CONV_2D	          295.083	    6.557	    6.597	  0.300%	 13.695%	     0.000	        1	[inception_resnet_v2/activation_101/Relu;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_101/Conv2D]:10
	                 CONV_2D	          301.691	    3.547	    3.584	  0.163%	 13.858%	     0.000	        1	[inception_resnet_v2/activation_102/Relu;inception_resnet_v2/batch_normalization_102/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_102/Conv2D]:11
	                 CONV_2D	          305.283	    4.362	    4.275	  0.194%	 14.052%	     0.000	        1	[inception_resnet_v2/activation_103/Relu;inception_resnet_v2/batch_normalization_103/FusedBatchNormV3;inception_resnet_v2/batch_normalization_103/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_103/Conv2D]:12
	                 CONV_2D	          309.571	    4.233	    4.122	  0.187%	 14.239%	     0.000	        1	[inception_resnet_v2/activation_104/Relu;inception_resnet_v2/batch_normalization_104/FusedBatchNormV3;inception_resnet_v2/batch_normalization_103/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_104/Conv2D]:13
	                 CONV_2D	          313.706	    4.440	    4.473	  0.203%	 14.443%	     0.000	        1	[inception_resnet_v2/activation_99/Relu;inception_resnet_v2/batch_normalization_99/FusedBatchNormV3;inception_resnet_v2/batch_normalization_103/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_99/Conv2D]:14
	           CONCATENATION	          318.188	    0.504	    0.496	  0.023%	 14.465%	     0.000	        1	[inception_resnet_v2/mixed_5b/concat]:15
	                 CONV_2D	          318.692	    2.430	    2.467	  0.112%	 14.577%	     0.000	        1	[inception_resnet_v2/activation_106/Relu;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_106/Conv2D]:16
	                 CONV_2D	          321.168	    2.396	    2.436	  0.111%	 14.688%	     0.000	        1	[inception_resnet_v2/activation_107/Relu;inception_resnet_v2/batch_normalization_107/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_107/Conv2D]:17
	                 CONV_2D	          323.613	    2.687	    2.745	  0.125%	 14.812%	     0.000	        1	[inception_resnet_v2/activation_108/Relu;inception_resnet_v2/batch_normalization_108/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_108/Conv2D]:18
	                 CONV_2D	          326.370	    2.402	    2.445	  0.111%	 14.924%	     0.000	        1	[inception_resnet_v2/activation_109/Relu;inception_resnet_v2/batch_normalization_109/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_109/Conv2D]:19
	                 CONV_2D	          328.824	    3.101	    3.152	  0.143%	 15.067%	     0.000	        1	[inception_resnet_v2/activation_110/Relu;inception_resnet_v2/batch_normalization_110/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_110/Conv2D]:20
	                 CONV_2D	          331.985	    3.389	    3.439	  0.156%	 15.223%	     0.000	        1	[inception_resnet_v2/activation_111/Relu;inception_resnet_v2/batch_normalization_111/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_111/Conv2D]:21
	           CONCATENATION	          335.435	    0.244	    0.240	  0.011%	 15.234%	     0.000	        1	[inception_resnet_v2/block35_1_mixed/concat]:22
	                 CONV_2D	          335.682	   10.765	   10.829	  0.492%	 15.726%	     0.000	        1	[inception_resnet_v2/block35_1/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_1_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_1_conv/Conv2D]:23
	                     ADD	          346.521	   37.244	   37.483	  1.703%	 17.428%	     0.000	        1	[inception_resnet_v2/block35_1_ac/Relu;inception_resnet_v2/block35_1/add]:24
	                 CONV_2D	          384.015	    2.442	    2.472	  0.112%	 17.540%	     0.000	        1	[inception_resnet_v2/activation_112/Relu;inception_resnet_v2/batch_normalization_112/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_112/Conv2D]:25
	                 CONV_2D	          386.496	    2.374	    2.410	  0.109%	 17.650%	     0.000	        1	[inception_resnet_v2/activation_113/Relu;inception_resnet_v2/batch_normalization_113/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_113/Conv2D]:26
	                 CONV_2D	          388.914	    2.680	    2.726	  0.124%	 17.774%	     0.000	        1	[inception_resnet_v2/activation_114/Relu;inception_resnet_v2/batch_normalization_114/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_114/Conv2D]:27
	                 CONV_2D	          391.650	    2.381	    2.447	  0.111%	 17.885%	     0.000	        1	[inception_resnet_v2/activation_115/Relu;inception_resnet_v2/batch_normalization_115/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_115/Conv2D]:28
	                 CONV_2D	          394.106	    3.135	    3.168	  0.144%	 18.029%	     0.000	        1	[inception_resnet_v2/activation_116/Relu;inception_resnet_v2/batch_normalization_116/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_116/Conv2D]:29
	                 CONV_2D	          397.284	    3.384	    3.422	  0.155%	 18.184%	     0.000	        1	[inception_resnet_v2/activation_117/Relu;inception_resnet_v2/batch_normalization_117/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_117/Conv2D]:30
	           CONCATENATION	          400.716	    0.229	    0.223	  0.010%	 18.194%	     0.000	        1	[inception_resnet_v2/block35_2_mixed/concat]:31
	                 CONV_2D	          400.946	   10.729	   10.823	  0.492%	 18.686%	     0.000	        1	[inception_resnet_v2/block35_2/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_2_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_2_conv/Conv2D]:32
	                     ADD	          411.779	   37.258	   37.467	  1.702%	 20.388%	     0.000	        1	[inception_resnet_v2/block35_2_ac/Relu;inception_resnet_v2/block35_2/add]:33
	                 CONV_2D	          449.258	    2.438	    2.463	  0.112%	 20.500%	     0.000	        1	[inception_resnet_v2/activation_118/Relu;inception_resnet_v2/batch_normalization_118/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_118/Conv2D]:34
	                 CONV_2D	          451.730	    2.367	    2.429	  0.110%	 20.610%	     0.000	        1	[inception_resnet_v2/activation_119/Relu;inception_resnet_v2/batch_normalization_119/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_119/Conv2D]:35
	                 CONV_2D	          454.167	    2.688	    2.716	  0.123%	 20.733%	     0.000	        1	[inception_resnet_v2/activation_120/Relu;inception_resnet_v2/batch_normalization_120/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_120/Conv2D]:36
	                 CONV_2D	          456.892	    2.370	    2.443	  0.111%	 20.844%	     0.000	        1	[inception_resnet_v2/activation_121/Relu;inception_resnet_v2/batch_normalization_121/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_121/Conv2D]:37
	                 CONV_2D	          459.343	    3.089	    3.159	  0.143%	 20.988%	     0.000	        1	[inception_resnet_v2/activation_122/Relu;inception_resnet_v2/batch_normalization_122/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_122/Conv2D]:38
	                 CONV_2D	          462.512	    3.348	    3.411	  0.155%	 21.143%	     0.000	        1	[inception_resnet_v2/activation_123/Relu;inception_resnet_v2/batch_normalization_123/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_123/Conv2D]:39
	           CONCATENATION	          465.933	    0.245	    0.236	  0.011%	 21.154%	     0.000	        1	[inception_resnet_v2/block35_3_mixed/concat]:40
	                 CONV_2D	          466.177	   10.745	   10.929	  0.496%	 21.650%	     0.000	        1	[inception_resnet_v2/block35_3/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_3_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_3_conv/Conv2D]:41
	                     ADD	          477.116	   37.214	   37.435	  1.700%	 23.350%	     0.000	        1	[inception_resnet_v2/block35_3_ac/Relu;inception_resnet_v2/block35_3/add]:42
	                 CONV_2D	          514.562	    2.429	    2.458	  0.112%	 23.462%	     0.000	        1	[inception_resnet_v2/activation_124/Relu;inception_resnet_v2/batch_normalization_124/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_124/Conv2D]:43
	                 CONV_2D	          517.029	    2.357	    2.421	  0.110%	 23.572%	     0.000	        1	[inception_resnet_v2/activation_125/Relu;inception_resnet_v2/batch_normalization_125/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_125/Conv2D]:44
	                 CONV_2D	          519.459	    2.674	    2.723	  0.124%	 23.696%	     0.000	        1	[inception_resnet_v2/activation_126/Relu;inception_resnet_v2/batch_normalization_126/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_126/Conv2D]:45
	                 CONV_2D	          522.191	    2.379	    2.429	  0.110%	 23.806%	     0.000	        1	[inception_resnet_v2/activation_127/Relu;inception_resnet_v2/batch_normalization_127/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_127/Conv2D]:46
	                 CONV_2D	          524.629	    3.065	    3.160	  0.144%	 23.949%	     0.000	        1	[inception_resnet_v2/activation_128/Relu;inception_resnet_v2/batch_normalization_128/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_128/Conv2D]:47
	                 CONV_2D	          527.798	    3.377	    3.424	  0.156%	 24.105%	     0.000	        1	[inception_resnet_v2/activation_129/Relu;inception_resnet_v2/batch_normalization_129/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_129/Conv2D]:48
	           CONCATENATION	          531.235	    0.233	    0.236	  0.011%	 24.116%	     0.000	        1	[inception_resnet_v2/block35_4_mixed/concat]:49
	                 CONV_2D	          531.479	   10.718	   10.812	  0.491%	 24.607%	     0.000	        1	[inception_resnet_v2/block35_4/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_4_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_4_conv/Conv2D]:50
	                     ADD	          542.301	   37.321	   37.426	  1.700%	 26.307%	     0.000	        1	[inception_resnet_v2/block35_4_ac/Relu;inception_resnet_v2/block35_4/add]:51
	                 CONV_2D	          579.739	    2.426	    2.463	  0.112%	 26.419%	     0.000	        1	[inception_resnet_v2/activation_130/Relu;inception_resnet_v2/batch_normalization_130/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_130/Conv2D]:52
	                 CONV_2D	          582.210	    2.363	    2.417	  0.110%	 26.528%	     0.000	        1	[inception_resnet_v2/activation_131/Relu;inception_resnet_v2/batch_normalization_131/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_131/Conv2D]:53
	                 CONV_2D	          584.635	    2.666	    2.723	  0.124%	 26.652%	     0.000	        1	[inception_resnet_v2/activation_132/Relu;inception_resnet_v2/batch_normalization_132/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_132/Conv2D]:54
	                 CONV_2D	          587.368	    2.397	    2.419	  0.110%	 26.762%	     0.000	        1	[inception_resnet_v2/activation_133/Relu;inception_resnet_v2/batch_normalization_133/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_133/Conv2D]:55
	                 CONV_2D	          589.795	    3.084	    3.146	  0.143%	 26.905%	     0.000	        1	[inception_resnet_v2/activation_134/Relu;inception_resnet_v2/batch_normalization_134/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_134/Conv2D]:56
	                 CONV_2D	          592.951	    3.304	    3.401	  0.154%	 27.059%	     0.000	        1	[inception_resnet_v2/activation_135/Relu;inception_resnet_v2/batch_normalization_135/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_135/Conv2D]:57
	           CONCATENATION	          596.362	    0.247	    0.239	  0.011%	 27.070%	     0.000	        1	[inception_resnet_v2/block35_5_mixed/concat]:58
	                 CONV_2D	          596.608	   10.838	   10.839	  0.492%	 27.563%	     0.000	        1	[inception_resnet_v2/block35_5/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_5_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_5_conv/Conv2D]:59
	                     ADD	          607.458	   37.306	   37.590	  1.707%	 29.270%	     0.000	        1	[inception_resnet_v2/block35_5_ac/Relu;inception_resnet_v2/block35_5/add]:60
	                 CONV_2D	          645.059	    2.451	    2.454	  0.111%	 29.381%	     0.000	        1	[inception_resnet_v2/activation_136/Relu;inception_resnet_v2/batch_normalization_136/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_136/Conv2D]:61
	                 CONV_2D	          647.522	    2.371	    2.409	  0.109%	 29.491%	     0.000	        1	[inception_resnet_v2/activation_137/Relu;inception_resnet_v2/batch_normalization_137/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_137/Conv2D]:62
	                 CONV_2D	          649.939	    2.651	    2.714	  0.123%	 29.614%	     0.000	        1	[inception_resnet_v2/activation_138/Relu;inception_resnet_v2/batch_normalization_138/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_138/Conv2D]:63
	                 CONV_2D	          652.662	    2.413	    2.419	  0.110%	 29.724%	     0.000	        1	[inception_resnet_v2/activation_139/Relu;inception_resnet_v2/batch_normalization_139/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_139/Conv2D]:64
	                 CONV_2D	          655.089	    3.117	    3.140	  0.143%	 29.867%	     0.000	        1	[inception_resnet_v2/activation_140/Relu;inception_resnet_v2/batch_normalization_140/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_140/Conv2D]:65
	                 CONV_2D	          658.238	    3.355	    3.395	  0.154%	 30.021%	     0.000	        1	[inception_resnet_v2/activation_141/Relu;inception_resnet_v2/batch_normalization_141/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_141/Conv2D]:66
	           CONCATENATION	          661.643	    0.209	    0.222	  0.010%	 30.031%	     0.000	        1	[inception_resnet_v2/block35_6_mixed/concat]:67
	                 CONV_2D	          661.872	   10.714	   10.800	  0.491%	 30.521%	     0.000	        1	[inception_resnet_v2/block35_6/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_6_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_6_conv/Conv2D]:68
	                     ADD	          672.682	   37.253	   37.421	  1.700%	 32.221%	     0.000	        1	[inception_resnet_v2/block35_6_ac/Relu;inception_resnet_v2/block35_6/add]:69
	                 CONV_2D	          710.115	    2.438	    2.457	  0.112%	 32.333%	     0.000	        1	[inception_resnet_v2/activation_142/Relu;inception_resnet_v2/batch_normalization_142/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_142/Conv2D]:70
	                 CONV_2D	          712.581	    2.377	    2.415	  0.110%	 32.443%	     0.000	        1	[inception_resnet_v2/activation_143/Relu;inception_resnet_v2/batch_normalization_143/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_143/Conv2D]:71
	                 CONV_2D	          715.005	    2.695	    2.718	  0.123%	 32.566%	     0.000	        1	[inception_resnet_v2/activation_144/Relu;inception_resnet_v2/batch_normalization_144/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_144/Conv2D]:72
	                 CONV_2D	          717.732	    2.386	    2.430	  0.110%	 32.676%	     0.000	        1	[inception_resnet_v2/activation_145/Relu;inception_resnet_v2/batch_normalization_145/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_145/Conv2D]:73
	                 CONV_2D	          720.170	    3.101	    3.153	  0.143%	 32.820%	     0.000	        1	[inception_resnet_v2/activation_146/Relu;inception_resnet_v2/batch_normalization_146/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_146/Conv2D]:74
	                 CONV_2D	          723.333	    3.387	    3.411	  0.155%	 32.975%	     0.000	        1	[inception_resnet_v2/activation_147/Relu;inception_resnet_v2/batch_normalization_147/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_147/Conv2D]:75
	           CONCATENATION	          726.754	    0.241	    0.240	  0.011%	 32.985%	     0.000	        1	[inception_resnet_v2/block35_7_mixed/concat]:76
	                 CONV_2D	          727.003	   10.714	   10.799	  0.491%	 33.476%	     0.000	        1	[inception_resnet_v2/block35_7/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_7_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_7_conv/Conv2D]:77
	                     ADD	          737.811	   37.282	   37.476	  1.702%	 35.178%	     0.000	        1	[inception_resnet_v2/block35_7_ac/Relu;inception_resnet_v2/block35_7/add]:78
	                 CONV_2D	          775.298	    2.413	    2.469	  0.112%	 35.290%	     0.000	        1	[inception_resnet_v2/activation_148/Relu;inception_resnet_v2/batch_normalization_148/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_148/Conv2D]:79
	                 CONV_2D	          777.776	    2.397	    2.431	  0.110%	 35.401%	     0.000	        1	[inception_resnet_v2/activation_149/Relu;inception_resnet_v2/batch_normalization_149/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_149/Conv2D]:80
	                 CONV_2D	          780.215	    2.728	    2.798	  0.127%	 35.528%	     0.000	        1	[inception_resnet_v2/activation_150/Relu;inception_resnet_v2/batch_normalization_150/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_150/Conv2D]:81
	                 CONV_2D	          783.023	    2.453	    2.504	  0.114%	 35.642%	     0.000	        1	[inception_resnet_v2/activation_151/Relu;inception_resnet_v2/batch_normalization_151/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_151/Conv2D]:82
	                 CONV_2D	          785.535	    3.146	    3.208	  0.146%	 35.787%	     0.000	        1	[inception_resnet_v2/activation_152/Relu;inception_resnet_v2/batch_normalization_152/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_152/Conv2D]:83
	                 CONV_2D	          788.754	    3.488	    3.526	  0.160%	 35.947%	     0.000	        1	[inception_resnet_v2/activation_153/Relu;inception_resnet_v2/batch_normalization_153/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_153/Conv2D]:84
	           CONCATENATION	          792.291	    0.200	    0.223	  0.010%	 35.958%	     0.000	        1	[inception_resnet_v2/block35_8_mixed/concat]:85
	                 CONV_2D	          792.521	   10.801	   10.893	  0.495%	 36.452%	     0.000	        1	[inception_resnet_v2/block35_8/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_8_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_8_conv/Conv2D]:86
	                     ADD	          803.426	   37.240	   37.486	  1.703%	 38.155%	     0.000	        1	[inception_resnet_v2/block35_8_ac/Relu;inception_resnet_v2/block35_8/add]:87
	                 CONV_2D	          840.923	    2.527	    2.536	  0.115%	 38.270%	     0.000	        1	[inception_resnet_v2/activation_154/Relu;inception_resnet_v2/batch_normalization_154/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_154/Conv2D]:88
	                 CONV_2D	          843.468	    2.481	    2.474	  0.112%	 38.383%	     0.000	        1	[inception_resnet_v2/activation_155/Relu;inception_resnet_v2/batch_normalization_155/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_155/Conv2D]:89
	                 CONV_2D	          845.951	    2.752	    2.807	  0.127%	 38.510%	     0.000	        1	[inception_resnet_v2/activation_156/Relu;inception_resnet_v2/batch_normalization_156/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_156/Conv2D]:90
	                 CONV_2D	          848.769	    2.471	    2.490	  0.113%	 38.623%	     0.000	        1	[inception_resnet_v2/activation_157/Relu;inception_resnet_v2/batch_normalization_157/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_157/Conv2D]:91
	                 CONV_2D	          851.267	    3.156	    3.213	  0.146%	 38.769%	     0.000	        1	[inception_resnet_v2/activation_158/Relu;inception_resnet_v2/batch_normalization_158/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_158/Conv2D]:92
	                 CONV_2D	          854.491	    3.482	    3.478	  0.158%	 38.927%	     0.000	        1	[inception_resnet_v2/activation_159/Relu;inception_resnet_v2/batch_normalization_159/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_159/Conv2D]:93
	           CONCATENATION	          857.980	    0.245	    0.245	  0.011%	 38.938%	     0.000	        1	[inception_resnet_v2/block35_9_mixed/concat]:94
	                 CONV_2D	          858.232	   10.831	   10.873	  0.494%	 39.432%	     0.000	        1	[inception_resnet_v2/block35_9/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_9_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_9_conv/Conv2D]:95
	                     ADD	          869.115	   37.215	   37.402	  1.699%	 41.131%	     0.000	        1	[inception_resnet_v2/block35_9_ac/Relu;inception_resnet_v2/block35_9/add]:96
	                 CONV_2D	          906.529	    2.528	    2.532	  0.115%	 41.246%	     0.000	        1	[inception_resnet_v2/activation_160/Relu;inception_resnet_v2/batch_normalization_160/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_160/Conv2D]:97
	                 CONV_2D	          909.070	    2.451	    2.476	  0.112%	 41.358%	     0.000	        1	[inception_resnet_v2/activation_161/Relu;inception_resnet_v2/batch_normalization_161/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_161/Conv2D]:98
	                 CONV_2D	          911.554	    2.741	    2.789	  0.127%	 41.485%	     0.000	        1	[inception_resnet_v2/activation_162/Relu;inception_resnet_v2/batch_normalization_162/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_162/Conv2D]:99
	                 CONV_2D	          914.353	    2.485	    2.516	  0.114%	 41.599%	     0.000	        1	[inception_resnet_v2/activation_163/Relu;inception_resnet_v2/batch_normalization_163/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_163/Conv2D]:100
	                 CONV_2D	          916.879	    3.193	    3.206	  0.146%	 41.745%	     0.000	        1	[inception_resnet_v2/activation_164/Relu;inception_resnet_v2/batch_normalization_164/FusedBatchNormV3;inception_resnet_v2/batch_normalization_100/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_164/Conv2D]:101
	                 CONV_2D	          920.094	    3.509	    3.482	  0.158%	 41.903%	     0.000	        1	[inception_resnet_v2/activation_165/Relu;inception_resnet_v2/batch_normalization_165/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_165/Conv2D]:102
	           CONCATENATION	          923.589	    0.218	    0.228	  0.010%	 41.914%	     0.000	        1	[inception_resnet_v2/block35_10_mixed/concat]:103
	                 CONV_2D	          923.824	   10.836	   10.870	  0.494%	 42.407%	     0.000	        1	[inception_resnet_v2/block35_10/mul;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/block35_10_conv/BiasAdd;inception_resnet_v2/block35_1/mul/y;inception_resnet_v2/block35_10_conv/Conv2D]:104
	                     ADD	          934.705	   37.341	   37.389	  1.698%	 44.106%	     0.000	        1	[inception_resnet_v2/block35_10_ac/Relu;inception_resnet_v2/block35_10/add]:105
	                 CONV_2D	          972.105	    7.754	    7.851	  0.357%	 44.462%	     0.000	        1	[inception_resnet_v2/activation_166/Relu;inception_resnet_v2/batch_normalization_166/FusedBatchNormV3;inception_resnet_v2/batch_normalization_166/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_166/Conv2D]:106
	                 CONV_2D	          979.968	    8.624	    8.652	  0.393%	 44.855%	     0.000	        1	[inception_resnet_v2/activation_167/Relu;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_167/Conv2D]:107
	                 CONV_2D	          988.631	   25.697	   25.696	  1.167%	 46.022%	     0.000	        1	[inception_resnet_v2/activation_168/Relu;inception_resnet_v2/batch_normalization_168/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_168/Conv2D]:108
	                 CONV_2D	         1014.341	    8.075	    8.011	  0.364%	 46.386%	     0.000	        1	[inception_resnet_v2/activation_169/Relu;inception_resnet_v2/batch_normalization_169/FusedBatchNormV3;inception_resnet_v2/batch_normalization_166/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_169/Conv2D]:109
	             MAX_POOL_2D	         1022.363	    2.350	    2.357	  0.107%	 46.493%	     0.000	        1	[inception_resnet_v2/max_pooling2d_6/MaxPool]:110
	           CONCATENATION	         1024.729	    0.302	    0.293	  0.013%	 46.507%	     0.000	        1	[inception_resnet_v2/mixed_6a/concat]:111
	                 CONV_2D	         1025.030	    2.945	    2.931	  0.133%	 46.640%	     0.000	        1	[inception_resnet_v2/activation_170/Relu;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_170/Conv2D]:112
	                 CONV_2D	         1027.969	    2.072	    2.094	  0.095%	 46.735%	     0.000	        1	[inception_resnet_v2/activation_171/Relu;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_171/Conv2D]:113
	                 CONV_2D	         1030.072	    1.325	    1.363	  0.062%	 46.797%	     0.000	        1	[inception_resnet_v2/activation_172/Relu;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_172/Conv2D]:114
	                 CONV_2D	         1031.444	    3.018	    3.091	  0.140%	 46.937%	     0.000	        1	[inception_resnet_v2/activation_173/Relu;inception_resnet_v2/batch_normalization_173/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_173/Conv2D]:115
	           CONCATENATION	         1034.544	    0.127	    0.120	  0.005%	 46.943%	     0.000	        1	[inception_resnet_v2/block17_1_mixed/concat]:116
	                 CONV_2D	         1034.671	    7.462	    7.472	  0.339%	 47.282%	     0.000	        1	[inception_resnet_v2/block17_1/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_1_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_1_conv/Conv2D]:117
	                     ADD	         1042.152	   29.890	   30.083	  1.366%	 48.648%	     0.000	        1	[inception_resnet_v2/block17_1_ac/Relu;inception_resnet_v2/block17_1/add]:118
	                 CONV_2D	         1072.246	    2.875	    2.916	  0.132%	 48.781%	     0.000	        1	[inception_resnet_v2/activation_174/Relu;inception_resnet_v2/batch_normalization_174/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_174/Conv2D]:119
	                 CONV_2D	         1075.171	    2.068	    2.089	  0.095%	 48.876%	     0.000	        1	[inception_resnet_v2/activation_175/Relu;inception_resnet_v2/batch_normalization_175/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_175/Conv2D]:120
	                 CONV_2D	         1077.268	    1.284	    1.328	  0.060%	 48.936%	     0.000	        1	[inception_resnet_v2/activation_176/Relu;inception_resnet_v2/batch_normalization_176/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_176/Conv2D]:121
	                 CONV_2D	         1078.605	    2.976	    3.037	  0.138%	 49.074%	     0.000	        1	[inception_resnet_v2/activation_177/Relu;inception_resnet_v2/batch_normalization_177/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_177/Conv2D]:122
	           CONCATENATION	         1081.650	    0.102	    0.112	  0.005%	 49.079%	     0.000	        1	[inception_resnet_v2/block17_2_mixed/concat]:123
	                 CONV_2D	         1081.769	    7.409	    7.462	  0.339%	 49.418%	     0.000	        1	[inception_resnet_v2/block17_2/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_2_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_2_conv/Conv2D]:124
	                     ADD	         1089.241	   29.927	   29.975	  1.362%	 50.780%	     0.000	        1	[inception_resnet_v2/block17_2_ac/Relu;inception_resnet_v2/block17_2/add]:125
	                 CONV_2D	         1119.226	    2.899	    2.905	  0.132%	 50.912%	     0.000	        1	[inception_resnet_v2/activation_178/Relu;inception_resnet_v2/batch_normalization_178/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_178/Conv2D]:126
	                 CONV_2D	         1122.139	    2.064	    2.086	  0.095%	 51.006%	     0.000	        1	[inception_resnet_v2/activation_179/Relu;inception_resnet_v2/batch_normalization_179/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_179/Conv2D]:127
	                 CONV_2D	         1124.233	    1.297	    1.315	  0.060%	 51.066%	     0.000	        1	[inception_resnet_v2/activation_180/Relu;inception_resnet_v2/batch_normalization_180/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_180/Conv2D]:128
	                 CONV_2D	         1125.557	    3.004	    3.049	  0.138%	 51.205%	     0.000	        1	[inception_resnet_v2/activation_181/Relu;inception_resnet_v2/batch_normalization_181/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_181/Conv2D]:129
	           CONCATENATION	         1128.614	    0.113	    0.113	  0.005%	 51.210%	     0.000	        1	[inception_resnet_v2/block17_3_mixed/concat]:130
	                 CONV_2D	         1128.734	    7.404	    7.461	  0.339%	 51.549%	     0.000	        1	[inception_resnet_v2/block17_3/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_3_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_3_conv/Conv2D]:131
	                     ADD	         1136.203	   29.855	   29.975	  1.362%	 52.910%	     0.000	        1	[inception_resnet_v2/block17_3_ac/Relu;inception_resnet_v2/block17_3/add]:132
	                 CONV_2D	         1166.189	    2.892	    2.910	  0.132%	 53.042%	     0.000	        1	[inception_resnet_v2/activation_182/Relu;inception_resnet_v2/batch_normalization_182/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_182/Conv2D]:133
	                 CONV_2D	         1169.107	    2.053	    2.097	  0.095%	 53.137%	     0.000	        1	[inception_resnet_v2/activation_183/Relu;inception_resnet_v2/batch_normalization_183/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_183/Conv2D]:134
	                 CONV_2D	         1171.212	    1.302	    1.320	  0.060%	 53.197%	     0.000	        1	[inception_resnet_v2/activation_184/Relu;inception_resnet_v2/batch_normalization_184/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_184/Conv2D]:135
	                 CONV_2D	         1172.540	    2.969	    3.027	  0.138%	 53.335%	     0.000	        1	[inception_resnet_v2/activation_185/Relu;inception_resnet_v2/batch_normalization_185/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_185/Conv2D]:136
	           CONCATENATION	         1175.576	    0.089	    0.108	  0.005%	 53.340%	     0.000	        1	[inception_resnet_v2/block17_4_mixed/concat]:137
	                 CONV_2D	         1175.691	    7.495	    7.469	  0.339%	 53.679%	     0.000	        1	[inception_resnet_v2/block17_4/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_4_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_4_conv/Conv2D]:138
	                     ADD	         1183.168	   29.899	   29.999	  1.363%	 55.042%	     0.000	        1	[inception_resnet_v2/block17_4_ac/Relu;inception_resnet_v2/block17_4/add]:139
	                 CONV_2D	         1213.178	    2.876	    2.914	  0.132%	 55.174%	     0.000	        1	[inception_resnet_v2/activation_186/Relu;inception_resnet_v2/batch_normalization_186/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_186/Conv2D]:140
	                 CONV_2D	         1216.101	    2.078	    2.087	  0.095%	 55.269%	     0.000	        1	[inception_resnet_v2/activation_187/Relu;inception_resnet_v2/batch_normalization_187/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_187/Conv2D]:141
	                 CONV_2D	         1218.196	    1.301	    1.315	  0.060%	 55.329%	     0.000	        1	[inception_resnet_v2/activation_188/Relu;inception_resnet_v2/batch_normalization_188/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_188/Conv2D]:142
	                 CONV_2D	         1219.521	    2.989	    3.038	  0.138%	 55.467%	     0.000	        1	[inception_resnet_v2/activation_189/Relu;inception_resnet_v2/batch_normalization_189/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_189/Conv2D]:143
	           CONCATENATION	         1222.568	    0.101	    0.111	  0.005%	 55.472%	     0.000	        1	[inception_resnet_v2/block17_5_mixed/concat]:144
	                 CONV_2D	         1222.686	    7.421	    7.464	  0.339%	 55.811%	     0.000	        1	[inception_resnet_v2/block17_5/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_5_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_5_conv/Conv2D]:145
	                     ADD	         1230.160	   29.894	   30.008	  1.363%	 57.174%	     0.000	        1	[inception_resnet_v2/block17_5_ac/Relu;inception_resnet_v2/block17_5/add]:146
	                 CONV_2D	         1260.179	    2.981	    3.013	  0.137%	 57.311%	     0.000	        1	[inception_resnet_v2/activation_190/Relu;inception_resnet_v2/batch_normalization_190/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_190/Conv2D]:147
	                 CONV_2D	         1263.202	    2.062	    2.103	  0.096%	 57.406%	     0.000	        1	[inception_resnet_v2/activation_191/Relu;inception_resnet_v2/batch_normalization_191/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_191/Conv2D]:148
	                 CONV_2D	         1265.313	    1.285	    1.330	  0.060%	 57.467%	     0.000	        1	[inception_resnet_v2/activation_192/Relu;inception_resnet_v2/batch_normalization_192/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_192/Conv2D]:149
	                 CONV_2D	         1266.651	    3.019	    3.096	  0.141%	 57.607%	     0.000	        1	[inception_resnet_v2/activation_193/Relu;inception_resnet_v2/batch_normalization_193/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_193/Conv2D]:150
	           CONCATENATION	         1269.756	    0.097	    0.115	  0.005%	 57.612%	     0.000	        1	[inception_resnet_v2/block17_6_mixed/concat]:151
	                 CONV_2D	         1269.877	    7.411	    7.474	  0.339%	 57.952%	     0.000	        1	[inception_resnet_v2/block17_6/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_6_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_6_conv/Conv2D]:152
	                     ADD	         1277.360	   29.892	   30.002	  1.363%	 59.315%	     0.000	        1	[inception_resnet_v2/block17_6_ac/Relu;inception_resnet_v2/block17_6/add]:153
	                 CONV_2D	         1307.375	    3.196	    3.101	  0.141%	 59.455%	     0.000	        1	[inception_resnet_v2/activation_194/Relu;inception_resnet_v2/batch_normalization_194/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_194/Conv2D]:154
	                 CONV_2D	         1310.485	    2.074	    2.093	  0.095%	 59.551%	     0.000	        1	[inception_resnet_v2/activation_195/Relu;inception_resnet_v2/batch_normalization_195/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_195/Conv2D]:155
	                 CONV_2D	         1312.587	    1.308	    1.331	  0.060%	 59.611%	     0.000	        1	[inception_resnet_v2/activation_196/Relu;inception_resnet_v2/batch_normalization_196/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_196/Conv2D]:156
	                 CONV_2D	         1313.927	    3.048	    3.134	  0.142%	 59.753%	     0.000	        1	[inception_resnet_v2/activation_197/Relu;inception_resnet_v2/batch_normalization_197/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_197/Conv2D]:157
	           CONCATENATION	         1317.070	    0.106	    0.122	  0.006%	 59.759%	     0.000	        1	[inception_resnet_v2/block17_7_mixed/concat]:158
	                 CONV_2D	         1317.199	    7.419	    7.464	  0.339%	 60.098%	     0.000	        1	[inception_resnet_v2/block17_7/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_7_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_7_conv/Conv2D]:159
	                     ADD	         1324.673	   29.877	   30.032	  1.364%	 61.462%	     0.000	        1	[inception_resnet_v2/block17_7_ac/Relu;inception_resnet_v2/block17_7/add]:160
	                 CONV_2D	         1354.716	    3.109	    3.095	  0.141%	 61.603%	     0.000	        1	[inception_resnet_v2/activation_198/Relu;inception_resnet_v2/batch_normalization_198/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_198/Conv2D]:161
	                 CONV_2D	         1357.820	    2.057	    2.094	  0.095%	 61.698%	     0.000	        1	[inception_resnet_v2/activation_199/Relu;inception_resnet_v2/batch_normalization_199/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_199/Conv2D]:162
	                 CONV_2D	         1359.923	    1.299	    1.329	  0.060%	 61.758%	     0.000	        1	[inception_resnet_v2/activation_200/Relu;inception_resnet_v2/batch_normalization_200/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_200/Conv2D]:163
	                 CONV_2D	         1361.260	    3.029	    3.100	  0.141%	 61.899%	     0.000	        1	[inception_resnet_v2/activation_201/Relu;inception_resnet_v2/batch_normalization_201/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_201/Conv2D]:164
	           CONCATENATION	         1364.369	    0.088	    0.109	  0.005%	 61.904%	     0.000	        1	[inception_resnet_v2/block17_8_mixed/concat]:165
	                 CONV_2D	         1364.485	    7.476	    7.471	  0.339%	 62.243%	     0.000	        1	[inception_resnet_v2/block17_8/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_8_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_8_conv/Conv2D]:166
	                     ADD	         1371.965	   29.938	   29.990	  1.362%	 63.605%	     0.000	        1	[inception_resnet_v2/block17_8_ac/Relu;inception_resnet_v2/block17_8/add]:167
	                 CONV_2D	         1401.966	    3.075	    3.112	  0.141%	 63.747%	     0.000	        1	[inception_resnet_v2/activation_202/Relu;inception_resnet_v2/batch_normalization_202/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_202/Conv2D]:168
	                 CONV_2D	         1405.088	    2.073	    2.098	  0.095%	 63.842%	     0.000	        1	[inception_resnet_v2/activation_203/Relu;inception_resnet_v2/batch_normalization_203/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_203/Conv2D]:169
	                 CONV_2D	         1407.194	    1.297	    1.322	  0.060%	 63.902%	     0.000	        1	[inception_resnet_v2/activation_204/Relu;inception_resnet_v2/batch_normalization_204/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_204/Conv2D]:170
	                 CONV_2D	         1408.524	    3.038	    3.094	  0.141%	 64.043%	     0.000	        1	[inception_resnet_v2/activation_205/Relu;inception_resnet_v2/batch_normalization_205/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_205/Conv2D]:171
	           CONCATENATION	         1411.627	    0.100	    0.112	  0.005%	 64.048%	     0.000	        1	[inception_resnet_v2/block17_9_mixed/concat]:172
	                 CONV_2D	         1411.746	    7.427	    7.480	  0.340%	 64.388%	     0.000	        1	[inception_resnet_v2/block17_9/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_9_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_9_conv/Conv2D]:173
	                     ADD	         1419.235	   29.929	   29.977	  1.362%	 65.749%	     0.000	        1	[inception_resnet_v2/block17_9_ac/Relu;inception_resnet_v2/block17_9/add]:174
	                 CONV_2D	         1449.223	    3.075	    3.098	  0.141%	 65.890%	     0.000	        1	[inception_resnet_v2/activation_206/Relu;inception_resnet_v2/batch_normalization_206/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_206/Conv2D]:175
	                 CONV_2D	         1452.331	    2.073	    2.099	  0.095%	 65.985%	     0.000	        1	[inception_resnet_v2/activation_207/Relu;inception_resnet_v2/batch_normalization_207/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_207/Conv2D]:176
	                 CONV_2D	         1454.438	    1.290	    1.323	  0.060%	 66.045%	     0.000	        1	[inception_resnet_v2/activation_208/Relu;inception_resnet_v2/batch_normalization_208/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_208/Conv2D]:177
	                 CONV_2D	         1455.770	    3.043	    3.113	  0.141%	 66.187%	     0.000	        1	[inception_resnet_v2/activation_209/Relu;inception_resnet_v2/batch_normalization_209/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_209/Conv2D]:178
	           CONCATENATION	         1458.893	    0.108	    0.109	  0.005%	 66.192%	     0.000	        1	[inception_resnet_v2/block17_10_mixed/concat]:179
	                 CONV_2D	         1459.009	    7.415	    7.458	  0.339%	 66.530%	     0.000	        1	[inception_resnet_v2/block17_10/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_10_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_10_conv/Conv2D]:180
	                     ADD	         1466.476	   29.919	   29.991	  1.362%	 67.893%	     0.000	        1	[inception_resnet_v2/block17_10_ac/Relu;inception_resnet_v2/block17_10/add]:181
	                 CONV_2D	         1496.479	    3.075	    3.128	  0.142%	 68.035%	     0.000	        1	[inception_resnet_v2/activation_210/Relu;inception_resnet_v2/batch_normalization_210/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_210/Conv2D]:182
	                 CONV_2D	         1499.616	    2.065	    2.116	  0.096%	 68.131%	     0.000	        1	[inception_resnet_v2/activation_211/Relu;inception_resnet_v2/batch_normalization_211/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_211/Conv2D]:183
	                 CONV_2D	         1501.741	    1.298	    1.326	  0.060%	 68.191%	     0.000	        1	[inception_resnet_v2/activation_212/Relu;inception_resnet_v2/batch_normalization_212/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_212/Conv2D]:184
	                 CONV_2D	         1503.075	    3.023	    3.125	  0.142%	 68.333%	     0.000	        1	[inception_resnet_v2/activation_213/Relu;inception_resnet_v2/batch_normalization_213/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_213/Conv2D]:185
	           CONCATENATION	         1506.210	    0.112	    0.116	  0.005%	 68.338%	     0.000	        1	[inception_resnet_v2/block17_11_mixed/concat]:186
	                 CONV_2D	         1506.333	    7.434	    7.476	  0.340%	 68.678%	     0.000	        1	[inception_resnet_v2/block17_11/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_11_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_11_conv/Conv2D]:187
	                     ADD	         1513.818	   30.046	   29.992	  1.362%	 70.040%	     0.000	        1	[inception_resnet_v2/block17_11_ac/Relu;inception_resnet_v2/block17_11/add]:188
	                 CONV_2D	         1543.823	    3.067	    3.097	  0.141%	 70.181%	     0.000	        1	[inception_resnet_v2/activation_214/Relu;inception_resnet_v2/batch_normalization_214/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_214/Conv2D]:189
	                 CONV_2D	         1546.930	    2.058	    2.118	  0.096%	 70.277%	     0.000	        1	[inception_resnet_v2/activation_215/Relu;inception_resnet_v2/batch_normalization_215/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_215/Conv2D]:190
	                 CONV_2D	         1549.057	    1.334	    1.327	  0.060%	 70.337%	     0.000	        1	[inception_resnet_v2/activation_216/Relu;inception_resnet_v2/batch_normalization_216/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_216/Conv2D]:191
	                 CONV_2D	         1550.392	    3.149	    3.100	  0.141%	 70.478%	     0.000	        1	[inception_resnet_v2/activation_217/Relu;inception_resnet_v2/batch_normalization_217/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_217/Conv2D]:192
	           CONCATENATION	         1553.502	    0.130	    0.114	  0.005%	 70.483%	     0.000	        1	[inception_resnet_v2/block17_12_mixed/concat]:193
	                 CONV_2D	         1553.622	    7.546	    7.494	  0.340%	 70.824%	     0.000	        1	[inception_resnet_v2/block17_12/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_12_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_12_conv/Conv2D]:194
	                     ADD	         1561.126	   29.910	   30.114	  1.368%	 72.192%	     0.000	        1	[inception_resnet_v2/block17_12_ac/Relu;inception_resnet_v2/block17_12/add]:195
	                 CONV_2D	         1591.251	    3.094	    3.126	  0.142%	 72.334%	     0.000	        1	[inception_resnet_v2/activation_218/Relu;inception_resnet_v2/batch_normalization_218/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_218/Conv2D]:196
	                 CONV_2D	         1594.387	    2.076	    2.116	  0.096%	 72.430%	     0.000	        1	[inception_resnet_v2/activation_219/Relu;inception_resnet_v2/batch_normalization_219/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_219/Conv2D]:197
	                 CONV_2D	         1596.511	    1.286	    1.327	  0.060%	 72.490%	     0.000	        1	[inception_resnet_v2/activation_220/Relu;inception_resnet_v2/batch_normalization_220/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_220/Conv2D]:198
	                 CONV_2D	         1597.847	    3.096	    3.128	  0.142%	 72.632%	     0.000	        1	[inception_resnet_v2/activation_221/Relu;inception_resnet_v2/batch_normalization_221/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_221/Conv2D]:199
	           CONCATENATION	         1600.984	    0.121	    0.114	  0.005%	 72.637%	     0.000	        1	[inception_resnet_v2/block17_13_mixed/concat]:200
	                 CONV_2D	         1601.105	    7.411	    7.482	  0.340%	 72.977%	     0.000	        1	[inception_resnet_v2/block17_13/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_13_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_13_conv/Conv2D]:201
	                     ADD	         1608.597	   29.910	   30.005	  1.363%	 74.340%	     0.000	        1	[inception_resnet_v2/block17_13_ac/Relu;inception_resnet_v2/block17_13/add]:202
	                 CONV_2D	         1638.613	    3.106	    3.097	  0.141%	 74.481%	     0.000	        1	[inception_resnet_v2/activation_222/Relu;inception_resnet_v2/batch_normalization_222/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_222/Conv2D]:203
	                 CONV_2D	         1641.720	    2.106	    2.103	  0.096%	 74.576%	     0.000	        1	[inception_resnet_v2/activation_223/Relu;inception_resnet_v2/batch_normalization_223/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_223/Conv2D]:204
	                 CONV_2D	         1643.831	    1.318	    1.320	  0.060%	 74.636%	     0.000	        1	[inception_resnet_v2/activation_224/Relu;inception_resnet_v2/batch_normalization_224/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_224/Conv2D]:205
	                 CONV_2D	         1645.159	    3.156	    3.086	  0.140%	 74.776%	     0.000	        1	[inception_resnet_v2/activation_225/Relu;inception_resnet_v2/batch_normalization_225/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_225/Conv2D]:206
	           CONCATENATION	         1648.254	    0.136	    0.110	  0.005%	 74.781%	     0.000	        1	[inception_resnet_v2/block17_14_mixed/concat]:207
	                 CONV_2D	         1648.371	    7.525	    7.452	  0.338%	 75.120%	     0.000	        1	[inception_resnet_v2/block17_14/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_14_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_14_conv/Conv2D]:208
	                     ADD	         1655.832	   30.046	   29.998	  1.363%	 76.482%	     0.000	        1	[inception_resnet_v2/block17_14_ac/Relu;inception_resnet_v2/block17_14/add]:209
	                 CONV_2D	         1685.842	    3.165	    3.124	  0.142%	 76.624%	     0.000	        1	[inception_resnet_v2/activation_226/Relu;inception_resnet_v2/batch_normalization_226/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_226/Conv2D]:210
	                 CONV_2D	         1688.976	    2.152	    2.109	  0.096%	 76.720%	     0.000	        1	[inception_resnet_v2/activation_227/Relu;inception_resnet_v2/batch_normalization_227/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_227/Conv2D]:211
	                 CONV_2D	         1691.093	    1.328	    1.319	  0.060%	 76.780%	     0.000	        1	[inception_resnet_v2/activation_228/Relu;inception_resnet_v2/batch_normalization_228/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_228/Conv2D]:212
	                 CONV_2D	         1692.420	    3.166	    3.098	  0.141%	 76.921%	     0.000	        1	[inception_resnet_v2/activation_229/Relu;inception_resnet_v2/batch_normalization_229/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_229/Conv2D]:213
	           CONCATENATION	         1695.528	    0.118	    0.123	  0.006%	 76.926%	     0.000	        1	[inception_resnet_v2/block17_15_mixed/concat]:214
	                 CONV_2D	         1695.658	    7.606	    7.497	  0.341%	 77.267%	     0.000	        1	[inception_resnet_v2/block17_15/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_15_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_15_conv/Conv2D]:215
	                     ADD	         1703.164	   30.230	   30.035	  1.364%	 78.631%	     0.000	        1	[inception_resnet_v2/block17_15_ac/Relu;inception_resnet_v2/block17_15/add]:216
	                 CONV_2D	         1733.210	    3.139	    3.098	  0.141%	 78.772%	     0.000	        1	[inception_resnet_v2/activation_230/Relu;inception_resnet_v2/batch_normalization_230/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_230/Conv2D]:217
	                 CONV_2D	         1736.318	    2.187	    2.102	  0.095%	 78.867%	     0.000	        1	[inception_resnet_v2/activation_231/Relu;inception_resnet_v2/batch_normalization_231/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_231/Conv2D]:218
	                 CONV_2D	         1738.430	    1.344	    1.317	  0.060%	 78.927%	     0.000	        1	[inception_resnet_v2/activation_232/Relu;inception_resnet_v2/batch_normalization_232/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_232/Conv2D]:219
	                 CONV_2D	         1739.755	    3.253	    3.102	  0.141%	 79.068%	     0.000	        1	[inception_resnet_v2/activation_233/Relu;inception_resnet_v2/batch_normalization_233/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_233/Conv2D]:220
	           CONCATENATION	         1742.866	    0.112	    0.111	  0.005%	 79.073%	     0.000	        1	[inception_resnet_v2/block17_16_mixed/concat]:221
	                 CONV_2D	         1742.984	    7.551	    7.472	  0.339%	 79.412%	     0.000	        1	[inception_resnet_v2/block17_16/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_16_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_16_conv/Conv2D]:222
	                     ADD	         1750.467	   30.238	   30.020	  1.364%	 80.776%	     0.000	        1	[inception_resnet_v2/block17_16_ac/Relu;inception_resnet_v2/block17_16/add]:223
	                 CONV_2D	         1780.497	    3.159	    3.116	  0.142%	 80.917%	     0.000	        1	[inception_resnet_v2/activation_234/Relu;inception_resnet_v2/batch_normalization_234/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_234/Conv2D]:224
	                 CONV_2D	         1783.623	    2.166	    2.096	  0.095%	 81.013%	     0.000	        1	[inception_resnet_v2/activation_235/Relu;inception_resnet_v2/batch_normalization_235/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_235/Conv2D]:225
	                 CONV_2D	         1785.727	    1.354	    1.328	  0.060%	 81.073%	     0.000	        1	[inception_resnet_v2/activation_236/Relu;inception_resnet_v2/batch_normalization_236/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_236/Conv2D]:226
	                 CONV_2D	         1787.063	    3.239	    3.193	  0.145%	 81.218%	     0.000	        1	[inception_resnet_v2/activation_237/Relu;inception_resnet_v2/batch_normalization_237/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_237/Conv2D]:227
	           CONCATENATION	         1790.268	    0.121	    0.124	  0.006%	 81.224%	     0.000	        1	[inception_resnet_v2/block17_17_mixed/concat]:228
	                 CONV_2D	         1790.399	    7.623	    7.483	  0.340%	 81.563%	     0.000	        1	[inception_resnet_v2/block17_17/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_17_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_17_conv/Conv2D]:229
	                     ADD	         1797.892	   30.219	   30.042	  1.365%	 82.928%	     0.000	        1	[inception_resnet_v2/block17_17_ac/Relu;inception_resnet_v2/block17_17/add]:230
	                 CONV_2D	         1827.945	    3.148	    3.097	  0.141%	 83.069%	     0.000	        1	[inception_resnet_v2/activation_238/Relu;inception_resnet_v2/batch_normalization_238/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_238/Conv2D]:231
	                 CONV_2D	         1831.050	    2.198	    2.208	  0.100%	 83.169%	     0.000	        1	[inception_resnet_v2/activation_239/Relu;inception_resnet_v2/batch_normalization_239/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_239/Conv2D]:232
	                 CONV_2D	         1833.267	    1.375	    1.321	  0.060%	 83.229%	     0.000	        1	[inception_resnet_v2/activation_240/Relu;inception_resnet_v2/batch_normalization_240/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_240/Conv2D]:233
	                 CONV_2D	         1834.596	    3.203	    3.102	  0.141%	 83.370%	     0.000	        1	[inception_resnet_v2/activation_241/Relu;inception_resnet_v2/batch_normalization_241/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_241/Conv2D]:234
	           CONCATENATION	         1837.708	    0.153	    0.117	  0.005%	 83.375%	     0.000	        1	[inception_resnet_v2/block17_18_mixed/concat]:235
	                 CONV_2D	         1837.832	    7.571	    7.468	  0.339%	 83.714%	     0.000	        1	[inception_resnet_v2/block17_18/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_18_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_18_conv/Conv2D]:236
	                     ADD	         1845.309	   33.831	   30.184	  1.371%	 85.085%	     0.000	        1	[inception_resnet_v2/block17_18_ac/Relu;inception_resnet_v2/block17_18/add]:237
	                 CONV_2D	         1875.504	    4.428	    3.179	  0.144%	 85.230%	     0.000	        1	[inception_resnet_v2/activation_242/Relu;inception_resnet_v2/batch_normalization_242/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_242/Conv2D]:238
	                 CONV_2D	         1878.693	    2.263	    2.111	  0.096%	 85.326%	     0.000	        1	[inception_resnet_v2/activation_243/Relu;inception_resnet_v2/batch_normalization_243/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_243/Conv2D]:239
	                 CONV_2D	         1880.812	    1.419	    1.335	  0.061%	 85.386%	     0.000	        1	[inception_resnet_v2/activation_244/Relu;inception_resnet_v2/batch_normalization_244/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_244/Conv2D]:240
	                 CONV_2D	         1882.155	    4.900	    3.186	  0.145%	 85.531%	     0.000	        1	[inception_resnet_v2/activation_245/Relu;inception_resnet_v2/batch_normalization_245/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_245/Conv2D]:241
	           CONCATENATION	         1885.350	    0.129	    0.115	  0.005%	 85.536%	     0.000	        1	[inception_resnet_v2/block17_19_mixed/concat]:242
	                 CONV_2D	         1885.472	    8.985	    7.526	  0.342%	 85.878%	     0.000	        1	[inception_resnet_v2/block17_19/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_19_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_19_conv/Conv2D]:243
	                     ADD	         1893.007	   34.872	   30.226	  1.373%	 87.251%	     0.000	        1	[inception_resnet_v2/block17_19_ac/Relu;inception_resnet_v2/block17_19/add]:244
	                 CONV_2D	         1923.245	    4.578	    3.177	  0.144%	 87.395%	     0.000	        1	[inception_resnet_v2/activation_246/Relu;inception_resnet_v2/batch_normalization_246/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_246/Conv2D]:245
	                 CONV_2D	         1926.431	    2.175	    2.101	  0.095%	 87.491%	     0.000	        1	[inception_resnet_v2/activation_247/Relu;inception_resnet_v2/batch_normalization_247/FusedBatchNormV3;inception_resnet_v2/batch_normalization_171/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_247/Conv2D]:246
	                 CONV_2D	         1928.540	    1.352	    1.322	  0.060%	 87.551%	     0.000	        1	[inception_resnet_v2/activation_248/Relu;inception_resnet_v2/batch_normalization_248/FusedBatchNormV3;inception_resnet_v2/batch_normalization_172/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_248/Conv2D]:247
	                 CONV_2D	         1929.894	    4.527	    3.151	  0.143%	 87.694%	     0.000	        1	[inception_resnet_v2/activation_249/Relu;inception_resnet_v2/batch_normalization_249/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_249/Conv2D]:248
	           CONCATENATION	         1933.054	    0.184	    0.112	  0.005%	 87.699%	     0.000	        1	[inception_resnet_v2/block17_20_mixed/concat]:249
	                 CONV_2D	         1933.173	    9.834	    7.583	  0.344%	 88.043%	     0.000	        1	[inception_resnet_v2/block17_20/mul;inception_resnet_v2/block17_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block17_20_conv/BiasAdd;inception_resnet_v2/block17_1/mul/y;inception_resnet_v2/block17_20_conv/Conv2D]:250
	                     ADD	         1940.767	   30.609	   30.052	  1.365%	 89.408%	     0.000	        1	[inception_resnet_v2/block17_20_ac/Relu;inception_resnet_v2/block17_20/add]:251
	                 CONV_2D	         1970.831	    4.134	    3.913	  0.178%	 89.586%	     0.000	        1	[inception_resnet_v2/activation_250/Relu;inception_resnet_v2/batch_normalization_250/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_250/Conv2D]:252
	                 CONV_2D	         1974.753	    1.774	    1.764	  0.080%	 89.666%	     0.000	        1	[inception_resnet_v2/activation_251/Relu;inception_resnet_v2/batch_normalization_251/FusedBatchNormV3;inception_resnet_v2/batch_normalization_166/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_251/Conv2D]:253
	                 CONV_2D	         1976.525	    3.852	    3.659	  0.166%	 89.832%	     0.000	        1	[inception_resnet_v2/activation_252/Relu;inception_resnet_v2/batch_normalization_252/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_252/Conv2D]:254
	                 CONV_2D	         1980.192	    1.641	    1.350	  0.061%	 89.894%	     0.000	        1	[inception_resnet_v2/activation_253/Relu;inception_resnet_v2/batch_normalization_253/FusedBatchNormV3;inception_resnet_v2/batch_normalization_253/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_253/Conv2D]:255
	                 CONV_2D	         1981.551	    3.817	    3.706	  0.168%	 90.062%	     0.000	        1	[inception_resnet_v2/activation_254/Relu;inception_resnet_v2/batch_normalization_254/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_254/Conv2D]:256
	                 CONV_2D	         1985.266	    6.577	    6.416	  0.291%	 90.354%	     0.000	        1	[inception_resnet_v2/activation_255/Relu;inception_resnet_v2/batch_normalization_255/FusedBatchNormV3;inception_resnet_v2/batch_normalization_253/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_255/Conv2D]:257
	                 CONV_2D	         1991.694	    1.607	    1.490	  0.068%	 90.421%	     0.000	        1	[inception_resnet_v2/activation_256/Relu;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3;inception_resnet_v2/batch_normalization_256/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_256/Conv2D]:258
	             MAX_POOL_2D	         1993.192	    1.939	    1.825	  0.083%	 90.504%	     0.000	        1	[inception_resnet_v2/max_pooling2d_7/MaxPool]:259
	           CONCATENATION	         1995.026	    0.220	    0.118	  0.005%	 90.509%	     0.000	        1	[inception_resnet_v2/mixed_7a/concat]:260
	                 CONV_2D	         1995.150	    0.994	    0.927	  0.042%	 90.552%	     0.000	        1	[inception_resnet_v2/activation_257/Relu;inception_resnet_v2/batch_normalization_257/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_257/Conv2D]:261
	                 CONV_2D	         1996.086	    1.018	    0.906	  0.041%	 90.593%	     0.000	        1	[inception_resnet_v2/activation_258/Relu;inception_resnet_v2/batch_normalization_258/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_258/Conv2D]:262
	                 CONV_2D	         1996.999	    0.488	    0.430	  0.020%	 90.612%	     0.000	        1	[inception_resnet_v2/activation_259/Relu;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_259/Conv2D]:263
	                 CONV_2D	         1997.437	    0.522	    0.456	  0.021%	 90.633%	     0.000	        1	[inception_resnet_v2/activation_260/Relu;inception_resnet_v2/batch_normalization_260/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_260/Conv2D]:264
	           CONCATENATION	         1997.901	    0.076	    0.041	  0.002%	 90.635%	     0.000	        1	[inception_resnet_v2/block8_1_mixed/concat]:265
	                 CONV_2D	         1997.949	    3.181	    3.081	  0.140%	 90.775%	     0.000	        1	[inception_resnet_v2/block8_1/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_1_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_1_conv/Conv2D]:266
	                     ADD	         2001.039	   13.079	   12.731	  0.578%	 91.353%	     0.000	        1	[inception_resnet_v2/block8_1_ac/Relu;inception_resnet_v2/block8_1/add]:267
	                 CONV_2D	         2013.780	    0.950	    0.928	  0.042%	 91.395%	     0.000	        1	[inception_resnet_v2/activation_261/Relu;inception_resnet_v2/batch_normalization_261/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_261/Conv2D]:268
	                 CONV_2D	         2014.715	    0.987	    0.906	  0.041%	 91.436%	     0.000	        1	[inception_resnet_v2/activation_262/Relu;inception_resnet_v2/batch_normalization_262/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_262/Conv2D]:269
	                 CONV_2D	         2015.629	    0.472	    0.427	  0.019%	 91.456%	     0.000	        1	[inception_resnet_v2/activation_263/Relu;inception_resnet_v2/batch_normalization_263/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_263/Conv2D]:270
	                 CONV_2D	         2016.064	    0.590	    0.455	  0.021%	 91.476%	     0.000	        1	[inception_resnet_v2/activation_264/Relu;inception_resnet_v2/batch_normalization_264/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_264/Conv2D]:271
	           CONCATENATION	         2016.526	    0.057	    0.042	  0.002%	 91.478%	     0.000	        1	[inception_resnet_v2/block8_2_mixed/concat]:272
	                 CONV_2D	         2016.574	    3.135	    3.076	  0.140%	 91.618%	     0.000	        1	[inception_resnet_v2/block8_2/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_2_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_2_conv/Conv2D]:273
	                     ADD	         2019.661	   13.045	   12.721	  0.578%	 92.196%	     0.000	        1	[inception_resnet_v2/block8_2_ac/Relu;inception_resnet_v2/block8_2/add]:274
	                 CONV_2D	         2032.391	    0.938	    0.917	  0.042%	 92.237%	     0.000	        1	[inception_resnet_v2/activation_265/Relu;inception_resnet_v2/batch_normalization_265/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_265/Conv2D]:275
	                 CONV_2D	         2033.315	    0.892	    0.896	  0.041%	 92.278%	     0.000	        1	[inception_resnet_v2/activation_266/Relu;inception_resnet_v2/batch_normalization_266/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_266/Conv2D]:276
	                 CONV_2D	         2034.219	    0.423	    0.426	  0.019%	 92.298%	     0.000	        1	[inception_resnet_v2/activation_267/Relu;inception_resnet_v2/batch_normalization_267/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_267/Conv2D]:277
	                 CONV_2D	         2034.653	    0.505	    0.458	  0.021%	 92.318%	     0.000	        1	[inception_resnet_v2/activation_268/Relu;inception_resnet_v2/batch_normalization_268/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_268/Conv2D]:278
	           CONCATENATION	         2035.119	    0.061	    0.039	  0.002%	 92.320%	     0.000	        1	[inception_resnet_v2/block8_3_mixed/concat]:279
	                 CONV_2D	         2035.164	    3.074	    3.057	  0.139%	 92.459%	     0.000	        1	[inception_resnet_v2/block8_3/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_3_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_3_conv/Conv2D]:280
	                     ADD	         2038.230	   12.761	   12.683	  0.576%	 93.035%	     0.000	        1	[inception_resnet_v2/block8_3_ac/Relu;inception_resnet_v2/block8_3/add]:281
	                 CONV_2D	         2050.921	    0.937	    0.924	  0.042%	 93.077%	     0.000	        1	[inception_resnet_v2/activation_269/Relu;inception_resnet_v2/batch_normalization_269/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_269/Conv2D]:282
	                 CONV_2D	         2051.854	    0.893	    0.893	  0.041%	 93.118%	     0.000	        1	[inception_resnet_v2/activation_270/Relu;inception_resnet_v2/batch_normalization_270/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_270/Conv2D]:283
	                 CONV_2D	         2052.754	    0.444	    0.423	  0.019%	 93.137%	     0.000	        1	[inception_resnet_v2/activation_271/Relu;inception_resnet_v2/batch_normalization_271/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_271/Conv2D]:284
	                 CONV_2D	         2053.184	    0.466	    0.447	  0.020%	 93.157%	     0.000	        1	[inception_resnet_v2/activation_272/Relu;inception_resnet_v2/batch_normalization_272/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_272/Conv2D]:285
	           CONCATENATION	         2053.638	    0.043	    0.040	  0.002%	 93.159%	     0.000	        1	[inception_resnet_v2/block8_4_mixed/concat]:286
	                 CONV_2D	         2053.684	    3.078	    3.061	  0.139%	 93.298%	     0.000	        1	[inception_resnet_v2/block8_4/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_4_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_4_conv/Conv2D]:287
	                     ADD	         2056.754	   12.732	   12.728	  0.578%	 93.876%	     0.000	        1	[inception_resnet_v2/block8_4_ac/Relu;inception_resnet_v2/block8_4/add]:288
	                 CONV_2D	         2069.492	    0.930	    0.952	  0.043%	 93.919%	     0.000	        1	[inception_resnet_v2/activation_273/Relu;inception_resnet_v2/batch_normalization_273/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_273/Conv2D]:289
	                 CONV_2D	         2070.452	    0.967	    0.909	  0.041%	 93.961%	     0.000	        1	[inception_resnet_v2/activation_274/Relu;inception_resnet_v2/batch_normalization_274/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_274/Conv2D]:290
	                 CONV_2D	         2071.368	    0.437	    0.428	  0.019%	 93.980%	     0.000	        1	[inception_resnet_v2/activation_275/Relu;inception_resnet_v2/batch_normalization_275/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_275/Conv2D]:291
	                 CONV_2D	         2071.803	    0.452	    0.450	  0.020%	 94.000%	     0.000	        1	[inception_resnet_v2/activation_276/Relu;inception_resnet_v2/batch_normalization_276/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_276/Conv2D]:292
	           CONCATENATION	         2072.261	    0.043	    0.041	  0.002%	 94.002%	     0.000	        1	[inception_resnet_v2/block8_5_mixed/concat]:293
	                 CONV_2D	         2072.308	    3.100	    3.064	  0.139%	 94.141%	     0.000	        1	[inception_resnet_v2/block8_5/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_5_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_5_conv/Conv2D]:294
	                     ADD	         2075.383	   12.788	   12.709	  0.577%	 94.719%	     0.000	        1	[inception_resnet_v2/block8_5_ac/Relu;inception_resnet_v2/block8_5/add]:295
	                 CONV_2D	         2088.101	    0.921	    0.912	  0.041%	 94.760%	     0.000	        1	[inception_resnet_v2/activation_277/Relu;inception_resnet_v2/batch_normalization_277/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_277/Conv2D]:296
	                 CONV_2D	         2089.021	    0.898	    0.901	  0.041%	 94.801%	     0.000	        1	[inception_resnet_v2/activation_278/Relu;inception_resnet_v2/batch_normalization_278/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_278/Conv2D]:297
	                 CONV_2D	         2089.930	    0.473	    0.431	  0.020%	 94.821%	     0.000	        1	[inception_resnet_v2/activation_279/Relu;inception_resnet_v2/batch_normalization_279/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_279/Conv2D]:298
	                 CONV_2D	         2090.368	    0.472	    0.456	  0.021%	 94.841%	     0.000	        1	[inception_resnet_v2/activation_280/Relu;inception_resnet_v2/batch_normalization_280/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_280/Conv2D]:299
	           CONCATENATION	         2090.833	    0.059	    0.041	  0.002%	 94.843%	     0.000	        1	[inception_resnet_v2/block8_6_mixed/concat]:300
	                 CONV_2D	         2090.880	    3.055	    3.061	  0.139%	 94.982%	     0.000	        1	[inception_resnet_v2/block8_6/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_6_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_6_conv/Conv2D]:301
	                     ADD	         2093.950	   12.856	   12.733	  0.578%	 95.561%	     0.000	        1	[inception_resnet_v2/block8_6_ac/Relu;inception_resnet_v2/block8_6/add]:302
	                 CONV_2D	         2106.695	    0.937	    0.942	  0.043%	 95.603%	     0.000	        1	[inception_resnet_v2/activation_281/Relu;inception_resnet_v2/batch_normalization_281/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_281/Conv2D]:303
	                 CONV_2D	         2107.644	    0.923	    0.905	  0.041%	 95.645%	     0.000	        1	[inception_resnet_v2/activation_282/Relu;inception_resnet_v2/batch_normalization_282/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_282/Conv2D]:304
	                 CONV_2D	         2108.557	    0.461	    0.426	  0.019%	 95.664%	     0.000	        1	[inception_resnet_v2/activation_283/Relu;inception_resnet_v2/batch_normalization_283/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_283/Conv2D]:305
	                 CONV_2D	         2108.990	    0.471	    0.450	  0.020%	 95.684%	     0.000	        1	[inception_resnet_v2/activation_284/Relu;inception_resnet_v2/batch_normalization_284/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_284/Conv2D]:306
	           CONCATENATION	         2109.448	    0.045	    0.042	  0.002%	 95.686%	     0.000	        1	[inception_resnet_v2/block8_7_mixed/concat]:307
	                 CONV_2D	         2109.497	    3.148	    3.075	  0.140%	 95.826%	     0.000	        1	[inception_resnet_v2/block8_7/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_7_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_7_conv/Conv2D]:308
	                     ADD	         2112.580	   12.739	   12.712	  0.577%	 96.403%	     0.000	        1	[inception_resnet_v2/block8_7_ac/Relu;inception_resnet_v2/block8_7/add]:309
	                 CONV_2D	         2125.302	    0.977	    0.934	  0.042%	 96.446%	     0.000	        1	[inception_resnet_v2/activation_285/Relu;inception_resnet_v2/batch_normalization_285/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_285/Conv2D]:310
	                 CONV_2D	         2126.244	    0.906	    0.902	  0.041%	 96.487%	     0.000	        1	[inception_resnet_v2/activation_286/Relu;inception_resnet_v2/batch_normalization_286/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_286/Conv2D]:311
	                 CONV_2D	         2127.154	    0.427	    0.435	  0.020%	 96.506%	     0.000	        1	[inception_resnet_v2/activation_287/Relu;inception_resnet_v2/batch_normalization_287/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_287/Conv2D]:312
	                 CONV_2D	         2127.596	    0.449	    0.450	  0.020%	 96.527%	     0.000	        1	[inception_resnet_v2/activation_288/Relu;inception_resnet_v2/batch_normalization_288/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_288/Conv2D]:313
	           CONCATENATION	         2128.053	    0.044	    0.046	  0.002%	 96.529%	     0.000	        1	[inception_resnet_v2/block8_8_mixed/concat]:314
	                 CONV_2D	         2128.106	    3.127	    3.080	  0.140%	 96.669%	     0.000	        1	[inception_resnet_v2/block8_8/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_8_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_8_conv/Conv2D]:315
	                     ADD	         2131.195	   12.798	   12.715	  0.578%	 97.246%	     0.000	        1	[inception_resnet_v2/block8_8_ac/Relu;inception_resnet_v2/block8_8/add]:316
	                 CONV_2D	         2143.919	    0.954	    0.931	  0.042%	 97.289%	     0.000	        1	[inception_resnet_v2/activation_289/Relu;inception_resnet_v2/batch_normalization_289/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_289/Conv2D]:317
	                 CONV_2D	         2144.858	    0.937	    0.902	  0.041%	 97.330%	     0.000	        1	[inception_resnet_v2/activation_290/Relu;inception_resnet_v2/batch_normalization_290/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_290/Conv2D]:318
	                 CONV_2D	         2145.767	    0.445	    0.422	  0.019%	 97.349%	     0.000	        1	[inception_resnet_v2/activation_291/Relu;inception_resnet_v2/batch_normalization_291/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_291/Conv2D]:319
	                 CONV_2D	         2146.197	    0.445	    0.450	  0.020%	 97.369%	     0.000	        1	[inception_resnet_v2/activation_292/Relu;inception_resnet_v2/batch_normalization_292/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_292/Conv2D]:320
	           CONCATENATION	         2146.654	    0.043	    0.042	  0.002%	 97.371%	     0.000	        1	[inception_resnet_v2/block8_9_mixed/concat]:321
	                 CONV_2D	         2146.703	    3.121	    3.070	  0.139%	 97.511%	     0.000	        1	[inception_resnet_v2/block8_9/mul;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_9_conv/BiasAdd;inception_resnet_v2/block8_1/mul/y;inception_resnet_v2/block8_9_conv/Conv2D]:322
	                     ADD	         2149.783	   12.734	   12.732	  0.578%	 98.089%	     0.000	        1	[inception_resnet_v2/block8_9_ac/Relu;inception_resnet_v2/block8_9/add]:323
	                 CONV_2D	         2162.524	    0.920	    0.922	  0.042%	 98.131%	     0.000	        1	[inception_resnet_v2/activation_293/Relu;inception_resnet_v2/batch_normalization_293/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_293/Conv2D]:324
	                 CONV_2D	         2163.454	    0.896	    0.902	  0.041%	 98.172%	     0.000	        1	[inception_resnet_v2/activation_294/Relu;inception_resnet_v2/batch_normalization_294/FusedBatchNormV3;inception_resnet_v2/batch_normalization_170/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_294/Conv2D]:325
	                 CONV_2D	         2164.363	    0.429	    0.422	  0.019%	 98.191%	     0.000	        1	[inception_resnet_v2/activation_295/Relu;inception_resnet_v2/batch_normalization_295/FusedBatchNormV3;inception_resnet_v2/batch_normalization_259/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_295/Conv2D]:326
	                 CONV_2D	         2164.792	    0.438	    0.450	  0.020%	 98.211%	     0.000	        1	[inception_resnet_v2/activation_296/Relu;inception_resnet_v2/batch_normalization_296/FusedBatchNormV3;inception_resnet_v2/batch_normalization_167/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_296/Conv2D]:327
	           CONCATENATION	         2165.250	    0.050	    0.042	  0.002%	 98.213%	     0.000	        1	[inception_resnet_v2/block8_10_mixed/concat]:328
	                 CONV_2D	         2165.299	    3.106	    3.056	  0.139%	 98.352%	     0.000	        1	[inception_resnet_v2/block8_10_conv/BiasAdd;inception_resnet_v2/block8_10_conv/BiasAdd/ReadVariableOp;inception_resnet_v2/block8_10_conv/Conv2D]:329
	                     ADD	         2168.363	   12.798	   12.702	  0.577%	 98.929%	     0.000	        1	[inception_resnet_v2/block8_10/add]:330
	                 CONV_2D	         2181.074	    6.293	    6.236	  0.283%	 99.212%	     0.000	        1	[inception_resnet_v2/conv_7b_ac/Relu;inception_resnet_v2/conv_7b_bn/FusedBatchNormV3;inception_resnet_v2/conv_7b_bn/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv_7b/Conv2D]:331
	                    MEAN	         2187.320	   16.831	   16.815	  0.764%	 99.976%	     0.000	        1	[inception_resnet_v2/avg_pool/Mean]:332
	         FULLY_CONNECTED	         2204.145	    0.465	    0.433	  0.020%	 99.996%	     0.000	        1	[inception_resnet_v2/predictions/MatMul;inception_resnet_v2/predictions/BiasAdd]:333
	                 SOFTMAX	         2204.587	    0.085	    0.092	  0.004%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:334

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          115.185	   65.694	   65.960	  2.996%	  2.996%	     0.000	        1	[inception_resnet_v2/activation_96/Relu;inception_resnet_v2/batch_normalization_96/FusedBatchNormV3;inception_resnet_v2/batch_normalization_101/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_96/Conv2D]:2
	                 CONV_2D	            0.060	   63.806	   64.528	  2.931%	  5.927%	     0.000	        1	[inception_resnet_v2/activation_94/Relu;inception_resnet_v2/batch_normalization_94/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_94/Conv2D]:0
	                 CONV_2D	           64.600	   50.549	   50.573	  2.297%	  8.224%	     0.000	        1	[inception_resnet_v2/activation_95/Relu;inception_resnet_v2/batch_normalization_95/FusedBatchNormV3;inception_resnet_v2/batch_normalization_106/FusedBatchNormV3/ReadVariableOp;inception_resnet_v2/conv2d_95/Conv2D]:1
	         AVERAGE_POOL_2D	          242.904	   45.209	   45.414	  2.063%	 10.287%	     0.000	        1	[inception_resnet_v2/average_pooling2d_9/AvgPool]:7
	                     ADD	          607.458	   37.306	   37.590	  1.707%	 11.994%	     0.000	        1	[inception_resnet_v2/block35_5_ac/Relu;inception_resnet_v2/block35_5/add]:60
	                     ADD	          803.426	   37.240	   37.486	  1.703%	 13.697%	     0.000	        1	[inception_resnet_v2/block35_8_ac/Relu;inception_resnet_v2/block35_8/add]:87
	                     ADD	          346.521	   37.244	   37.483	  1.703%	 15.400%	     0.000	        1	[inception_resnet_v2/block35_1_ac/Relu;inception_resnet_v2/block35_1/add]:24
	                     ADD	          737.811	   37.282	   37.476	  1.702%	 17.102%	     0.000	        1	[inception_resnet_v2/block35_7_ac/Relu;inception_resnet_v2/block35_7/add]:78
	                     ADD	          411.779	   37.258	   37.467	  1.702%	 18.804%	     0.000	        1	[inception_resnet_v2/block35_2_ac/Relu;inception_resnet_v2/block35_2/add]:33
	                     ADD	          477.116	   37.214	   37.435	  1.700%	 20.504%	     0.000	        1	[inception_resnet_v2/block35_3_ac/Relu;inception_resnet_v2/block35_3/add]:42

Number of nodes executed: 335
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                     ADD	       40	  1102.417	    50.078%	    50.078%	     0.000	       40
	                 CONV_2D	      244	  1011.452	    45.946%	    96.024%	     0.000	      244
	         AVERAGE_POOL_2D	        1	    45.414	     2.063%	    98.087%	     0.000	        1
	             MAX_POOL_2D	        4	    18.862	     0.857%	    98.943%	     0.000	        4
	                    MEAN	        1	    16.815	     0.764%	    99.707%	     0.000	        1
	           CONCATENATION	       43	     5.921	     0.269%	    99.976%	     0.000	       43
	         FULLY_CONNECTED	        1	     0.433	     0.020%	    99.996%	     0.000	        1
	                 SOFTMAX	        1	     0.092	     0.004%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=2216685 curr=2195949 min=2193268 max=2216685 avg=2.20157e+06 std=5364
Memory (bytes): count=0
335 nodes observed



[ perf record: Woken up 188 times to write data ]
[ perf record: Captured and wrote 47.271 MB /tmp/data.record (265319 samples) ]

67.665

