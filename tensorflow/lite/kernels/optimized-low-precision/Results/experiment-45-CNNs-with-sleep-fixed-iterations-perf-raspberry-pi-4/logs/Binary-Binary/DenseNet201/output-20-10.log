STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/DenseNet201.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 32)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (3136, 64, ), and Output shape (3136, 128, ), and the ID is 1
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
, and the ID is 2
	Allocating LowPrecision Activations Tensors with Shape of (3136, 144)
Applying Conv Low-Precision for Kernel shape (128, 96, ), Input shape (3136, 96, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
(3136, 128, ), and the ID is 3
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 4
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 144)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (3136, 128, ), and Output shape (3136, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
5
Applying Conv Low-Precision for Kernel shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
(32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 6
	Allocating LowPrecision Activations Tensors with Shape of (3136, 144)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (3136, 160, ), and Output shape (3136, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
7
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 8
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 144)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (3136, 192, ), and Output shape (3136, 128, ), and the ID is 9	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)

	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
, and the ID is 10
	Allocating LowPrecision Activations Tensors with Shape of (3136, 144)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (3136, 224, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
, and Output shape (3136, 128, ), and the ID is 11
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 12
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 144)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (3136, 256, ), and Output shape (3136, 128, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
, and the ID is 15
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (784, 160, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
	Allocating LowPrecision Activations Tensors with Shape of (784, 32)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (784, 192, ), and Output shape (784, 128, ), and the ID is 18
	Allocating LowPrecision Activations Tensors with Shape of (784, 32)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (784, 224, ), and Output shape (784, 128, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
	Allocating LowPrecision Activations Tensors with Shape of (784, 32)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 22	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)

	Allocating LowPrecision Activations Tensors with Shape of (784, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 23
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (784, 288, ), and Output shape (784, 128, ), and the ID is 24
	Allocating LowPrecision Activations Tensors with Shape of (784, 48)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 25
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (784, 320, ), and Output shape (784, 128, ), and the ID is 26
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (784, 48)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 27
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (784, 352, ), and Output shape (784, 128, ), and the ID is 28
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (784, 48)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (784, 384, ), and Output shape (784, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
30
	Allocating LowPrecision Activations Tensors with Shape of (784, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 31
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (784, 416, ), and Output shape (784, 128, ), and the ID is 32
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 33
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (784, 448, ), and Output shape (784, 128, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 35
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (784, 480, ), and Output shape (784, 128, ), and the ID is 36
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 37
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (784, 512, ), and Output shape (784, 256, ), and the ID is 38
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (196, 256, ), and Output shape (196, 128, ), and the ID is 39
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
40
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (196, 288, ), and Output shape (196, 128, ), and the ID is 41
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (196, 48)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 42
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (196, 320, ), and Output shape (196, 128, ), and the ID is 43
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (196, 48)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 44
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (196, 352, ), and Output shape (196, 128, ), and the ID is 45
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (196, 48)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 46
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (196, 384, ), and Output shape (196, 128, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (196, 48)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 48
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (196, 416, ), and Output shape (196, 128, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (196, 64)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 50
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (196, 448, ), and Output shape (196, 128, ), and the ID is 51
	Allocating LowPrecision Activations Tensors with Shape of (196, 64)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 52
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (196, 480, ), and Output shape (196, 128, ), and the ID is 53
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (196, 64)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 54
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (196, 512, ), and Output shape (196, 128, ), and the ID is 55	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)

	Allocating LowPrecision Activations Tensors with Shape of (196, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 56
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 544, ), Input shape (196, 544, ), and Output shape (196, 128, ), and the ID is 57
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
	Allocating LowPrecision Activations Tensors with Shape of (196, 80)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 58
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (196, 576, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
59
	Allocating LowPrecision Activations Tensors with Shape of (196, 80)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
60
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 608, ), Input shape (196, 608, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
61
	Allocating LowPrecision Activations Tensors with Shape of (196, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
	Allocating LowPrecision Activations Tensors with Shape of (196, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 62
Applying Conv Low-Precision for Kernel shape (128, 640, ), Input shape (196, 640, ), and Output shape (196, 128, ), and the ID is 63
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 64
Applying Conv Low-Precision for Kernel shape (128, 672, ), Input shape (196, 672, ), and Output shape (196, 128, ), and the ID is 65
	Allocating LowPrecision Activations Tensors with Shape of (196, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 66
Applying Conv Low-Precision for Kernel shape (128, 704, ), Input shape (196, 704, ), and Output shape (196, 128, ), and the ID is 67
	Allocating LowPrecision Activations Tensors with Shape of (196, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 68
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 736, ), Input shape (196, 736, ), and Output shape (196, 128, ), and the ID is 69
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
	Allocating LowPrecision Activations Tensors with Shape of (196, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 70
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
Applying Conv Low-Precision for Kernel shape (128, 768, ), Input shape (196, 768, ), and Output shape (196, 128, ), and the ID is 71
	Allocating LowPrecision Activations Tensors with Shape of (196, 96)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 72
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 800, ), Input shape (196, 800, ), and Output shape (196, 128, ), and the ID is 73
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
	Allocating LowPrecision Activations Tensors with Shape of (196, 112)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 74
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 832, ), Input shape (196, 832, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
(196, 128, ), and the ID is 75
	Allocating LowPrecision Activations Tensors with Shape of (196, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 76
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 864, ), Input shape (196, 864, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
(196, 128, ), and the ID is 77
	Allocating LowPrecision Activations Tensors with Shape of (196, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
, and the ID is 78
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (196, 896, ), and Output shape (196, 128, ), and the ID is 79
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
	Allocating LowPrecision Activations Tensors with Shape of (196, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 80
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (196, 928, ), and Output shape (196, 128, ), and the ID is 81
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 82
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (196, 960, ), and Output shape (196, 128, ), and the ID is 83	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)

	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 84
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (196, 992, ), and Output shape (196, 128, ), and the ID is 85
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 86
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (196, 1024, ), and Output shape (196, 128, ), and the ID is 87
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 88
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (196, 1056, ), and Output shape (196, 128, ), and the ID is 89
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 90
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (196, 1088, ), and Output shape (196, 128, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 92
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (196, 1120, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
, and the ID is 93
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (196, 1152, ), and Output shape (196, 128, ), and the ID is 95
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
(196, 32, ), and the ID is 96
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (196, 1184, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
(196, 128, ), and the ID is 97
	Allocating LowPrecision Activations Tensors with Shape of (196, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 98
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (196, 1216, ), and Output shape (196, 128, ), and the ID is 99
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (196, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 100
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (196, 1248, ), and Output shape (196, 128, ), and the ID is 101
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (196, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 102
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (196, 1280, ), and Output shape (196, 128, ), and the ID is 103
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (196, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
(196, 128, ), and Output shape (196, 32, ), and the ID is 104
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (196, 1312, ), and Output shape (196, 128, ), and the ID is 105
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (196, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 106
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (196, 1344, ), and Output shape (196, 128, ), and the ID is 107
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (196, 176)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 108
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (196, 1376, ), and Output shape (196, 128, ), and the ID is 109
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (196, 176)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 110
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (196, 1408, ), and Output shape (196, 128, ), and the ID is 111
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (196, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
(196, 32, ), and the ID is 112
Applying Conv Low-Precision for Kernel shape (128, 1440, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
, Input shape (196, 1440, ), and Output shape (196, 128, ), and the ID is 113
	Allocating LowPrecision Activations Tensors with Shape of (196, 192)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 114
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (196, 1472, ), and Output shape (196, 128, ), and the ID is 115
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (196, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (196, 1504, ), and Output shape (196, 128, ), and the ID is 117	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)

	Allocating LowPrecision Activations Tensors with Shape of (196, 192)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 118
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (196, 1536, ), and Output shape (196, 128, ), and the ID is 119
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (196, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
(196, 32, ), and the ID is 120
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (196, 1568, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
121
	Allocating LowPrecision Activations Tensors with Shape of (196, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 122
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (196, 1600, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
, and Output shape (196, 128, ), and the ID is 123
	Allocating LowPrecision Activations Tensors with Shape of (196, 208)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 124
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (196, 1632, ), and Output shape (196, 128, ), and the ID is 125
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (196, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 126
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (196, 1664, ), and Output shape (196, 128, ), and the ID is 127
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (196, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 128
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (196, 1696, ), and Output shape (196, 128, ), and the ID is 129
	Allocating LowPrecision Activations Tensors with Shape of (196, 224)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 130
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (196, 1728, ), and Output shape (196, 128, ), and the ID is 131
	Allocating LowPrecision Activations Tensors with Shape of (196, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 132
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (196, 1760, ), and Output shape (196, 128, ), and the ID is 133
	Allocating LowPrecision Activations Tensors with Shape of (196, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 134
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (896, 1792, ), Input shape (196, 1792, ), and Output shape (196, 896, ), and the ID is 135
	Allocating LowPrecision Weight Tensors with Shape of (896, 224)
	Allocating LowPrecision Activations Tensors with Shape of (196, 224)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (49, 896, ), and Output shape (49, 128, ), and the ID is 136	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)

	Allocating LowPrecision Activations Tensors with Shape of (52, 112)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 137
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (49, 928, ), and Output shape (49, 128, ), and the ID is 138
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (52, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (49, 960, ), and Output shape (49, 128, ), and the ID is 140
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (52, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 141
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (49, 992, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
, and the ID is 142
	Allocating LowPrecision Activations Tensors with Shape of (52, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 143
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (49, 1024, ), and Output shape (49, 128, ), and the ID is 144
	Allocating LowPrecision Activations Tensors with Shape of (52, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 145
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (49, 1056, ), and Output shape (49, 128, ), and the ID is 146
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 147
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (49, 1088, ), and Output shape (49, 128, ), and the ID is 148
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 149
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (49, 1120, ), and Output shape (49, 128, ), and the ID is 150
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 151
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (49, 1152, ), and Output shape (49, 128, ), and the ID is 152
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 153
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (49, 1184, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
, and the ID is 154
	Allocating LowPrecision Activations Tensors with Shape of (52, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 155
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (49, 1216, ), and Output shape (49, 128, ), and the ID is 156
	Allocating LowPrecision Activations Tensors with Shape of (52, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 157	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)

	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (49, 1248, ), and Output shape (49, 128, ), and the ID is 158
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (52, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 159
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (49, 1280, ), and Output shape (49, 128, ), and the ID is 160
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (52, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 161
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (49, 1312, ), and Output shape (49, 128, ), and the ID is 162
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (52, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 163
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (49, 1344, ), and Output shape (49, 128, ), and the ID is 164
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (52, 176)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 165
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (49, 1376, ), and Output shape (49, 128, ), and the ID is 166	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)

	Allocating LowPrecision Activations Tensors with Shape of (52, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 167
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (49, 1408, ), and Output shape (49, 128, ), and the ID is 168
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (52, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 169
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (49, 1440, ), and Output shape (49, 128, ), and the ID is 170
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (52, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 171
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (49, 1472, ), and Output shape (49, 128, ), and the ID is 172
	Allocating LowPrecision Activations Tensors with Shape of (52, 192)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 173
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (49, 1504, ), and Output shape (49, 128, ), and the ID is 174
	Allocating LowPrecision Activations Tensors with Shape of (52, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 175
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (49, 1536, ), and Output shape (49, 128, ), and the ID is 176
	Allocating LowPrecision Activations Tensors with Shape of (52, 192)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 177
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (49, 1568, ), and Output shape (49, 128, ), and the ID is 178
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (52, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 179
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (49, 1600, ), and Output shape (49, 128, ), and the ID is 180
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (52, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 181
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (49, 1632, ), and Output shape (49, 128, ), and the ID is 182
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (52, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 183
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (49, 1664, ), and Output shape (49, 128, ), and the ID is 184
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (52, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 185
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (49, 1696, ), and Output shape (49, 128, ), and the ID is 186
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (52, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 187
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (49, 1728, ), and Output shape (49, 128, ), and the ID is 188
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (52, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 189
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (49, 1760, ), and Output shape (49, 128, ), and the ID is 190
	Allocating LowPrecision Activations Tensors with Shape of (52, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 191
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1792, ), Input shape (49, 1792, ), and Output shape (49, 128, ), and the ID is 192
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (52, 224)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 193
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1824, ), Input shape (49, 1824, ), and Output shape (49, 128, ), and the ID is 194
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
	Allocating LowPrecision Activations Tensors with Shape of (52, 240)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 195
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
	Allocating LowPrecision Activations Tensors with Shape of (52, 240)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (128, 1856, ), Input shape (49, 1856, ), and Output shape (49, 128, ), and the ID is 196
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 197
Applying Conv Low-Precision for Kernel shape (128, 1888, ), Input shape (49, 1888, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
, and Output shape (49, 128, ), and the ID is 198
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
	Allocating LowPrecision Activations Tensors with Shape of (52, 240)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 144)
	Allocating LowPrecision Activations Tensors with Shape of (52, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 199
Applying Low-Precision for shape (1000, 1920, ) and Input shape (1, 1920, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 240)
	Transformed Activation Shape From: (1, 1920) To: (1, 240)
The input model file size (MB): 20.5199
Initialized session in 109.393ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=3677488 curr=3645528 min=3645105 max=3677488 avg=3.65552e+06 std=10200

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=3660328 curr=3666580 min=3654210 max=3684270 avg=3.66238e+06 std=6511

Inference timings in us: Init: 109393, First inference: 3677488, Warmup (avg): 3.65552e+06, Inference (avg): 3.66238e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=26.0625 overall=146.957
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   87.047	   87.047	100.000%	100.000%	 12784.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   87.047	   87.047	100.000%	100.000%	 12784.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	    87.047	   100.000%	   100.000%	 12784.000	        1

Timings (microseconds): count=1 curr=87047
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.027	    3.685	    3.711	  0.101%	  0.101%	     0.000	        1	[densenet201/zero_padding2d/Pad]:0
	                 CONV_2D	            3.748	   44.185	   44.069	  1.205%	  1.306%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                     PAD	           47.830	   18.174	   18.300	  0.500%	  1.807%	     0.000	        1	[densenet201/zero_padding2d_1/Pad]:2
	             MAX_POOL_2D	           66.142	    5.249	    5.229	  0.143%	  1.950%	     0.000	        1	[densenet201/pool1/MaxPool]:3
	                     MUL	           71.381	   14.662	   14.731	  0.403%	  2.352%	     0.000	        1	[densenet201/conv2_block1_0_bn/FusedBatchNormV31]:4
	                     ADD	           86.121	   19.551	   19.718	  0.539%	  2.891%	     0.000	        1	[densenet201/conv2_block1_0_relu/Relu;densenet201/conv2_block1_0_bn/FusedBatchNormV3]:5
	                 CONV_2D	          105.849	   14.171	   14.284	  0.391%	  3.282%	     0.000	        1	[densenet201/conv2_block1_1_relu/Relu;densenet201/conv2_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block1_1_conv/Conv2D]:6
	                 CONV_2D	          120.145	   14.603	   14.136	  0.386%	  3.668%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	           CONCATENATION	          134.292	    0.349	    0.383	  0.010%	  3.679%	     0.000	        1	[densenet201/conv2_block1_concat/concat]:8
	                     MUL	          134.685	   21.649	   21.736	  0.594%	  4.273%	     0.000	        1	[densenet201/conv2_block2_0_bn/FusedBatchNormV31]:9
	                     ADD	          156.431	   29.199	   29.321	  0.802%	  5.075%	     0.000	        1	[densenet201/conv2_block2_0_relu/Relu;densenet201/conv2_block2_0_bn/FusedBatchNormV3]:10
	                 CONV_2D	          185.763	   14.094	   14.246	  0.389%	  5.464%	     0.000	        1	[densenet201/conv2_block2_1_relu/Relu;densenet201/conv2_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block2_1_conv/Conv2D]:11
	                 CONV_2D	          200.019	   16.235	   15.909	  0.435%	  5.899%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	           CONCATENATION	          215.940	    0.514	    0.470	  0.013%	  5.912%	     0.000	        1	[densenet201/conv2_block2_concat/concat]:13
	                     MUL	          216.419	   28.614	   28.773	  0.787%	  6.698%	     0.000	        1	[densenet201/conv2_block3_0_bn/FusedBatchNormV3]:14
	                     ADD	          245.202	   38.693	   38.972	  1.065%	  7.764%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                 CONV_2D	          284.185	   14.113	   14.274	  0.390%	  8.154%	     0.000	        1	[densenet201/conv2_block3_1_relu/Relu;densenet201/conv2_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block3_1_conv/Conv2D]:16
	                 CONV_2D	          298.471	   14.307	   14.030	  0.384%	  8.538%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	           CONCATENATION	          312.513	    0.646	    0.560	  0.015%	  8.553%	     0.000	        1	[densenet201/conv2_block3_concat/concat]:18
	                     MUL	          313.086	   35.716	   35.919	  0.982%	  9.535%	     0.000	        1	[densenet201/conv2_block4_0_bn/FusedBatchNormV3]:19
	                     ADD	          349.017	   48.299	   48.649	  1.330%	 10.865%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                 CONV_2D	          397.682	   13.486	   13.621	  0.372%	 11.237%	     0.000	        1	[densenet201/conv2_block4_1_relu/Relu;densenet201/conv2_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block4_1_conv/Conv2D]:21
	                 CONV_2D	          411.314	   14.030	   14.284	  0.390%	 11.628%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	           CONCATENATION	          425.610	    0.595	    0.710	  0.019%	 11.647%	     0.000	        1	[densenet201/conv2_block4_concat/concat]:23
	                     MUL	          426.332	   42.605	   42.952	  1.174%	 12.821%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	          469.296	   57.924	   58.226	  1.592%	 14.413%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                 CONV_2D	          527.533	   13.444	   13.576	  0.371%	 14.784%	     0.000	        1	[densenet201/conv2_block5_1_relu/Relu;densenet201/conv2_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block5_1_conv/Conv2D]:26
	                 CONV_2D	          541.120	   14.627	   14.307	  0.391%	 15.175%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	           CONCATENATION	          555.439	    0.787	    0.805	  0.022%	 15.197%	     0.000	        1	[densenet201/conv2_block5_concat/concat]:28
	                     MUL	          556.255	   50.054	   49.997	  1.367%	 16.564%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	          606.264	   67.555	   67.822	  1.854%	 18.418%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                 CONV_2D	          674.097	   13.479	   13.569	  0.371%	 18.789%	     0.000	        1	[densenet201/conv2_block6_1_relu/Relu;densenet201/conv2_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block6_1_conv/Conv2D]:31
	                 CONV_2D	          687.677	   14.687	   14.361	  0.393%	 19.182%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	           CONCATENATION	          702.052	    0.883	    0.960	  0.026%	 19.208%	     0.000	        1	[densenet201/conv2_block6_concat/concat]:33
	                     MUL	          703.023	   57.097	   56.922	  1.556%	 20.764%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     ADD	          759.956	   77.702	   77.022	  2.106%	 22.870%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                 CONV_2D	          836.989	   13.789	   13.632	  0.373%	 23.243%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	         AVERAGE_POOL_2D	          850.637	   11.165	   11.085	  0.303%	 23.546%	     0.000	        1	[densenet201/pool2_pool/AvgPool]:37
	                     MUL	          861.731	    7.221	    7.199	  0.197%	 23.742%	     0.000	        1	[densenet201/conv3_block1_0_bn/FusedBatchNormV3]:38
	                     ADD	          868.938	    9.878	    9.731	  0.266%	 24.008%	     0.000	        1	[densenet201/conv3_block1_0_relu/Relu;densenet201/conv3_block1_0_bn/FusedBatchNormV3]:39
	                 CONV_2D	          878.678	    3.724	    3.600	  0.098%	 24.107%	     0.000	        1	[densenet201/conv3_block1_1_relu/Relu;densenet201/conv3_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block1_1_conv/Conv2D]:40
	                 CONV_2D	          882.287	    3.429	    3.159	  0.086%	 24.193%	     0.000	        1	[densenet201/conv3_block1_2_conv/Conv2D1]:41
	           CONCATENATION	          885.458	    0.170	    0.222	  0.006%	 24.199%	     0.000	        1	[densenet201/conv3_block1_concat/concat]:42
	                     MUL	          885.687	    9.067	    8.960	  0.245%	 24.444%	     0.000	        1	[densenet201/conv3_block2_0_bn/FusedBatchNormV31]:43
	                     ADD	          894.656	   12.195	   12.103	  0.331%	 24.775%	     0.000	        1	[densenet201/conv3_block2_0_relu/Relu;densenet201/conv3_block2_0_bn/FusedBatchNormV3]:44
	                 CONV_2D	          906.768	    3.501	    3.424	  0.094%	 24.869%	     0.000	        1	[densenet201/conv3_block2_1_relu/Relu;densenet201/conv3_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block2_1_conv/Conv2D]:45
	                 CONV_2D	          910.202	    3.269	    3.159	  0.086%	 24.955%	     0.000	        1	[densenet201/conv3_block2_2_conv/Conv2D1]:46
	           CONCATENATION	          913.374	    0.178	    0.256	  0.007%	 24.962%	     0.000	        1	[densenet201/conv3_block2_concat/concat]:47
	                     MUL	          913.637	   10.828	   10.702	  0.293%	 25.255%	     0.000	        1	[densenet201/conv3_block3_0_bn/FusedBatchNormV31]:48
	                     ADD	          924.348	   14.613	   14.513	  0.397%	 25.651%	     0.000	        1	[densenet201/conv3_block3_0_relu/Relu;densenet201/conv3_block3_0_bn/FusedBatchNormV3]:49
	                 CONV_2D	          938.870	    3.469	    3.432	  0.094%	 25.745%	     0.000	        1	[densenet201/conv3_block3_1_relu/Relu;densenet201/conv3_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block3_1_conv/Conv2D]:50
	                 CONV_2D	          942.310	    3.233	    3.161	  0.086%	 25.832%	     0.000	        1	[densenet201/conv3_block3_2_conv/Conv2D1]:51
	           CONCATENATION	          945.483	    0.164	    0.284	  0.008%	 25.839%	     0.000	        1	[densenet201/conv3_block3_concat/concat]:52
	                     MUL	          945.774	   12.578	   12.471	  0.341%	 26.180%	     0.000	        1	[densenet201/conv3_block4_0_bn/FusedBatchNormV31]:53
	                     ADD	          958.254	   17.044	   16.922	  0.463%	 26.643%	     0.000	        1	[densenet201/conv3_block4_0_relu/Relu;densenet201/conv3_block4_0_bn/FusedBatchNormV3]:54
	                 CONV_2D	          975.185	    3.452	    3.415	  0.093%	 26.736%	     0.000	        1	[densenet201/conv3_block4_1_relu/Relu;densenet201/conv3_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block4_1_conv/Conv2D]:55
	                 CONV_2D	          978.609	    3.197	    3.132	  0.086%	 26.822%	     0.000	        1	[densenet201/conv3_block4_2_conv/Conv2D1]:56
	           CONCATENATION	          981.753	    0.218	    0.312	  0.009%	 26.830%	     0.000	        1	[densenet201/conv3_block4_concat/concat]:57
	                     MUL	          982.073	   14.413	   14.279	  0.390%	 27.221%	     0.000	        1	[densenet201/conv3_block5_0_bn/FusedBatchNormV3]:58
	                     ADD	          996.361	   19.514	   19.301	  0.528%	 27.748%	     0.000	        1	[densenet201/conv3_block5_0_relu/Relu;densenet201/conv3_block5_0_bn/FusedBatchNormV3]:59
	                 CONV_2D	         1015.672	    3.506	    3.448	  0.094%	 27.843%	     0.000	        1	[densenet201/conv3_block5_1_relu/Relu;densenet201/conv3_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block5_1_conv/Conv2D]:60
	                 CONV_2D	         1019.129	    3.301	    3.149	  0.086%	 27.929%	     0.000	        1	[densenet201/conv3_block5_2_conv/Conv2D1]:61
	           CONCATENATION	         1022.291	    0.264	    0.355	  0.010%	 27.939%	     0.000	        1	[densenet201/conv3_block5_concat/concat]:62
	                     MUL	         1022.654	   16.128	   16.004	  0.438%	 28.376%	     0.000	        1	[densenet201/conv3_block6_0_bn/FusedBatchNormV3]:63
	                     ADD	         1038.668	   21.802	   21.605	  0.591%	 28.967%	     0.000	        1	[densenet201/conv3_block6_0_relu/Relu;densenet201/conv3_block6_0_bn/FusedBatchNormV3]:64
	                 CONV_2D	         1060.283	    3.317	    3.270	  0.089%	 29.056%	     0.000	        1	[densenet201/conv3_block6_1_relu/Relu;densenet201/conv3_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block6_1_conv/Conv2D]:65
	                 CONV_2D	         1063.562	    3.201	    3.139	  0.086%	 29.142%	     0.000	        1	[densenet201/conv3_block6_2_conv/Conv2D1]:66
	           CONCATENATION	         1066.713	    0.330	    0.390	  0.011%	 29.153%	     0.000	        1	[densenet201/conv3_block6_concat/concat]:67
	                     MUL	         1067.111	   17.822	   17.735	  0.485%	 29.637%	     0.000	        1	[densenet201/conv3_block7_0_bn/FusedBatchNormV3]:68
	                     ADD	         1084.855	   24.080	   23.963	  0.655%	 30.292%	     0.000	        1	[densenet201/conv3_block7_0_relu/Relu;densenet201/conv3_block7_0_bn/FusedBatchNormV3]:69
	                 CONV_2D	         1108.829	    3.266	    3.251	  0.089%	 30.381%	     0.000	        1	[densenet201/conv3_block7_1_relu/Relu;densenet201/conv3_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block7_1_conv/Conv2D]:70
	                 CONV_2D	         1112.088	    3.204	    3.132	  0.086%	 30.467%	     0.000	        1	[densenet201/conv3_block7_2_conv/Conv2D1]:71
	           CONCATENATION	         1115.231	    0.252	    0.435	  0.012%	 30.479%	     0.000	        1	[densenet201/conv3_block7_concat/concat]:72
	                     MUL	         1115.675	   19.653	   19.511	  0.533%	 31.012%	     0.000	        1	[densenet201/conv3_block8_0_bn/FusedBatchNormV3]:73
	                     ADD	         1135.198	   26.442	   26.365	  0.721%	 31.733%	     0.000	        1	[densenet201/conv3_block8_0_relu/Relu;densenet201/conv3_block8_0_bn/FusedBatchNormV3]:74
	                 CONV_2D	         1161.574	    3.309	    3.259	  0.089%	 31.822%	     0.000	        1	[densenet201/conv3_block8_1_relu/Relu;densenet201/conv3_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block8_1_conv/Conv2D]:75
	                 CONV_2D	         1164.842	    3.124	    3.139	  0.086%	 31.908%	     0.000	        1	[densenet201/conv3_block8_2_conv/Conv2D1]:76
	           CONCATENATION	         1167.993	    0.299	    0.478	  0.013%	 31.921%	     0.000	        1	[densenet201/conv3_block8_concat/concat]:77
	                     MUL	         1168.479	   21.335	   21.277	  0.582%	 32.503%	     0.000	        1	[densenet201/conv3_block9_0_bn/FusedBatchNormV3]:78
	                     ADD	         1189.769	   28.789	   28.747	  0.786%	 33.289%	     0.000	        1	[densenet201/conv3_block9_0_relu/Relu;densenet201/conv3_block9_0_bn/FusedBatchNormV3]:79
	                 CONV_2D	         1218.526	    3.262	    3.272	  0.089%	 33.378%	     0.000	        1	[densenet201/conv3_block9_1_relu/Relu;densenet201/conv3_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block9_1_conv/Conv2D]:80
	                 CONV_2D	         1221.807	    3.198	    3.224	  0.088%	 33.466%	     0.000	        1	[densenet201/conv3_block9_2_conv/Conv2D1]:81
	           CONCATENATION	         1225.043	    0.355	    0.488	  0.013%	 33.480%	     0.000	        1	[densenet201/conv3_block9_concat/concat]:82
	                     MUL	         1225.539	   23.107	   23.045	  0.630%	 34.110%	     0.000	        1	[densenet201/conv3_block10_0_bn/FusedBatchNormV3]:83
	                     ADD	         1248.594	   31.218	   31.162	  0.852%	 34.962%	     0.000	        1	[densenet201/conv3_block10_0_relu/Relu;densenet201/conv3_block10_0_bn/FusedBatchNormV3]:84
	                 CONV_2D	         1279.767	    3.197	    3.126	  0.085%	 35.047%	     0.000	        1	[densenet201/conv3_block10_1_relu/Relu;densenet201/conv3_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block10_1_conv/Conv2D]:85
	                 CONV_2D	         1282.903	    3.195	    3.250	  0.089%	 35.136%	     0.000	        1	[densenet201/conv3_block10_2_conv/Conv2D1]:86
	           CONCATENATION	         1286.165	    0.365	    0.523	  0.014%	 35.150%	     0.000	        1	[densenet201/conv3_block10_concat/concat]:87
	                     MUL	         1286.696	   24.899	   24.849	  0.679%	 35.829%	     0.000	        1	[densenet201/conv3_block11_0_bn/FusedBatchNormV3]:88
	                     ADD	         1311.555	   33.672	   33.544	  0.917%	 36.746%	     0.000	        1	[densenet201/conv3_block11_0_relu/Relu;densenet201/conv3_block11_0_bn/FusedBatchNormV3]:89
	                 CONV_2D	         1345.111	    3.132	    3.115	  0.085%	 36.832%	     0.000	        1	[densenet201/conv3_block11_1_relu/Relu;densenet201/conv3_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block11_1_conv/Conv2D]:90
	                 CONV_2D	         1348.234	    3.699	    3.671	  0.100%	 36.932%	     0.000	        1	[densenet201/conv3_block11_2_conv/Conv2D1]:91
	           CONCATENATION	         1351.917	    0.391	    0.530	  0.014%	 36.946%	     0.000	        1	[densenet201/conv3_block11_concat/concat]:92
	                     MUL	         1352.456	   26.712	   26.572	  0.726%	 37.673%	     0.000	        1	[densenet201/conv3_block12_0_bn/FusedBatchNormV3]:93
	                     ADD	         1379.039	   36.138	   35.966	  0.983%	 38.656%	     0.000	        1	[densenet201/conv3_block12_0_relu/Relu;densenet201/conv3_block12_0_bn/FusedBatchNormV3]:94
	                 CONV_2D	         1415.016	    3.091	    3.125	  0.085%	 38.742%	     0.000	        1	[densenet201/conv3_block12_1_relu/Relu;densenet201/conv3_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block12_1_conv/Conv2D]:95
	                 CONV_2D	         1418.150	    3.247	    3.233	  0.088%	 38.830%	     0.000	        1	[densenet201/conv3_block12_2_conv/Conv2D1]:96
	           CONCATENATION	         1421.394	    0.433	    0.519	  0.014%	 38.844%	     0.000	        1	[densenet201/conv3_block12_concat/concat]:97
	                     MUL	         1421.923	   28.398	   28.416	  0.777%	 39.621%	     0.000	        1	[densenet201/pool3_bn/FusedBatchNormV3]:98
	                     ADD	         1450.350	   38.480	   38.368	  1.049%	 40.670%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99
	                 CONV_2D	         1488.730	    5.351	    5.340	  0.146%	 40.816%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100
	         AVERAGE_POOL_2D	         1494.079	    5.524	    5.491	  0.150%	 40.966%	     0.000	        1	[densenet201/pool3_pool/AvgPool]:101
	                     MUL	         1499.580	    3.638	    3.571	  0.098%	 41.064%	     0.000	        1	[densenet201/conv4_block1_0_bn/FusedBatchNormV31]:102
	                     ADD	         1503.159	    4.860	    4.837	  0.132%	 41.196%	     0.000	        1	[densenet201/conv4_block1_0_relu/Relu;densenet201/conv4_block1_0_bn/FusedBatchNormV3]:103
	                 CONV_2D	         1508.005	    0.896	    0.888	  0.024%	 41.220%	     0.000	        1	[densenet201/conv4_block1_1_relu/Relu;densenet201/conv4_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block1_1_conv/Conv2D]:104
	                 CONV_2D	         1508.901	    0.737	    0.762	  0.021%	 41.241%	     0.000	        1	[densenet201/conv4_block1_2_conv/Conv2D1]:105
	           CONCATENATION	         1509.671	    0.120	    0.078	  0.002%	 41.243%	     0.000	        1	[densenet201/conv4_block1_concat/concat]:106
	                     MUL	         1509.757	    4.046	    4.014	  0.110%	 41.353%	     0.000	        1	[densenet201/conv4_block2_0_bn/FusedBatchNormV31]:107
	                     ADD	         1513.779	    5.462	    5.421	  0.148%	 41.501%	     0.000	        1	[densenet201/conv4_block2_0_relu/Relu;densenet201/conv4_block2_0_bn/FusedBatchNormV3]:108
	                 CONV_2D	         1519.209	    0.914	    0.852	  0.023%	 41.524%	     0.000	        1	[densenet201/conv4_block2_1_relu/Relu;densenet201/conv4_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block2_1_conv/Conv2D]:109
	                 CONV_2D	         1520.069	    0.759	    0.718	  0.020%	 41.544%	     0.000	        1	[densenet201/conv4_block2_2_conv/Conv2D1]:110
	           CONCATENATION	         1520.796	    0.085	    0.079	  0.002%	 41.546%	     0.000	        1	[densenet201/conv4_block2_concat/concat]:111
	                     MUL	         1520.884	    4.477	    4.460	  0.122%	 41.668%	     0.000	        1	[densenet201/conv4_block3_0_bn/FusedBatchNormV31]:112
	                     ADD	         1525.352	    6.055	    6.012	  0.164%	 41.832%	     0.000	        1	[densenet201/conv4_block3_0_relu/Relu;densenet201/conv4_block3_0_bn/FusedBatchNormV3]:113
	                 CONV_2D	         1531.373	    0.852	    0.856	  0.023%	 41.856%	     0.000	        1	[densenet201/conv4_block3_1_relu/Relu;densenet201/conv4_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block3_1_conv/Conv2D]:114
	                 CONV_2D	         1532.239	    0.686	    0.722	  0.020%	 41.876%	     0.000	        1	[densenet201/conv4_block3_2_conv/Conv2D1]:115
	           CONCATENATION	         1532.969	    0.077	    0.072	  0.002%	 41.878%	     0.000	        1	[densenet201/conv4_block3_concat/concat]:116
	                     MUL	         1533.047	    4.847	    4.889	  0.134%	 42.011%	     0.000	        1	[densenet201/conv4_block4_0_bn/FusedBatchNormV31]:117
	                     ADD	         1537.945	    6.599	    6.618	  0.181%	 42.192%	     0.000	        1	[densenet201/conv4_block4_0_relu/Relu;densenet201/conv4_block4_0_bn/FusedBatchNormV3]:118
	                 CONV_2D	         1544.572	    0.828	    0.835	  0.023%	 42.215%	     0.000	        1	[densenet201/conv4_block4_1_relu/Relu;densenet201/conv4_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block4_1_conv/Conv2D]:119
	                 CONV_2D	         1545.414	    0.701	    0.747	  0.020%	 42.235%	     0.000	        1	[densenet201/conv4_block4_2_conv/Conv2D1]:120
	           CONCATENATION	         1546.169	    0.062	    0.080	  0.002%	 42.238%	     0.000	        1	[densenet201/conv4_block4_concat/concat]:121
	                     MUL	         1546.258	    5.276	    5.344	  0.146%	 42.384%	     0.000	        1	[densenet201/conv4_block5_0_bn/FusedBatchNormV31]:122
	                     ADD	         1551.610	    7.139	    7.194	  0.197%	 42.580%	     0.000	        1	[densenet201/conv4_block5_0_relu/Relu;densenet201/conv4_block5_0_bn/FusedBatchNormV3]:123
	                 CONV_2D	         1558.813	    0.824	    0.856	  0.023%	 42.604%	     0.000	        1	[densenet201/conv4_block5_1_relu/Relu;densenet201/conv4_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block5_1_conv/Conv2D]:124
	                 CONV_2D	         1559.677	    0.682	    0.719	  0.020%	 42.623%	     0.000	        1	[densenet201/conv4_block5_2_conv/Conv2D1]:125
	           CONCATENATION	         1560.404	    0.050	    0.083	  0.002%	 42.626%	     0.000	        1	[densenet201/conv4_block5_concat/concat]:126
	                     MUL	         1560.493	    5.716	    5.780	  0.158%	 42.784%	     0.000	        1	[densenet201/conv4_block6_0_bn/FusedBatchNormV31]:127
	                     ADD	         1566.281	    7.723	    7.784	  0.213%	 42.996%	     0.000	        1	[densenet201/conv4_block6_0_relu/Relu;densenet201/conv4_block6_0_bn/FusedBatchNormV3]:128
	                 CONV_2D	         1574.075	    0.814	    0.813	  0.022%	 43.019%	     0.000	        1	[densenet201/conv4_block6_1_relu/Relu;densenet201/conv4_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block6_1_conv/Conv2D]:129
	                 CONV_2D	         1574.895	    0.691	    0.714	  0.020%	 43.038%	     0.000	        1	[densenet201/conv4_block6_2_conv/Conv2D1]:130
	           CONCATENATION	         1575.617	    0.080	    0.087	  0.002%	 43.041%	     0.000	        1	[densenet201/conv4_block6_concat/concat]:131
	                     MUL	         1575.711	    6.158	    6.215	  0.170%	 43.211%	     0.000	        1	[densenet201/conv4_block7_0_bn/FusedBatchNormV31]:132
	                     ADD	         1581.935	    8.305	    8.423	  0.230%	 43.441%	     0.000	        1	[densenet201/conv4_block7_0_relu/Relu;densenet201/conv4_block7_0_bn/FusedBatchNormV3]:133
	                 CONV_2D	         1590.367	    0.790	    0.807	  0.022%	 43.463%	     0.000	        1	[densenet201/conv4_block7_1_relu/Relu;densenet201/conv4_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block7_1_conv/Conv2D]:134
	                 CONV_2D	         1591.181	    0.685	    0.717	  0.020%	 43.482%	     0.000	        1	[densenet201/conv4_block7_2_conv/Conv2D1]:135
	           CONCATENATION	         1591.906	    0.053	    0.088	  0.002%	 43.485%	     0.000	        1	[densenet201/conv4_block7_concat/concat]:136
	                     MUL	         1592.000	    6.585	    6.669	  0.182%	 43.667%	     0.000	        1	[densenet201/conv4_block8_0_bn/FusedBatchNormV31]:137
	                     ADD	         1598.678	    8.959	    8.996	  0.246%	 43.913%	     0.000	        1	[densenet201/conv4_block8_0_relu/Relu;densenet201/conv4_block8_0_bn/FusedBatchNormV3]:138
	                 CONV_2D	         1607.683	    0.806	    0.810	  0.022%	 43.935%	     0.000	        1	[densenet201/conv4_block8_1_relu/Relu;densenet201/conv4_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block8_1_conv/Conv2D]:139
	                 CONV_2D	         1608.500	    0.676	    0.716	  0.020%	 43.955%	     0.000	        1	[densenet201/conv4_block8_2_conv/Conv2D1]:140
	           CONCATENATION	         1609.225	    0.082	    0.099	  0.003%	 43.958%	     0.000	        1	[densenet201/conv4_block8_concat/concat]:141
	                     MUL	         1609.332	    7.028	    7.145	  0.195%	 44.153%	     0.000	        1	[densenet201/conv4_block9_0_bn/FusedBatchNormV31]:142
	                     ADD	         1616.485	    9.487	    9.637	  0.263%	 44.416%	     0.000	        1	[densenet201/conv4_block9_0_relu/Relu;densenet201/conv4_block9_0_bn/FusedBatchNormV3]:143
	                 CONV_2D	         1626.131	    0.782	    0.814	  0.022%	 44.439%	     0.000	        1	[densenet201/conv4_block9_1_relu/Relu;densenet201/conv4_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block9_1_conv/Conv2D]:144
	                 CONV_2D	         1626.953	    0.701	    0.722	  0.020%	 44.458%	     0.000	        1	[densenet201/conv4_block9_2_conv/Conv2D1]:145
	           CONCATENATION	         1627.682	    0.057	    0.099	  0.003%	 44.461%	     0.000	        1	[densenet201/conv4_block9_concat/concat]:146
	                     MUL	         1627.788	    7.443	    7.523	  0.206%	 44.667%	     0.000	        1	[densenet201/conv4_block10_0_bn/FusedBatchNormV31]:147
	                     ADD	         1635.320	   10.137	   10.266	  0.281%	 44.947%	     0.000	        1	[densenet201/conv4_block10_0_relu/Relu;densenet201/conv4_block10_0_bn/FusedBatchNormV3]:148
	                 CONV_2D	         1645.596	    0.741	    0.773	  0.021%	 44.969%	     0.000	        1	[densenet201/conv4_block10_1_relu/Relu;densenet201/conv4_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block10_1_conv/Conv2D]:149
	                 CONV_2D	         1646.377	    0.669	    0.719	  0.020%	 44.988%	     0.000	        1	[densenet201/conv4_block10_2_conv/Conv2D1]:150
	           CONCATENATION	         1647.103	    0.060	    0.099	  0.003%	 44.991%	     0.000	        1	[densenet201/conv4_block10_concat/concat]:151
	                     MUL	         1647.209	    7.930	    7.999	  0.219%	 45.210%	     0.000	        1	[densenet201/conv4_block11_0_bn/FusedBatchNormV31]:152
	                     ADD	         1655.217	   10.740	   10.866	  0.297%	 45.507%	     0.000	        1	[densenet201/conv4_block11_0_relu/Relu;densenet201/conv4_block11_0_bn/FusedBatchNormV3]:153
	                 CONV_2D	         1666.092	    0.739	    0.763	  0.021%	 45.527%	     0.000	        1	[densenet201/conv4_block11_1_relu/Relu;densenet201/conv4_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block11_1_conv/Conv2D]:154
	                 CONV_2D	         1666.862	    0.679	    0.709	  0.019%	 45.547%	     0.000	        1	[densenet201/conv4_block11_2_conv/Conv2D1]:155
	           CONCATENATION	         1667.579	    0.064	    0.112	  0.003%	 45.550%	     0.000	        1	[densenet201/conv4_block11_concat/concat]:156
	                     MUL	         1667.698	    8.334	    8.412	  0.230%	 45.780%	     0.000	        1	[densenet201/conv4_block12_0_bn/FusedBatchNormV31]:157
	                     ADD	         1676.118	   11.332	   11.444	  0.313%	 46.093%	     0.000	        1	[densenet201/conv4_block12_0_relu/Relu;densenet201/conv4_block12_0_bn/FusedBatchNormV3]:158
	                 CONV_2D	         1687.571	    0.783	    0.770	  0.021%	 46.114%	     0.000	        1	[densenet201/conv4_block12_1_relu/Relu;densenet201/conv4_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block12_1_conv/Conv2D]:159
	                 CONV_2D	         1688.348	    0.713	    0.717	  0.020%	 46.133%	     0.000	        1	[densenet201/conv4_block12_2_conv/Conv2D1]:160
	           CONCATENATION	         1689.073	    0.095	    0.108	  0.003%	 46.136%	     0.000	        1	[densenet201/conv4_block12_concat/concat]:161
	                     MUL	         1689.188	    8.767	    8.836	  0.242%	 46.378%	     0.000	        1	[densenet201/conv4_block13_0_bn/FusedBatchNormV31]:162
	                     ADD	         1698.036	   12.028	   12.040	  0.329%	 46.707%	     0.000	        1	[densenet201/conv4_block13_0_relu/Relu;densenet201/conv4_block13_0_bn/FusedBatchNormV3]:163
	                 CONV_2D	         1710.085	    0.763	    0.774	  0.021%	 46.728%	     0.000	        1	[densenet201/conv4_block13_1_relu/Relu;densenet201/conv4_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block13_1_conv/Conv2D]:164
	                 CONV_2D	         1710.866	    0.676	    0.709	  0.019%	 46.748%	     0.000	        1	[densenet201/conv4_block13_2_conv/Conv2D1]:165
	           CONCATENATION	         1711.582	    0.099	    0.113	  0.003%	 46.751%	     0.000	        1	[densenet201/conv4_block13_concat/concat]:166
	                     MUL	         1711.702	    9.210	    9.268	  0.253%	 47.004%	     0.000	        1	[densenet201/conv4_block14_0_bn/FusedBatchNormV31]:167
	                     ADD	         1720.979	   12.520	   12.639	  0.346%	 47.350%	     0.000	        1	[densenet201/conv4_block14_0_relu/Relu;densenet201/conv4_block14_0_bn/FusedBatchNormV3]:168
	                 CONV_2D	         1733.627	    0.703	    0.722	  0.020%	 47.369%	     0.000	        1	[densenet201/conv4_block14_1_relu/Relu;densenet201/conv4_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block14_1_conv/Conv2D]:169
	                 CONV_2D	         1734.357	    0.691	    0.735	  0.020%	 47.389%	     0.000	        1	[densenet201/conv4_block14_2_conv/Conv2D1]:170
	           CONCATENATION	         1735.099	    0.088	    0.113	  0.003%	 47.392%	     0.000	        1	[densenet201/conv4_block14_concat/concat]:171
	                     MUL	         1735.219	    9.638	    9.728	  0.266%	 47.658%	     0.000	        1	[densenet201/conv4_block15_0_bn/FusedBatchNormV31]:172
	                     ADD	         1744.956	   13.200	   13.226	  0.362%	 48.020%	     0.000	        1	[densenet201/conv4_block15_0_relu/Relu;densenet201/conv4_block15_0_bn/FusedBatchNormV3]:173
	                 CONV_2D	         1758.191	    0.717	    0.722	  0.020%	 48.040%	     0.000	        1	[densenet201/conv4_block15_1_relu/Relu;densenet201/conv4_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block15_1_conv/Conv2D]:174
	                 CONV_2D	         1758.920	    0.699	    0.707	  0.019%	 48.059%	     0.000	        1	[densenet201/conv4_block15_2_conv/Conv2D1]:175
	           CONCATENATION	         1759.636	    0.128	    0.115	  0.003%	 48.062%	     0.000	        1	[densenet201/conv4_block15_concat/concat]:176
	                     MUL	         1759.758	   10.105	   10.150	  0.277%	 48.340%	     0.000	        1	[densenet201/conv4_block16_0_bn/FusedBatchNormV31]:177
	                     ADD	         1769.917	   13.722	   13.842	  0.378%	 48.718%	     0.000	        1	[densenet201/conv4_block16_0_relu/Relu;densenet201/conv4_block16_0_bn/FusedBatchNormV3]:178
	                 CONV_2D	         1783.768	    0.707	    0.721	  0.020%	 48.738%	     0.000	        1	[densenet201/conv4_block16_1_relu/Relu;densenet201/conv4_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block16_1_conv/Conv2D]:179
	                 CONV_2D	         1784.496	    0.747	    0.802	  0.022%	 48.760%	     0.000	        1	[densenet201/conv4_block16_2_conv/Conv2D1]:180
	           CONCATENATION	         1785.306	    0.104	    0.123	  0.003%	 48.763%	     0.000	        1	[densenet201/conv4_block16_concat/concat]:181
	                     MUL	         1785.437	   10.491	   10.591	  0.290%	 49.053%	     0.000	        1	[densenet201/conv4_block17_0_bn/FusedBatchNormV31]:182
	                     ADD	         1796.036	   14.384	   14.357	  0.392%	 49.445%	     0.000	        1	[densenet201/conv4_block17_0_relu/Relu;densenet201/conv4_block17_0_bn/FusedBatchNormV3]:183
	                 CONV_2D	         1810.402	    0.708	    0.725	  0.020%	 49.465%	     0.000	        1	[densenet201/conv4_block17_1_relu/Relu;densenet201/conv4_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block17_1_conv/Conv2D]:184
	                 CONV_2D	         1811.134	    0.807	    0.798	  0.022%	 49.487%	     0.000	        1	[densenet201/conv4_block17_2_conv/Conv2D1]:185
	           CONCATENATION	         1811.940	    0.123	    0.132	  0.004%	 49.490%	     0.000	        1	[densenet201/conv4_block17_concat/concat]:186
	                     MUL	         1812.079	   10.967	   11.049	  0.302%	 49.792%	     0.000	        1	[densenet201/conv4_block18_0_bn/FusedBatchNormV31]:187
	                     ADD	         1823.138	   15.034	   15.045	  0.411%	 50.204%	     0.000	        1	[densenet201/conv4_block18_0_relu/Relu;densenet201/conv4_block18_0_bn/FusedBatchNormV3]:188
	                 CONV_2D	         1838.192	    0.689	    0.691	  0.019%	 50.223%	     0.000	        1	[densenet201/conv4_block18_1_relu/Relu;densenet201/conv4_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block18_1_conv/Conv2D]:189
	                 CONV_2D	         1838.890	    0.749	    0.759	  0.021%	 50.243%	     0.000	        1	[densenet201/conv4_block18_2_conv/Conv2D1]:190
	           CONCATENATION	         1839.658	    0.136	    0.138	  0.004%	 50.247%	     0.000	        1	[densenet201/conv4_block18_concat/concat]:191
	                     MUL	         1839.803	   11.405	   11.488	  0.314%	 50.561%	     0.000	        1	[densenet201/conv4_block19_0_bn/FusedBatchNormV31]:192
	                     ADD	         1851.299	   15.529	   15.621	  0.427%	 50.988%	     0.000	        1	[densenet201/conv4_block19_0_relu/Relu;densenet201/conv4_block19_0_bn/FusedBatchNormV3]:193
	                 CONV_2D	         1866.929	    0.687	    0.688	  0.019%	 51.007%	     0.000	        1	[densenet201/conv4_block19_1_relu/Relu;densenet201/conv4_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block19_1_conv/Conv2D]:194
	                 CONV_2D	         1867.624	    0.716	    0.805	  0.022%	 51.029%	     0.000	        1	[densenet201/conv4_block19_2_conv/Conv2D1]:195
	           CONCATENATION	         1868.438	    0.117	    0.142	  0.004%	 51.033%	     0.000	        1	[densenet201/conv4_block19_concat/concat]:196
	                     MUL	         1868.587	   11.841	   11.931	  0.326%	 51.359%	     0.000	        1	[densenet201/conv4_block20_0_bn/FusedBatchNormV31]:197
	                     ADD	         1880.527	   16.080	   16.183	  0.442%	 51.802%	     0.000	        1	[densenet201/conv4_block20_0_relu/Relu;densenet201/conv4_block20_0_bn/FusedBatchNormV3]:198
	                 CONV_2D	         1896.719	    0.664	    0.677	  0.018%	 51.820%	     0.000	        1	[densenet201/conv4_block20_1_relu/Relu;densenet201/conv4_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block20_1_conv/Conv2D]:199
	                 CONV_2D	         1897.403	    0.850	    0.811	  0.022%	 51.842%	     0.000	        1	[densenet201/conv4_block20_2_conv/Conv2D1]:200
	           CONCATENATION	         1898.222	    0.120	    0.154	  0.004%	 51.846%	     0.000	        1	[densenet201/conv4_block20_concat/concat]:201
	                     MUL	         1898.383	   12.310	   12.335	  0.337%	 52.184%	     0.000	        1	[densenet201/conv4_block21_0_bn/FusedBatchNormV3]:202
	                     ADD	         1910.727	   16.720	   16.765	  0.458%	 52.642%	     0.000	        1	[densenet201/conv4_block21_0_relu/Relu;densenet201/conv4_block21_0_bn/FusedBatchNormV3]:203
	                 CONV_2D	         1927.502	    0.669	    0.685	  0.019%	 52.661%	     0.000	        1	[densenet201/conv4_block21_1_relu/Relu;densenet201/conv4_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block21_1_conv/Conv2D]:204
	                 CONV_2D	         1928.193	    0.708	    0.735	  0.020%	 52.681%	     0.000	        1	[densenet201/conv4_block21_2_conv/Conv2D1]:205
	           CONCATENATION	         1928.936	    0.174	    0.152	  0.004%	 52.685%	     0.000	        1	[densenet201/conv4_block21_concat/concat]:206
	                     MUL	         1929.095	   12.707	   12.774	  0.349%	 53.034%	     0.000	        1	[densenet201/conv4_block22_0_bn/FusedBatchNormV3]:207
	                     ADD	         1941.878	   17.281	   17.373	  0.475%	 53.509%	     0.000	        1	[densenet201/conv4_block22_0_relu/Relu;densenet201/conv4_block22_0_bn/FusedBatchNormV3]:208
	                 CONV_2D	         1959.260	    0.601	    0.628	  0.017%	 53.526%	     0.000	        1	[densenet201/conv4_block22_1_relu/Relu;densenet201/conv4_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block22_1_conv/Conv2D]:209
	                 CONV_2D	         1959.895	    0.734	    0.796	  0.022%	 53.548%	     0.000	        1	[densenet201/conv4_block22_2_conv/Conv2D1]:210
	           CONCATENATION	         1960.699	    0.124	    0.171	  0.005%	 53.553%	     0.000	        1	[densenet201/conv4_block22_concat/concat]:211
	                     MUL	         1960.877	   13.133	   13.201	  0.361%	 53.914%	     0.000	        1	[densenet201/conv4_block23_0_bn/FusedBatchNormV3]:212
	                     ADD	         1974.087	   17.881	   17.984	  0.492%	 54.405%	     0.000	        1	[densenet201/conv4_block23_0_relu/Relu;densenet201/conv4_block23_0_bn/FusedBatchNormV3]:213
	                 CONV_2D	         1992.080	    0.602	    0.628	  0.017%	 54.422%	     0.000	        1	[densenet201/conv4_block23_1_relu/Relu;densenet201/conv4_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block23_1_conv/Conv2D]:214
	                 CONV_2D	         1992.715	    0.838	    0.773	  0.021%	 54.444%	     0.000	        1	[densenet201/conv4_block23_2_conv/Conv2D1]:215
	           CONCATENATION	         1993.496	    0.130	    0.177	  0.005%	 54.448%	     0.000	        1	[densenet201/conv4_block23_concat/concat]:216
	                     MUL	         1993.681	   13.591	   13.677	  0.374%	 54.822%	     0.000	        1	[densenet201/conv4_block24_0_bn/FusedBatchNormV3]:217
	                     ADD	         2007.367	   18.460	   18.563	  0.507%	 55.330%	     0.000	        1	[densenet201/conv4_block24_0_relu/Relu;densenet201/conv4_block24_0_bn/FusedBatchNormV3]:218
	                 CONV_2D	         2025.939	    0.628	    0.628	  0.017%	 55.347%	     0.000	        1	[densenet201/conv4_block24_1_relu/Relu;densenet201/conv4_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block24_1_conv/Conv2D]:219
	                 CONV_2D	         2026.574	    0.697	    0.736	  0.020%	 55.367%	     0.000	        1	[densenet201/conv4_block24_2_conv/Conv2D1]:220
	           CONCATENATION	         2027.318	    0.133	    0.171	  0.005%	 55.372%	     0.000	        1	[densenet201/conv4_block24_concat/concat]:221
	                     MUL	         2027.496	   13.991	   14.111	  0.386%	 55.758%	     0.000	        1	[densenet201/conv4_block25_0_bn/FusedBatchNormV3]:222
	                     ADD	         2041.617	   19.056	   19.222	  0.525%	 56.283%	     0.000	        1	[densenet201/conv4_block25_0_relu/Relu;densenet201/conv4_block25_0_bn/FusedBatchNormV3]:223
	                 CONV_2D	         2060.850	    0.600	    0.628	  0.017%	 56.300%	     0.000	        1	[densenet201/conv4_block25_1_relu/Relu;densenet201/conv4_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block25_1_conv/Conv2D]:224
	                 CONV_2D	         2061.485	    0.733	    0.808	  0.022%	 56.322%	     0.000	        1	[densenet201/conv4_block25_2_conv/Conv2D1]:225
	           CONCATENATION	         2062.302	    0.130	    0.206	  0.006%	 56.328%	     0.000	        1	[densenet201/conv4_block25_concat/concat]:226
	                     MUL	         2062.516	   14.466	   14.566	  0.398%	 56.726%	     0.000	        1	[densenet201/conv4_block26_0_bn/FusedBatchNormV3]:227
	                     ADD	         2077.091	   19.731	   19.779	  0.541%	 57.267%	     0.000	        1	[densenet201/conv4_block26_0_relu/Relu;densenet201/conv4_block26_0_bn/FusedBatchNormV3]:228
	                 CONV_2D	         2096.880	    1.467	    1.459	  0.040%	 57.307%	     0.000	        1	[densenet201/conv4_block26_1_relu/Relu;densenet201/conv4_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block26_1_conv/Conv2D]:229
	                 CONV_2D	         2098.348	    0.703	    0.747	  0.020%	 57.327%	     0.000	        1	[densenet201/conv4_block26_2_conv/Conv2D1]:230
	           CONCATENATION	         2099.102	    0.203	    0.199	  0.005%	 57.333%	     0.000	        1	[densenet201/conv4_block26_concat/concat]:231
	                     MUL	         2099.308	   14.893	   15.004	  0.410%	 57.743%	     0.000	        1	[densenet201/conv4_block27_0_bn/FusedBatchNormV3]:232
	                     ADD	         2114.322	   20.291	   20.412	  0.558%	 58.301%	     0.000	        1	[densenet201/conv4_block27_0_relu/Relu;densenet201/conv4_block27_0_bn/FusedBatchNormV3]:233
	                 CONV_2D	         2134.745	    1.470	    1.496	  0.041%	 58.342%	     0.000	        1	[densenet201/conv4_block27_1_relu/Relu;densenet201/conv4_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block27_1_conv/Conv2D]:234
	                 CONV_2D	         2136.249	    0.709	    0.759	  0.021%	 58.362%	     0.000	        1	[densenet201/conv4_block27_2_conv/Conv2D1]:235
	           CONCATENATION	         2137.017	    0.167	    0.217	  0.006%	 58.368%	     0.000	        1	[densenet201/conv4_block27_concat/concat]:236
	                     MUL	         2137.242	   15.479	   15.458	  0.423%	 58.791%	     0.000	        1	[densenet201/conv4_block28_0_bn/FusedBatchNormV3]:237
	                     ADD	         2152.710	   21.137	   21.070	  0.576%	 59.367%	     0.000	        1	[densenet201/conv4_block28_0_relu/Relu;densenet201/conv4_block28_0_bn/FusedBatchNormV3]:238
	                 CONV_2D	         2173.791	    1.596	    1.494	  0.041%	 59.408%	     0.000	        1	[densenet201/conv4_block28_1_relu/Relu;densenet201/conv4_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block28_1_conv/Conv2D]:239
	                 CONV_2D	         2175.293	    0.814	    0.761	  0.021%	 59.429%	     0.000	        1	[densenet201/conv4_block28_2_conv/Conv2D1]:240
	           CONCATENATION	         2176.063	    0.126	    0.178	  0.005%	 59.434%	     0.000	        1	[densenet201/conv4_block28_concat/concat]:241
	                     MUL	         2176.248	   15.889	   15.872	  0.434%	 59.867%	     0.000	        1	[densenet201/conv4_block29_0_bn/FusedBatchNormV3]:242
	                     ADD	         2192.129	   21.510	   21.599	  0.590%	 60.458%	     0.000	        1	[densenet201/conv4_block29_0_relu/Relu;densenet201/conv4_block29_0_bn/FusedBatchNormV3]:243
	                 CONV_2D	         2213.737	    1.494	    1.489	  0.041%	 60.499%	     0.000	        1	[densenet201/conv4_block29_1_relu/Relu;densenet201/conv4_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block29_1_conv/Conv2D]:244
	                 CONV_2D	         2215.235	    0.750	    0.755	  0.021%	 60.519%	     0.000	        1	[densenet201/conv4_block29_2_conv/Conv2D1]:245
	           CONCATENATION	         2215.998	    0.167	    0.188	  0.005%	 60.524%	     0.000	        1	[densenet201/conv4_block29_concat/concat]:246
	                     MUL	         2216.193	   16.217	   16.298	  0.446%	 60.970%	     0.000	        1	[densenet201/conv4_block30_0_bn/FusedBatchNormV3]:247
	                     ADD	         2232.501	   22.046	   22.219	  0.607%	 61.577%	     0.000	        1	[densenet201/conv4_block30_0_relu/Relu;densenet201/conv4_block30_0_bn/FusedBatchNormV3]:248
	                 CONV_2D	         2254.730	    1.432	    1.440	  0.039%	 61.617%	     0.000	        1	[densenet201/conv4_block30_1_relu/Relu;densenet201/conv4_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block30_1_conv/Conv2D]:249
	                 CONV_2D	         2256.178	    0.731	    0.751	  0.021%	 61.637%	     0.000	        1	[densenet201/conv4_block30_2_conv/Conv2D1]:250
	           CONCATENATION	         2256.937	    0.162	    0.190	  0.005%	 61.642%	     0.000	        1	[densenet201/conv4_block30_concat/concat]:251
	                     MUL	         2257.135	   16.642	   16.760	  0.458%	 62.101%	     0.000	        1	[densenet201/conv4_block31_0_bn/FusedBatchNormV3]:252
	                     ADD	         2273.904	   22.633	   22.824	  0.624%	 62.725%	     0.000	        1	[densenet201/conv4_block31_0_relu/Relu;densenet201/conv4_block31_0_bn/FusedBatchNormV3]:253
	                 CONV_2D	         2296.737	    1.420	    1.453	  0.040%	 62.764%	     0.000	        1	[densenet201/conv4_block31_1_relu/Relu;densenet201/conv4_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block31_1_conv/Conv2D]:254
	                 CONV_2D	         2298.200	    0.748	    0.770	  0.021%	 62.785%	     0.000	        1	[densenet201/conv4_block31_2_conv/Conv2D1]:255
	           CONCATENATION	         2298.978	    0.187	    0.205	  0.006%	 62.791%	     0.000	        1	[densenet201/conv4_block31_concat/concat]:256
	                     MUL	         2299.190	   17.112	   17.224	  0.471%	 63.262%	     0.000	        1	[densenet201/conv4_block32_0_bn/FusedBatchNormV3]:257
	                     ADD	         2316.423	   23.320	   23.470	  0.642%	 63.904%	     0.000	        1	[densenet201/conv4_block32_0_relu/Relu;densenet201/conv4_block32_0_bn/FusedBatchNormV3]:258
	                 CONV_2D	         2339.903	    1.432	    1.451	  0.040%	 63.943%	     0.000	        1	[densenet201/conv4_block32_1_relu/Relu;densenet201/conv4_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block32_1_conv/Conv2D]:259
	                 CONV_2D	         2341.362	    0.744	    0.762	  0.021%	 63.964%	     0.000	        1	[densenet201/conv4_block32_2_conv/Conv2D1]:260
	           CONCATENATION	         2342.132	    0.179	    0.199	  0.005%	 63.969%	     0.000	        1	[densenet201/conv4_block32_concat/concat]:261
	                     MUL	         2342.339	   17.566	   17.687	  0.484%	 64.453%	     0.000	        1	[densenet201/conv4_block33_0_bn/FusedBatchNormV3]:262
	                     ADD	         2360.036	   23.906	   24.001	  0.656%	 65.109%	     0.000	        1	[densenet201/conv4_block33_0_relu/Relu;densenet201/conv4_block33_0_bn/FusedBatchNormV3]:263
	                 CONV_2D	         2384.046	    1.435	    1.453	  0.040%	 65.149%	     0.000	        1	[densenet201/conv4_block33_1_relu/Relu;densenet201/conv4_block33_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block33_1_conv/Conv2D]:264
	                 CONV_2D	         2385.508	    0.737	    0.776	  0.021%	 65.170%	     0.000	        1	[densenet201/conv4_block33_2_conv/Conv2D1]:265
	           CONCATENATION	         2386.292	    0.182	    0.217	  0.006%	 65.176%	     0.000	        1	[densenet201/conv4_block33_concat/concat]:266
	                     MUL	         2386.517	   17.963	   18.075	  0.494%	 65.670%	     0.000	        1	[densenet201/conv4_block34_0_bn/FusedBatchNormV3]:267
	                     ADD	         2404.600	   24.649	   24.816	  0.678%	 66.349%	     0.000	        1	[densenet201/conv4_block34_0_relu/Relu;densenet201/conv4_block34_0_bn/FusedBatchNormV3]:268
	                 CONV_2D	         2429.426	    1.377	    1.424	  0.039%	 66.387%	     0.000	        1	[densenet201/conv4_block34_1_relu/Relu;densenet201/conv4_block34_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block34_1_conv/Conv2D]:269
	                 CONV_2D	         2430.858	    0.724	    0.755	  0.021%	 66.408%	     0.000	        1	[densenet201/conv4_block34_2_conv/Conv2D1]:270
	           CONCATENATION	         2431.620	    0.196	    0.204	  0.006%	 66.414%	     0.000	        1	[densenet201/conv4_block34_concat/concat]:271
	                     MUL	         2431.831	   18.394	   18.634	  0.509%	 66.923%	     0.000	        1	[densenet201/conv4_block35_0_bn/FusedBatchNormV3]:272
	                     ADD	         2450.474	   25.025	   25.405	  0.695%	 67.618%	     0.000	        1	[densenet201/conv4_block35_0_relu/Relu;densenet201/conv4_block35_0_bn/FusedBatchNormV3]:273
	                 CONV_2D	         2475.890	    1.403	    1.419	  0.039%	 67.656%	     0.000	        1	[densenet201/conv4_block35_1_relu/Relu;densenet201/conv4_block35_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block35_1_conv/Conv2D]:274
	                 CONV_2D	         2477.317	    0.743	    0.788	  0.022%	 67.678%	     0.000	        1	[densenet201/conv4_block35_2_conv/Conv2D1]:275
	           CONCATENATION	         2478.117	    0.210	    0.227	  0.006%	 67.684%	     0.000	        1	[densenet201/conv4_block35_concat/concat]:276
	                     MUL	         2478.351	   18.842	   19.048	  0.521%	 68.205%	     0.000	        1	[densenet201/conv4_block36_0_bn/FusedBatchNormV3]:277
	                     ADD	         2497.408	   25.688	   25.874	  0.707%	 68.912%	     0.000	        1	[densenet201/conv4_block36_0_relu/Relu;densenet201/conv4_block36_0_bn/FusedBatchNormV3]:278
	                 CONV_2D	         2523.293	    1.387	    1.470	  0.040%	 68.952%	     0.000	        1	[densenet201/conv4_block36_1_relu/Relu;densenet201/conv4_block36_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block36_1_conv/Conv2D]:279
	                 CONV_2D	         2524.771	    0.737	    0.762	  0.021%	 68.973%	     0.000	        1	[densenet201/conv4_block36_2_conv/Conv2D1]:280
	           CONCATENATION	         2525.541	    0.228	    0.222	  0.006%	 68.979%	     0.000	        1	[densenet201/conv4_block36_concat/concat]:281
	                     MUL	         2525.771	   19.354	   19.458	  0.532%	 69.511%	     0.000	        1	[densenet201/conv4_block37_0_bn/FusedBatchNormV3]:282
	                     ADD	         2545.238	   26.419	   26.460	  0.723%	 70.235%	     0.000	        1	[densenet201/conv4_block37_0_relu/Relu;densenet201/conv4_block37_0_bn/FusedBatchNormV3]:283
	                 CONV_2D	         2571.708	    1.393	    1.410	  0.039%	 70.273%	     0.000	        1	[densenet201/conv4_block37_1_relu/Relu;densenet201/conv4_block37_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block37_1_conv/Conv2D]:284
	                 CONV_2D	         2573.126	    0.746	    0.774	  0.021%	 70.294%	     0.000	        1	[densenet201/conv4_block37_2_conv/Conv2D1]:285
	           CONCATENATION	         2573.909	    0.177	    0.226	  0.006%	 70.301%	     0.000	        1	[densenet201/conv4_block37_concat/concat]:286
	                     MUL	         2574.142	   19.836	   19.911	  0.544%	 70.845%	     0.000	        1	[densenet201/conv4_block38_0_bn/FusedBatchNormV3]:287
	                     ADD	         2594.064	   26.889	   27.056	  0.740%	 71.585%	     0.000	        1	[densenet201/conv4_block38_0_relu/Relu;densenet201/conv4_block38_0_bn/FusedBatchNormV3]:288
	                 CONV_2D	         2621.130	    1.362	    1.382	  0.038%	 71.622%	     0.000	        1	[densenet201/conv4_block38_1_relu/Relu;densenet201/conv4_block38_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block38_1_conv/Conv2D]:289
	                 CONV_2D	         2622.521	    0.763	    0.797	  0.022%	 71.644%	     0.000	        1	[densenet201/conv4_block38_2_conv/Conv2D1]:290
	           CONCATENATION	         2623.328	    0.202	    0.239	  0.007%	 71.651%	     0.000	        1	[densenet201/conv4_block38_concat/concat]:291
	                     MUL	         2623.575	   20.155	   20.327	  0.556%	 72.206%	     0.000	        1	[densenet201/conv4_block39_0_bn/FusedBatchNormV3]:292
	                     ADD	         2643.912	   27.483	   27.738	  0.758%	 72.965%	     0.000	        1	[densenet201/conv4_block39_0_relu/Relu;densenet201/conv4_block39_0_bn/FusedBatchNormV3]:293
	                 CONV_2D	         2671.659	    1.381	    1.377	  0.038%	 73.002%	     0.000	        1	[densenet201/conv4_block39_1_relu/Relu;densenet201/conv4_block39_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block39_1_conv/Conv2D]:294
	                 CONV_2D	         2673.045	    0.766	    0.775	  0.021%	 73.023%	     0.000	        1	[densenet201/conv4_block39_2_conv/Conv2D1]:295
	           CONCATENATION	         2673.829	    0.209	    0.235	  0.006%	 73.030%	     0.000	        1	[densenet201/conv4_block39_concat/concat]:296
	                     MUL	         2674.072	   20.644	   20.836	  0.570%	 73.599%	     0.000	        1	[densenet201/conv4_block40_0_bn/FusedBatchNormV3]:297
	                     ADD	         2694.918	   28.014	   28.298	  0.774%	 74.373%	     0.000	        1	[densenet201/conv4_block40_0_relu/Relu;densenet201/conv4_block40_0_bn/FusedBatchNormV3]:298
	                 CONV_2D	         2723.229	    1.361	    1.374	  0.038%	 74.411%	     0.000	        1	[densenet201/conv4_block40_1_relu/Relu;densenet201/conv4_block40_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block40_1_conv/Conv2D]:299
	                 CONV_2D	         2724.612	    0.735	    0.775	  0.021%	 74.432%	     0.000	        1	[densenet201/conv4_block40_2_conv/Conv2D1]:300
	           CONCATENATION	         2725.396	    0.227	    0.246	  0.007%	 74.439%	     0.000	        1	[densenet201/conv4_block40_concat/concat]:301
	                     MUL	         2725.650	   21.130	   21.270	  0.581%	 75.020%	     0.000	        1	[densenet201/conv4_block41_0_bn/FusedBatchNormV3]:302
	                     ADD	         2746.929	   28.636	   28.911	  0.790%	 75.810%	     0.000	        1	[densenet201/conv4_block41_0_relu/Relu;densenet201/conv4_block41_0_bn/FusedBatchNormV3]:303
	                 CONV_2D	         2775.852	    1.371	    1.415	  0.039%	 75.849%	     0.000	        1	[densenet201/conv4_block41_1_relu/Relu;densenet201/conv4_block41_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block41_1_conv/Conv2D]:304
	                 CONV_2D	         2777.276	    0.738	    0.796	  0.022%	 75.871%	     0.000	        1	[densenet201/conv4_block41_2_conv/Conv2D1]:305
	           CONCATENATION	         2778.080	    0.259	    0.259	  0.007%	 75.878%	     0.000	        1	[densenet201/conv4_block41_concat/concat]:306
	                     MUL	         2778.349	   21.463	   21.749	  0.595%	 76.473%	     0.000	        1	[densenet201/conv4_block42_0_bn/FusedBatchNormV3]:307
	                     ADD	         2800.109	   29.247	   29.593	  0.809%	 77.282%	     0.000	        1	[densenet201/conv4_block42_0_relu/Relu;densenet201/conv4_block42_0_bn/FusedBatchNormV3]:308
	                 CONV_2D	         2829.713	    1.302	    1.322	  0.036%	 77.318%	     0.000	        1	[densenet201/conv4_block42_1_relu/Relu;densenet201/conv4_block42_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block42_1_conv/Conv2D]:309
	                 CONV_2D	         2831.044	    0.744	    0.780	  0.021%	 77.339%	     0.000	        1	[densenet201/conv4_block42_2_conv/Conv2D1]:310
	           CONCATENATION	         2831.834	    0.254	    0.263	  0.007%	 77.346%	     0.000	        1	[densenet201/conv4_block42_concat/concat]:311
	                     MUL	         2832.104	   21.915	   22.097	  0.604%	 77.950%	     0.000	        1	[densenet201/conv4_block43_0_bn/FusedBatchNormV3]:312
	                     ADD	         2854.211	   29.839	   30.226	  0.826%	 78.777%	     0.000	        1	[densenet201/conv4_block43_0_relu/Relu;densenet201/conv4_block43_0_bn/FusedBatchNormV3]:313
	                 CONV_2D	         2884.448	    1.314	    1.325	  0.036%	 78.813%	     0.000	        1	[densenet201/conv4_block43_1_relu/Relu;densenet201/conv4_block43_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block43_1_conv/Conv2D]:314
	                 CONV_2D	         2885.782	    0.725	    0.783	  0.021%	 78.834%	     0.000	        1	[densenet201/conv4_block43_2_conv/Conv2D1]:315
	           CONCATENATION	         2886.574	    0.270	    0.270	  0.007%	 78.842%	     0.000	        1	[densenet201/conv4_block43_concat/concat]:316
	                     MUL	         2886.852	   22.339	   22.521	  0.616%	 79.457%	     0.000	        1	[densenet201/conv4_block44_0_bn/FusedBatchNormV3]:317
	                     ADD	         2909.383	   30.482	   30.641	  0.838%	 80.295%	     0.000	        1	[densenet201/conv4_block44_0_relu/Relu;densenet201/conv4_block44_0_bn/FusedBatchNormV3]:318
	                 CONV_2D	         2940.035	    1.379	    1.339	  0.037%	 80.332%	     0.000	        1	[densenet201/conv4_block44_1_relu/Relu;densenet201/conv4_block44_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block44_1_conv/Conv2D]:319
	                 CONV_2D	         2941.383	    0.788	    0.773	  0.021%	 80.353%	     0.000	        1	[densenet201/conv4_block44_2_conv/Conv2D1]:320
	           CONCATENATION	         2942.165	    0.285	    0.284	  0.008%	 80.361%	     0.000	        1	[densenet201/conv4_block44_concat/concat]:321
	                     MUL	         2942.456	   22.731	   22.945	  0.627%	 80.988%	     0.000	        1	[densenet201/conv4_block45_0_bn/FusedBatchNormV3]:322
	                     ADD	         2965.411	   30.958	   31.231	  0.854%	 81.842%	     0.000	        1	[densenet201/conv4_block45_0_relu/Relu;densenet201/conv4_block45_0_bn/FusedBatchNormV3]:323
	                 CONV_2D	         2996.653	    1.304	    1.322	  0.036%	 81.878%	     0.000	        1	[densenet201/conv4_block45_1_relu/Relu;densenet201/conv4_block45_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block45_1_conv/Conv2D]:324
	                 CONV_2D	         2997.983	    0.751	    0.781	  0.021%	 81.899%	     0.000	        1	[densenet201/conv4_block45_2_conv/Conv2D1]:325
	           CONCATENATION	         2998.773	    0.302	    0.285	  0.008%	 81.907%	     0.000	        1	[densenet201/conv4_block45_concat/concat]:326
	                     MUL	         2999.066	   23.233	   23.370	  0.639%	 82.546%	     0.000	        1	[densenet201/conv4_block46_0_bn/FusedBatchNormV3]:327
	                     ADD	         3022.446	   31.759	   31.786	  0.869%	 83.415%	     0.000	        1	[densenet201/conv4_block46_0_relu/Relu;densenet201/conv4_block46_0_bn/FusedBatchNormV3]:328
	                 CONV_2D	         3054.242	    1.337	    1.294	  0.035%	 83.450%	     0.000	        1	[densenet201/conv4_block46_1_relu/Relu;densenet201/conv4_block46_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block46_1_conv/Conv2D]:329
	                 CONV_2D	         3055.544	    0.807	    0.772	  0.021%	 83.471%	     0.000	        1	[densenet201/conv4_block46_2_conv/Conv2D1]:330
	           CONCATENATION	         3056.324	    0.273	    0.299	  0.008%	 83.479%	     0.000	        1	[densenet201/conv4_block46_concat/concat]:331
	                     MUL	         3056.633	   23.907	   23.839	  0.652%	 84.131%	     0.000	        1	[densenet201/conv4_block47_0_bn/FusedBatchNormV3]:332
	                     ADD	         3080.482	   32.374	   32.444	  0.887%	 85.018%	     0.000	        1	[densenet201/conv4_block47_0_relu/Relu;densenet201/conv4_block47_0_bn/FusedBatchNormV3]:333
	                 CONV_2D	         3112.936	    1.259	    1.288	  0.035%	 85.053%	     0.000	        1	[densenet201/conv4_block47_1_relu/Relu;densenet201/conv4_block47_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block47_1_conv/Conv2D]:334
	                 CONV_2D	         3114.232	    0.762	    0.777	  0.021%	 85.075%	     0.000	        1	[densenet201/conv4_block47_2_conv/Conv2D1]:335
	           CONCATENATION	         3115.018	    0.255	    0.290	  0.008%	 85.083%	     0.000	        1	[densenet201/conv4_block47_concat/concat]:336
	                     MUL	         3115.316	   24.178	   24.298	  0.664%	 85.747%	     0.000	        1	[densenet201/conv4_block48_0_bn/FusedBatchNormV3]:337
	                     ADD	         3139.625	   33.029	   33.059	  0.904%	 86.651%	     0.000	        1	[densenet201/conv4_block48_0_relu/Relu;densenet201/conv4_block48_0_bn/FusedBatchNormV3]:338
	                 CONV_2D	         3172.694	    1.285	    1.288	  0.035%	 86.686%	     0.000	        1	[densenet201/conv4_block48_1_relu/Relu;densenet201/conv4_block48_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block48_1_conv/Conv2D]:339
	                 CONV_2D	         3173.991	    0.747	    0.783	  0.021%	 86.707%	     0.000	        1	[densenet201/conv4_block48_2_conv/Conv2D1]:340
	           CONCATENATION	         3174.782	    0.260	    0.292	  0.008%	 86.715%	     0.000	        1	[densenet201/conv4_block48_concat/concat]:341
	                     MUL	         3175.081	   24.732	   24.662	  0.674%	 87.389%	     0.000	        1	[densenet201/pool4_bn/FusedBatchNormV3]:342
	                     ADD	         3199.753	   33.824	   33.622	  0.919%	 88.309%	     0.000	        1	[densenet201/pool4_relu/Relu;densenet201/pool4_bn/FusedBatchNormV3]:343
	                 CONV_2D	         3233.386	    7.677	    7.613	  0.208%	 88.517%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	         AVERAGE_POOL_2D	         3241.010	    4.935	    4.871	  0.133%	 88.650%	     0.000	        1	[densenet201/pool4_pool/AvgPool]:345
	                     MUL	         3245.889	    3.147	    3.106	  0.085%	 88.735%	     0.000	        1	[densenet201/conv5_block1_0_bn/FusedBatchNormV31]:346
	                     ADD	         3249.005	    4.306	    4.224	  0.115%	 88.850%	     0.000	        1	[densenet201/conv5_block1_0_relu/Relu;densenet201/conv5_block1_0_bn/FusedBatchNormV3]:347
	                 CONV_2D	         3253.238	    0.235	    0.220	  0.006%	 88.856%	     0.000	        1	[densenet201/conv5_block1_1_relu/Relu;densenet201/conv5_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block1_1_conv/Conv2D]:348
	                 CONV_2D	         3253.465	    0.196	    0.206	  0.006%	 88.862%	     0.000	        1	[densenet201/conv5_block1_2_conv/Conv2D1]:349
	           CONCATENATION	         3253.679	    0.047	    0.051	  0.001%	 88.863%	     0.000	        1	[densenet201/conv5_block1_concat/concat]:350
	                     MUL	         3253.736	    3.249	    3.207	  0.088%	 88.951%	     0.000	        1	[densenet201/conv5_block2_0_bn/FusedBatchNormV31]:351
	                     ADD	         3256.951	    4.492	    4.364	  0.119%	 89.070%	     0.000	        1	[densenet201/conv5_block2_0_relu/Relu;densenet201/conv5_block2_0_bn/FusedBatchNormV3]:352
	                 CONV_2D	         3261.323	    0.282	    0.189	  0.005%	 89.075%	     0.000	        1	[densenet201/conv5_block2_1_relu/Relu;densenet201/conv5_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block2_1_conv/Conv2D]:353
	                 CONV_2D	         3261.519	    0.270	    0.207	  0.006%	 89.081%	     0.000	        1	[densenet201/conv5_block2_2_conv/Conv2D1]:354
	           CONCATENATION	         3261.733	    0.069	    0.037	  0.001%	 89.082%	     0.000	        1	[densenet201/conv5_block2_concat/concat]:355
	                     MUL	         3261.777	    3.398	    3.316	  0.091%	 89.173%	     0.000	        1	[densenet201/conv5_block3_0_bn/FusedBatchNormV31]:356
	                     ADD	         3265.101	    4.577	    4.522	  0.124%	 89.296%	     0.000	        1	[densenet201/conv5_block3_0_relu/Relu;densenet201/conv5_block3_0_bn/FusedBatchNormV3]:357
	                 CONV_2D	         3269.632	    0.218	    0.188	  0.005%	 89.302%	     0.000	        1	[densenet201/conv5_block3_1_relu/Relu;densenet201/conv5_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block3_1_conv/Conv2D]:358
	                 CONV_2D	         3269.826	    0.256	    0.208	  0.006%	 89.307%	     0.000	        1	[densenet201/conv5_block3_2_conv/Conv2D1]:359
	           CONCATENATION	         3270.041	    0.068	    0.042	  0.001%	 89.308%	     0.000	        1	[densenet201/conv5_block3_concat/concat]:360
	                     MUL	         3270.090	    3.439	    3.427	  0.094%	 89.402%	     0.000	        1	[densenet201/conv5_block4_0_bn/FusedBatchNormV31]:361
	                     ADD	         3273.526	    4.864	    4.670	  0.128%	 89.530%	     0.000	        1	[densenet201/conv5_block4_0_relu/Relu;densenet201/conv5_block4_0_bn/FusedBatchNormV3]:362
	                 CONV_2D	         3278.205	    0.227	    0.193	  0.005%	 89.535%	     0.000	        1	[densenet201/conv5_block4_1_relu/Relu;densenet201/conv5_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block4_1_conv/Conv2D]:363
	                 CONV_2D	         3278.406	    0.226	    0.201	  0.005%	 89.540%	     0.000	        1	[densenet201/conv5_block4_2_conv/Conv2D1]:364
	           CONCATENATION	         3278.614	    0.059	    0.039	  0.001%	 89.542%	     0.000	        1	[densenet201/conv5_block4_concat/concat]:365
	                     MUL	         3278.660	    3.597	    3.542	  0.097%	 89.638%	     0.000	        1	[densenet201/conv5_block5_0_bn/FusedBatchNormV31]:366
	                     ADD	         3282.212	    4.894	    4.817	  0.132%	 89.770%	     0.000	        1	[densenet201/conv5_block5_0_relu/Relu;densenet201/conv5_block5_0_bn/FusedBatchNormV3]:367
	                 CONV_2D	         3287.037	    0.270	    0.194	  0.005%	 89.775%	     0.000	        1	[densenet201/conv5_block5_1_relu/Relu;densenet201/conv5_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block5_1_conv/Conv2D]:368
	                 CONV_2D	         3287.238	    0.206	    0.209	  0.006%	 89.781%	     0.000	        1	[densenet201/conv5_block5_2_conv/Conv2D1]:369
	           CONCATENATION	         3287.454	    0.085	    0.044	  0.001%	 89.782%	     0.000	        1	[densenet201/conv5_block5_concat/concat]:370
	                     MUL	         3287.504	    3.689	    3.644	  0.100%	 89.882%	     0.000	        1	[densenet201/conv5_block6_0_bn/FusedBatchNormV31]:371
	                     ADD	         3291.156	    5.018	    4.958	  0.136%	 90.017%	     0.000	        1	[densenet201/conv5_block6_0_relu/Relu;densenet201/conv5_block6_0_bn/FusedBatchNormV3]:372
	                 CONV_2D	         3296.122	    0.447	    0.411	  0.011%	 90.029%	     0.000	        1	[densenet201/conv5_block6_1_relu/Relu;densenet201/conv5_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block6_1_conv/Conv2D]:373
	                 CONV_2D	         3296.541	    0.274	    0.208	  0.006%	 90.034%	     0.000	        1	[densenet201/conv5_block6_2_conv/Conv2D1]:374
	           CONCATENATION	         3296.756	    0.068	    0.038	  0.001%	 90.035%	     0.000	        1	[densenet201/conv5_block6_concat/concat]:375
	                     MUL	         3296.802	    3.811	    3.759	  0.103%	 90.138%	     0.000	        1	[densenet201/conv5_block7_0_bn/FusedBatchNormV31]:376
	                     ADD	         3300.570	    5.239	    5.117	  0.140%	 90.278%	     0.000	        1	[densenet201/conv5_block7_0_relu/Relu;densenet201/conv5_block7_0_bn/FusedBatchNormV3]:377
	                 CONV_2D	         3305.696	    0.442	    0.409	  0.011%	 90.289%	     0.000	        1	[densenet201/conv5_block7_1_relu/Relu;densenet201/conv5_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block7_1_conv/Conv2D]:378
	                 CONV_2D	         3306.112	    0.262	    0.209	  0.006%	 90.295%	     0.000	        1	[densenet201/conv5_block7_2_conv/Conv2D1]:379
	           CONCATENATION	         3306.328	    0.067	    0.045	  0.001%	 90.296%	     0.000	        1	[densenet201/conv5_block7_concat/concat]:380
	                     MUL	         3306.380	    3.922	    3.868	  0.106%	 90.402%	     0.000	        1	[densenet201/conv5_block8_0_bn/FusedBatchNormV31]:381
	                     ADD	         3310.256	    5.311	    5.256	  0.144%	 90.546%	     0.000	        1	[densenet201/conv5_block8_0_relu/Relu;densenet201/conv5_block8_0_bn/FusedBatchNormV3]:382
	                 CONV_2D	         3315.522	    0.439	    0.418	  0.011%	 90.557%	     0.000	        1	[densenet201/conv5_block8_1_relu/Relu;densenet201/conv5_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block8_1_conv/Conv2D]:383
	                 CONV_2D	         3315.947	    0.229	    0.204	  0.006%	 90.563%	     0.000	        1	[densenet201/conv5_block8_2_conv/Conv2D1]:384
	           CONCATENATION	         3316.158	    0.052	    0.042	  0.001%	 90.564%	     0.000	        1	[densenet201/conv5_block8_concat/concat]:385
	                     MUL	         3316.207	    4.036	    3.980	  0.109%	 90.673%	     0.000	        1	[densenet201/conv5_block9_0_bn/FusedBatchNormV31]:386
	                     ADD	         3320.195	    5.500	    5.407	  0.148%	 90.820%	     0.000	        1	[densenet201/conv5_block9_0_relu/Relu;densenet201/conv5_block9_0_bn/FusedBatchNormV3]:387
	                 CONV_2D	         3325.611	    0.437	    0.412	  0.011%	 90.832%	     0.000	        1	[densenet201/conv5_block9_1_relu/Relu;densenet201/conv5_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block9_1_conv/Conv2D]:388
	                 CONV_2D	         3326.030	    0.234	    0.207	  0.006%	 90.837%	     0.000	        1	[densenet201/conv5_block9_2_conv/Conv2D1]:389
	           CONCATENATION	         3326.244	    0.053	    0.041	  0.001%	 90.838%	     0.000	        1	[densenet201/conv5_block9_concat/concat]:390
	                     MUL	         3326.291	    4.147	    4.093	  0.112%	 90.950%	     0.000	        1	[densenet201/conv5_block10_0_bn/FusedBatchNormV31]:391
	                     ADD	         3330.394	    5.628	    5.569	  0.152%	 91.103%	     0.000	        1	[densenet201/conv5_block10_0_relu/Relu;densenet201/conv5_block10_0_bn/FusedBatchNormV3]:392
	                 CONV_2D	         3335.972	    0.484	    0.406	  0.011%	 91.114%	     0.000	        1	[densenet201/conv5_block10_1_relu/Relu;densenet201/conv5_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block10_1_conv/Conv2D]:393
	                 CONV_2D	         3336.385	    0.268	    0.208	  0.006%	 91.119%	     0.000	        1	[densenet201/conv5_block10_2_conv/Conv2D1]:394
	           CONCATENATION	         3336.600	    0.074	    0.039	  0.001%	 91.120%	     0.000	        1	[densenet201/conv5_block10_concat/concat]:395
	                     MUL	         3336.646	    4.307	    4.200	  0.115%	 91.235%	     0.000	        1	[densenet201/conv5_block11_0_bn/FusedBatchNormV31]:396
	                     ADD	         3340.854	    5.788	    5.706	  0.156%	 91.391%	     0.000	        1	[densenet201/conv5_block11_0_relu/Relu;densenet201/conv5_block11_0_bn/FusedBatchNormV3]:397
	                 CONV_2D	         3346.568	    0.427	    0.400	  0.011%	 91.402%	     0.000	        1	[densenet201/conv5_block11_1_relu/Relu;densenet201/conv5_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block11_1_conv/Conv2D]:398
	                 CONV_2D	         3346.975	    0.234	    0.204	  0.006%	 91.408%	     0.000	        1	[densenet201/conv5_block11_2_conv/Conv2D1]:399
	           CONCATENATION	         3347.186	    0.056	    0.037	  0.001%	 91.409%	     0.000	        1	[densenet201/conv5_block11_concat/concat]:400
	                     MUL	         3347.229	    4.390	    4.312	  0.118%	 91.527%	     0.000	        1	[densenet201/conv5_block12_0_bn/FusedBatchNormV31]:401
	                     ADD	         3351.552	    5.977	    5.857	  0.160%	 91.687%	     0.000	        1	[densenet201/conv5_block12_0_relu/Relu;densenet201/conv5_block12_0_bn/FusedBatchNormV3]:402
	                 CONV_2D	         3357.417	    0.451	    0.401	  0.011%	 91.698%	     0.000	        1	[densenet201/conv5_block12_1_relu/Relu;densenet201/conv5_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block12_1_conv/Conv2D]:403
	                 CONV_2D	         3357.824	    0.234	    0.207	  0.006%	 91.703%	     0.000	        1	[densenet201/conv5_block12_2_conv/Conv2D1]:404
	           CONCATENATION	         3358.039	    0.057	    0.042	  0.001%	 91.704%	     0.000	        1	[densenet201/conv5_block12_concat/concat]:405
	                     MUL	         3358.087	    4.493	    4.417	  0.121%	 91.825%	     0.000	        1	[densenet201/conv5_block13_0_bn/FusedBatchNormV31]:406
	                     ADD	         3362.512	    6.107	    6.030	  0.165%	 91.990%	     0.000	        1	[densenet201/conv5_block13_0_relu/Relu;densenet201/conv5_block13_0_bn/FusedBatchNormV3]:407
	                 CONV_2D	         3368.552	    0.428	    0.404	  0.011%	 92.001%	     0.000	        1	[densenet201/conv5_block13_1_relu/Relu;densenet201/conv5_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block13_1_conv/Conv2D]:408
	                 CONV_2D	         3368.963	    0.241	    0.210	  0.006%	 92.007%	     0.000	        1	[densenet201/conv5_block13_2_conv/Conv2D1]:409
	           CONCATENATION	         3369.180	    0.060	    0.043	  0.001%	 92.008%	     0.000	        1	[densenet201/conv5_block13_concat/concat]:410
	                     MUL	         3369.230	    4.617	    4.540	  0.124%	 92.132%	     0.000	        1	[densenet201/conv5_block14_0_bn/FusedBatchNormV31]:411
	                     ADD	         3373.778	    6.314	    6.162	  0.168%	 92.301%	     0.000	        1	[densenet201/conv5_block14_0_relu/Relu;densenet201/conv5_block14_0_bn/FusedBatchNormV3]:412
	                 CONV_2D	         3379.949	    0.439	    0.389	  0.011%	 92.311%	     0.000	        1	[densenet201/conv5_block14_1_relu/Relu;densenet201/conv5_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block14_1_conv/Conv2D]:413
	                 CONV_2D	         3380.345	    0.273	    0.206	  0.006%	 92.317%	     0.000	        1	[densenet201/conv5_block14_2_conv/Conv2D1]:414
	           CONCATENATION	         3380.558	    0.061	    0.045	  0.001%	 92.318%	     0.000	        1	[densenet201/conv5_block14_concat/concat]:415
	                     MUL	         3380.610	    4.712	    4.629	  0.127%	 92.445%	     0.000	        1	[densenet201/conv5_block15_0_bn/FusedBatchNormV31]:416
	                     ADD	         3385.247	    6.410	    6.324	  0.173%	 92.618%	     0.000	        1	[densenet201/conv5_block15_0_relu/Relu;densenet201/conv5_block15_0_bn/FusedBatchNormV3]:417
	                 CONV_2D	         3391.580	    0.448	    0.397	  0.011%	 92.628%	     0.000	        1	[densenet201/conv5_block15_1_relu/Relu;densenet201/conv5_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block15_1_conv/Conv2D]:418
	                 CONV_2D	         3391.985	    0.252	    0.213	  0.006%	 92.634%	     0.000	        1	[densenet201/conv5_block15_2_conv/Conv2D1]:419
	           CONCATENATION	         3392.205	    0.061	    0.043	  0.001%	 92.635%	     0.000	        1	[densenet201/conv5_block15_concat/concat]:420
	                     MUL	         3392.254	    4.792	    4.743	  0.130%	 92.765%	     0.000	        1	[densenet201/conv5_block16_0_bn/FusedBatchNormV31]:421
	                     ADD	         3397.004	    6.597	    6.467	  0.177%	 92.942%	     0.000	        1	[densenet201/conv5_block16_0_relu/Relu;densenet201/conv5_block16_0_bn/FusedBatchNormV3]:422
	                 CONV_2D	         3403.480	    0.418	    0.411	  0.011%	 92.953%	     0.000	        1	[densenet201/conv5_block16_1_relu/Relu;densenet201/conv5_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block16_1_conv/Conv2D]:423
	                 CONV_2D	         3403.898	    0.264	    0.222	  0.006%	 92.959%	     0.000	        1	[densenet201/conv5_block16_2_conv/Conv2D1]:424
	           CONCATENATION	         3404.127	    0.063	    0.041	  0.001%	 92.960%	     0.000	        1	[densenet201/conv5_block16_concat/concat]:425
	                     MUL	         3404.175	    4.921	    4.866	  0.133%	 93.093%	     0.000	        1	[densenet201/conv5_block17_0_bn/FusedBatchNormV31]:426
	                     ADD	         3409.049	    6.642	    6.603	  0.181%	 93.274%	     0.000	        1	[densenet201/conv5_block17_0_relu/Relu;densenet201/conv5_block17_0_bn/FusedBatchNormV3]:427
	                 CONV_2D	         3415.661	    0.452	    0.405	  0.011%	 93.285%	     0.000	        1	[densenet201/conv5_block17_1_relu/Relu;densenet201/conv5_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block17_1_conv/Conv2D]:428
	                 CONV_2D	         3416.073	    0.254	    0.225	  0.006%	 93.291%	     0.000	        1	[densenet201/conv5_block17_2_conv/Conv2D1]:429
	           CONCATENATION	         3416.305	    0.057	    0.043	  0.001%	 93.292%	     0.000	        1	[densenet201/conv5_block17_concat/concat]:430
	                     MUL	         3416.355	    4.981	    4.958	  0.136%	 93.428%	     0.000	        1	[densenet201/conv5_block18_0_bn/FusedBatchNormV31]:431
	                     ADD	         3421.321	    6.801	    6.775	  0.185%	 93.613%	     0.000	        1	[densenet201/conv5_block18_0_relu/Relu;densenet201/conv5_block18_0_bn/FusedBatchNormV3]:432
	                 CONV_2D	         3428.105	    0.426	    0.387	  0.011%	 93.624%	     0.000	        1	[densenet201/conv5_block18_1_relu/Relu;densenet201/conv5_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block18_1_conv/Conv2D]:433
	                 CONV_2D	         3428.500	    0.212	    0.217	  0.006%	 93.630%	     0.000	        1	[densenet201/conv5_block18_2_conv/Conv2D1]:434
	           CONCATENATION	         3428.724	    0.065	    0.041	  0.001%	 93.631%	     0.000	        1	[densenet201/conv5_block18_concat/concat]:435
	                     MUL	         3428.771	    5.151	    5.075	  0.139%	 93.769%	     0.000	        1	[densenet201/conv5_block19_0_bn/FusedBatchNormV31]:436
	                     ADD	         3433.854	    7.051	    6.911	  0.189%	 93.958%	     0.000	        1	[densenet201/conv5_block19_0_relu/Relu;densenet201/conv5_block19_0_bn/FusedBatchNormV3]:437
	                 CONV_2D	         3440.774	    0.422	    0.389	  0.011%	 93.969%	     0.000	        1	[densenet201/conv5_block19_1_relu/Relu;densenet201/conv5_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block19_1_conv/Conv2D]:438
	                 CONV_2D	         3441.171	    0.203	    0.214	  0.006%	 93.975%	     0.000	        1	[densenet201/conv5_block19_2_conv/Conv2D1]:439
	           CONCATENATION	         3441.392	    0.064	    0.045	  0.001%	 93.976%	     0.000	        1	[densenet201/conv5_block19_concat/concat]:440
	                     MUL	         3441.445	    5.310	    5.191	  0.142%	 94.118%	     0.000	        1	[densenet201/conv5_block20_0_bn/FusedBatchNormV31]:441
	                     ADD	         3446.644	    7.178	    7.051	  0.193%	 94.311%	     0.000	        1	[densenet201/conv5_block20_0_relu/Relu;densenet201/conv5_block20_0_bn/FusedBatchNormV3]:442
	                 CONV_2D	         3453.704	    0.437	    0.393	  0.011%	 94.321%	     0.000	        1	[densenet201/conv5_block20_1_relu/Relu;densenet201/conv5_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block20_1_conv/Conv2D]:443
	                 CONV_2D	         3454.105	    0.258	    0.213	  0.006%	 94.327%	     0.000	        1	[densenet201/conv5_block20_2_conv/Conv2D1]:444
	           CONCATENATION	         3454.325	    0.093	    0.048	  0.001%	 94.329%	     0.000	        1	[densenet201/conv5_block20_concat/concat]:445
	                     MUL	         3454.381	    5.404	    5.295	  0.145%	 94.473%	     0.000	        1	[densenet201/conv5_block21_0_bn/FusedBatchNormV31]:446
	                     ADD	         3459.684	    7.378	    7.201	  0.197%	 94.670%	     0.000	        1	[densenet201/conv5_block21_0_relu/Relu;densenet201/conv5_block21_0_bn/FusedBatchNormV3]:447
	                 CONV_2D	         3466.894	    0.418	    0.393	  0.011%	 94.681%	     0.000	        1	[densenet201/conv5_block21_1_relu/Relu;densenet201/conv5_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block21_1_conv/Conv2D]:448
	                 CONV_2D	         3467.294	    0.254	    0.214	  0.006%	 94.687%	     0.000	        1	[densenet201/conv5_block21_2_conv/Conv2D1]:449
	           CONCATENATION	         3467.515	    0.073	    0.045	  0.001%	 94.688%	     0.000	        1	[densenet201/conv5_block21_concat/concat]:450
	                     MUL	         3467.567	    5.516	    5.406	  0.148%	 94.836%	     0.000	        1	[densenet201/conv5_block22_0_bn/FusedBatchNormV31]:451
	                     ADD	         3472.982	    7.455	    7.366	  0.201%	 95.037%	     0.000	        1	[densenet201/conv5_block22_0_relu/Relu;densenet201/conv5_block22_0_bn/FusedBatchNormV3]:452
	                 CONV_2D	         3480.357	    0.420	    0.393	  0.011%	 95.048%	     0.000	        1	[densenet201/conv5_block22_1_relu/Relu;densenet201/conv5_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block22_1_conv/Conv2D]:453
	                 CONV_2D	         3480.758	    0.314	    0.225	  0.006%	 95.054%	     0.000	        1	[densenet201/conv5_block22_2_conv/Conv2D1]:454
	           CONCATENATION	         3480.992	    0.073	    0.051	  0.001%	 95.056%	     0.000	        1	[densenet201/conv5_block22_concat/concat]:455
	                     MUL	         3481.050	    5.687	    5.520	  0.151%	 95.206%	     0.000	        1	[densenet201/conv5_block23_0_bn/FusedBatchNormV31]:456
	                     ADD	         3486.578	    7.673	    7.520	  0.206%	 95.412%	     0.000	        1	[densenet201/conv5_block23_0_relu/Relu;densenet201/conv5_block23_0_bn/FusedBatchNormV3]:457
	                 CONV_2D	         3494.107	    0.406	    0.383	  0.010%	 95.423%	     0.000	        1	[densenet201/conv5_block23_1_relu/Relu;densenet201/conv5_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block23_1_conv/Conv2D]:458
	                 CONV_2D	         3494.498	    0.222	    0.222	  0.006%	 95.429%	     0.000	        1	[densenet201/conv5_block23_2_conv/Conv2D1]:459
	           CONCATENATION	         3494.728	    0.056	    0.050	  0.001%	 95.430%	     0.000	        1	[densenet201/conv5_block23_concat/concat]:460
	                     MUL	         3494.785	    5.720	    5.632	  0.154%	 95.584%	     0.000	        1	[densenet201/conv5_block24_0_bn/FusedBatchNormV31]:461
	                     ADD	         3500.425	    7.666	    7.653	  0.209%	 95.793%	     0.000	        1	[densenet201/conv5_block24_0_relu/Relu;densenet201/conv5_block24_0_bn/FusedBatchNormV3]:462
	                 CONV_2D	         3508.086	    0.426	    0.383	  0.010%	 95.804%	     0.000	        1	[densenet201/conv5_block24_1_relu/Relu;densenet201/conv5_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block24_1_conv/Conv2D]:463
	                 CONV_2D	         3508.477	    0.228	    0.217	  0.006%	 95.810%	     0.000	        1	[densenet201/conv5_block24_2_conv/Conv2D1]:464
	           CONCATENATION	         3508.702	    0.076	    0.055	  0.002%	 95.811%	     0.000	        1	[densenet201/conv5_block24_concat/concat]:465
	                     MUL	         3508.764	    5.776	    5.742	  0.157%	 95.968%	     0.000	        1	[densenet201/conv5_block25_0_bn/FusedBatchNormV31]:466
	                     ADD	         3514.515	    7.885	    7.812	  0.214%	 96.182%	     0.000	        1	[densenet201/conv5_block25_0_relu/Relu;densenet201/conv5_block25_0_bn/FusedBatchNormV3]:467
	                 CONV_2D	         3522.336	    0.428	    0.391	  0.011%	 96.192%	     0.000	        1	[densenet201/conv5_block25_1_relu/Relu;densenet201/conv5_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block25_1_conv/Conv2D]:468
	                 CONV_2D	         3522.735	    0.265	    0.218	  0.006%	 96.198%	     0.000	        1	[densenet201/conv5_block25_2_conv/Conv2D1]:469
	           CONCATENATION	         3522.961	    0.093	    0.054	  0.001%	 96.200%	     0.000	        1	[densenet201/conv5_block25_concat/concat]:470
	                     MUL	         3523.024	    5.938	    5.867	  0.160%	 96.360%	     0.000	        1	[densenet201/conv5_block26_0_bn/FusedBatchNormV31]:471
	                     ADD	         3528.900	    8.107	    7.959	  0.218%	 96.578%	     0.000	        1	[densenet201/conv5_block26_0_relu/Relu;densenet201/conv5_block26_0_bn/FusedBatchNormV3]:472
	                 CONV_2D	         3536.868	    0.449	    0.379	  0.010%	 96.588%	     0.000	        1	[densenet201/conv5_block26_1_relu/Relu;densenet201/conv5_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block26_1_conv/Conv2D]:473
	                 CONV_2D	         3537.255	    0.207	    0.219	  0.006%	 96.594%	     0.000	        1	[densenet201/conv5_block26_2_conv/Conv2D1]:474
	           CONCATENATION	         3537.481	    0.075	    0.054	  0.001%	 96.596%	     0.000	        1	[densenet201/conv5_block26_concat/concat]:475
	                     MUL	         3537.541	    5.960	    5.957	  0.163%	 96.758%	     0.000	        1	[densenet201/conv5_block27_0_bn/FusedBatchNormV31]:476
	                     ADD	         3543.506	    8.199	    8.101	  0.221%	 96.980%	     0.000	        1	[densenet201/conv5_block27_0_relu/Relu;densenet201/conv5_block27_0_bn/FusedBatchNormV3]:477
	                 CONV_2D	         3551.616	    0.389	    0.372	  0.010%	 96.990%	     0.000	        1	[densenet201/conv5_block27_1_relu/Relu;densenet201/conv5_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block27_1_conv/Conv2D]:478
	                 CONV_2D	         3551.995	    0.224	    0.218	  0.006%	 96.996%	     0.000	        1	[densenet201/conv5_block27_2_conv/Conv2D1]:479
	           CONCATENATION	         3552.221	    0.064	    0.053	  0.001%	 96.997%	     0.000	        1	[densenet201/conv5_block27_concat/concat]:480
	                     MUL	         3552.281	    6.108	    6.066	  0.166%	 97.163%	     0.000	        1	[densenet201/conv5_block28_0_bn/FusedBatchNormV31]:481
	                     ADD	         3558.355	    8.323	    8.262	  0.226%	 97.389%	     0.000	        1	[densenet201/conv5_block28_0_relu/Relu;densenet201/conv5_block28_0_bn/FusedBatchNormV3]:482
	                 CONV_2D	         3566.626	    0.402	    0.382	  0.010%	 97.400%	     0.000	        1	[densenet201/conv5_block28_1_relu/Relu;densenet201/conv5_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block28_1_conv/Conv2D]:483
	                 CONV_2D	         3567.018	    0.229	    0.217	  0.006%	 97.406%	     0.000	        1	[densenet201/conv5_block28_2_conv/Conv2D1]:484
	           CONCATENATION	         3567.242	    0.078	    0.055	  0.002%	 97.407%	     0.000	        1	[densenet201/conv5_block28_concat/concat]:485
	                     MUL	         3567.304	    6.227	    6.171	  0.169%	 97.576%	     0.000	        1	[densenet201/conv5_block29_0_bn/FusedBatchNormV31]:486
	                     ADD	         3573.484	    8.473	    8.396	  0.230%	 97.805%	     0.000	        1	[densenet201/conv5_block29_0_relu/Relu;densenet201/conv5_block29_0_bn/FusedBatchNormV3]:487
	                 CONV_2D	         3581.888	    0.399	    0.370	  0.010%	 97.815%	     0.000	        1	[densenet201/conv5_block29_1_relu/Relu;densenet201/conv5_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block29_1_conv/Conv2D]:488
	                 CONV_2D	         3582.266	    0.210	    0.211	  0.006%	 97.821%	     0.000	        1	[densenet201/conv5_block29_2_conv/Conv2D1]:489
	           CONCATENATION	         3582.484	    0.067	    0.059	  0.002%	 97.823%	     0.000	        1	[densenet201/conv5_block29_concat/concat]:490
	                     MUL	         3582.550	    6.350	    6.325	  0.173%	 97.996%	     0.000	        1	[densenet201/conv5_block30_0_bn/FusedBatchNormV31]:491
	                     ADD	         3588.884	    8.605	    8.597	  0.235%	 98.231%	     0.000	        1	[densenet201/conv5_block30_0_relu/Relu;densenet201/conv5_block30_0_bn/FusedBatchNormV3]:492
	                 CONV_2D	         3597.490	    0.383	    0.361	  0.010%	 98.241%	     0.000	        1	[densenet201/conv5_block30_1_relu/Relu;densenet201/conv5_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block30_1_conv/Conv2D]:493
	                 CONV_2D	         3597.859	    0.221	    0.221	  0.006%	 98.247%	     0.000	        1	[densenet201/conv5_block30_2_conv/Conv2D1]:494
	           CONCATENATION	         3598.087	    0.063	    0.056	  0.002%	 98.248%	     0.000	        1	[densenet201/conv5_block30_concat/concat]:495
	                     MUL	         3598.150	    6.452	    6.397	  0.175%	 98.423%	     0.000	        1	[densenet201/conv5_block31_0_bn/FusedBatchNormV31]:496
	                     ADD	         3604.555	    8.771	    8.708	  0.238%	 98.661%	     0.000	        1	[densenet201/conv5_block31_0_relu/Relu;densenet201/conv5_block31_0_bn/FusedBatchNormV3]:497
	                 CONV_2D	         3613.272	    0.413	    0.380	  0.010%	 98.672%	     0.000	        1	[densenet201/conv5_block31_1_relu/Relu;densenet201/conv5_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block31_1_conv/Conv2D]:498
	                 CONV_2D	         3613.659	    0.219	    0.214	  0.006%	 98.677%	     0.000	        1	[densenet201/conv5_block31_2_conv/Conv2D1]:499
	           CONCATENATION	         3613.881	    0.062	    0.053	  0.001%	 98.679%	     0.000	        1	[densenet201/conv5_block31_concat/concat]:500
	                     MUL	         3613.941	    6.560	    6.503	  0.178%	 98.857%	     0.000	        1	[densenet201/conv5_block32_0_bn/FusedBatchNormV31]:501
	                     ADD	         3620.452	    8.992	    8.850	  0.242%	 99.099%	     0.000	        1	[densenet201/conv5_block32_0_relu/Relu;densenet201/conv5_block32_0_bn/FusedBatchNormV3]:502
	                 CONV_2D	         3629.311	    0.378	    0.360	  0.010%	 99.108%	     0.000	        1	[densenet201/conv5_block32_1_relu/Relu;densenet201/conv5_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block32_1_conv/Conv2D]:503
	                 CONV_2D	         3629.679	    0.222	    0.217	  0.006%	 99.114%	     0.000	        1	[densenet201/conv5_block32_2_conv/Conv2D210]:504
	           CONCATENATION	         3629.903	    0.079	    0.057	  0.002%	 99.116%	     0.000	        1	[densenet201/conv5_block32_concat/concat]:505
	                     MUL	         3629.966	    6.668	    6.614	  0.181%	 99.297%	     0.000	        1	[densenet201/bn/FusedBatchNormV31]:506
	                     ADD	         3636.588	    9.053	    8.985	  0.246%	 99.542%	     0.000	        1	[densenet201/relu/Relu;densenet201/bn/FusedBatchNormV3]:507
	                    MEAN	         3645.582	   16.222	   16.122	  0.441%	 99.983%	     0.000	        1	[densenet201/avg_pool/Mean]:508
	         FULLY_CONNECTED	         3661.713	    0.569	    0.530	  0.014%	 99.998%	     0.000	        1	[densenet201/predictions/MatMul;densenet201/predictions/BiasAdd]:509
	                 SOFTMAX	         3662.251	    0.084	    0.087	  0.002%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:510

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     ADD	          759.956	   77.702	   77.022	  2.106%	  2.106%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                     ADD	          606.264	   67.555	   67.822	  1.854%	  3.960%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                     ADD	          469.296	   57.924	   58.226	  1.592%	  5.552%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                     MUL	          703.023	   57.097	   56.922	  1.556%	  7.108%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     MUL	          556.255	   50.054	   49.997	  1.367%	  8.475%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	          349.017	   48.299	   48.649	  1.330%	  9.805%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                 CONV_2D	            3.748	   44.185	   44.069	  1.205%	 11.009%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                     MUL	          426.332	   42.605	   42.952	  1.174%	 12.184%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	          245.202	   38.693	   38.972	  1.065%	 13.249%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                     ADD	         1450.350	   38.480	   38.368	  1.049%	 14.298%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99

Number of nodes executed: 511
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                     ADD	      102	  1813.067	    49.569%	    49.569%	     0.000	      102
	                     MUL	      102	  1335.436	    36.511%	    86.080%	     0.000	      102
	                 CONV_2D	      200	   425.160	    11.624%	    97.704%	     0.000	      200
	                     PAD	        2	    22.011	     0.602%	    98.306%	     0.000	        2
	         AVERAGE_POOL_2D	        3	    21.444	     0.586%	    98.892%	     0.000	        3
	           CONCATENATION	       98	    18.553	     0.507%	    99.399%	     0.000	       98
	                    MEAN	        1	    16.122	     0.441%	    99.840%	     0.000	        1
	             MAX_POOL_2D	        1	     5.229	     0.143%	    99.983%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.529	     0.014%	    99.998%	     0.000	        1
	                 SOFTMAX	        1	     0.086	     0.002%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=3655706 curr=3662018 min=3649856 max=3679735 avg=3.65788e+06 std=6485
Memory (bytes): count=0
511 nodes observed



[ perf record: Woken up 310 times to write data ]
[ perf record: Captured and wrote 77.837 MB /tmp/data.record (439651 samples) ]

111.672

