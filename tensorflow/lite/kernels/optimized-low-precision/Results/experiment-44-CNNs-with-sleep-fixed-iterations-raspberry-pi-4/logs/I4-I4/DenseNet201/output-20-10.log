STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/f32i8/DenseNet201.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/f32i8/DenseNet201.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 80)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (3136, 64, ), and Output shape (3136, 128, ), and the ID is 1
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
(3136, 32, ), and the ID is 2
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 96, ), Input shape (3136, 96, ), and Output shape (3136, 128, ), and the ID is 3
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 48)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 48)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 4
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (3136, 128, ), and Output shape (3136, 128, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 64)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 6
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (3136, 160, ), and Output shape (3136, 128, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 80)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
, and the ID is 8
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (3136, 192, ), and Output shape (3136, 128, ), and the ID is 9
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 10
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (3136, 224, ), and Output shape (3136, 128, ), and the ID is 11
	Allocating LowPrecision Activations Tensors with Shape of (3136, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 32, ), and the ID is 12	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (3136, 256, ), and Output shape (3136, 128, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 128)
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (784, 128, ), and Output shape (784, 128, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
(784, 32, ), and the ID is 15
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 160, ), Input shape (784, 160, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 80)
	Allocating LowPrecision Activations Tensors with Shape of (784, 80)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 192, ), Input shape (784, 192, ), and Output shape (784, 128, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 96)
	Allocating LowPrecision Activations Tensors with Shape of (784, 96)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 224, ), Input shape (784, 224, ), and Output shape (784, 128, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 112)
	Allocating LowPrecision Activations Tensors with Shape of (784, 112)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 21	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 22
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (784, 128)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (784, 288, ), and Output shape (784, 128, ), and the ID is 24
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 25
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (784, 320, ), and Output shape (784, 128, ), and the ID is 26
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (784, 160)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 27
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (784, 352, ), and Output shape (784, 128, ), and the ID is 28
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (784, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (784, 384, ), and Output shape (784, 128, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (784, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 31
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (784, 416, ), and Output shape (784, 128, ), and the ID is 32
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 33
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (784, 448, ), and Output shape (784, 128, ), and the ID is 34
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 35
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (784, 480, ), and Output shape (784, 128, ), and the ID is 36
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (784, 128, ), and Output shape (784, 32, ), and the ID is 37
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (784, 512, ), and Output shape (784, 256, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (784, 208)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (784, 224)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
	Allocating LowPrecision Activations Tensors with Shape of (784, 240)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 256)
	Allocating LowPrecision Activations Tensors with Shape of (784, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (196, 256, ), and Output shape (196, 128, ), and the ID is 39
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 40
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
Applying Conv Low-Precision for Kernel shape (128, 288, ), Input shape (196, 288, ), and Output shape (196, 128, ), and the ID is 41
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 42
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 160)
	Allocating LowPrecision Activations Tensors with Shape of (196, 160)
Applying Conv Low-Precision for Kernel shape (128, 320, ), Input shape (196, 320, ), and Output shape (196, 128, ), and the ID is 43
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
(196, 32, ), and the ID is 44
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 352, ), Input shape (196, 352, ), and Output shape (196, 128, ), and the ID is 45
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 176)
	Allocating LowPrecision Activations Tensors with Shape of (196, 176)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 46
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 384, ), Input shape (196, 384, ), and Output shape (196, 128, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 192)
	Allocating LowPrecision Activations Tensors with Shape of (196, 192)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 48
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 416, ), Input shape (196, 416, ), and Output shape (196, 128, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 208)
	Allocating LowPrecision Activations Tensors with Shape of (196, 208)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 50
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 448, ), Input shape (196, 448, ), and Output shape (196, 128, ), and the ID is 51
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 224)
	Allocating LowPrecision Activations Tensors with Shape of (196, 224)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 52
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 480, ), Input shape (196, 480, ), and Output shape (196, 128, ), and the ID is 53
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 240)
	Allocating LowPrecision Activations Tensors with Shape of (196, 240)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 54
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (196, 512, ), and Output shape (196, 128, ), and the ID is 55
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 256)
	Allocating LowPrecision Activations Tensors with Shape of (196, 256)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 56
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 544, ), Input shape (196, 544, ), and Output shape (196, 128, ), and the ID is 57
	Allocating LowPrecision Weight Tensors with Shape of (128, 272)
	Allocating LowPrecision Activations Tensors with Shape of (196, 272)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 58
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (196, 576, ), and Output shape (196, 128, ), and the ID is 59
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 60
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 608, ), Input shape (196, 608, ), and Output shape (196, 128, ), and the ID is 61
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 304)
	Allocating LowPrecision Activations Tensors with Shape of (196, 304)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 62
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 640, ), Input shape (196, 640, ), and Output shape (196, 128, ), and the ID is 63
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 320)
	Allocating LowPrecision Activations Tensors with Shape of (196, 320)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
, and the ID is 64
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 336)
Applying Conv Low-Precision for Kernel shape (128, 672, ), Input shape (196, 672, ), and Output shape (196, 128, ), and the ID is 65
	Allocating LowPrecision Activations Tensors with Shape of (196, 336)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 66
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 704, ), Input shape (196, 704, ), and Output shape (196, 128, ), and the ID is 67	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 352)

	Allocating LowPrecision Activations Tensors with Shape of (196, 352)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 68
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 736, ), Input shape (196, 736, ), and Output shape (196, 128, ), and the ID is 69
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 368)
	Allocating LowPrecision Activations Tensors with Shape of (196, 368)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 70	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 768, ), Input shape (196, 768, ), and Output shape (196, 128, ), and the ID is 71
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 384)
	Allocating LowPrecision Activations Tensors with Shape of (196, 384)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
72
Applying Conv Low-Precision for Kernel shape (128, 800, ), Input shape (196, 800, ), and Output shape (196, 128, ), and the ID is 73	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 400)

	Allocating LowPrecision Activations Tensors with Shape of (196, 400)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 74	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 832, ), Input shape (196, 832, ), and Output shape (196, 128, ), and the ID is 75
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 416)
	Allocating LowPrecision Activations Tensors with Shape of (196, 416)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 76
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 864, ), Input shape (196, 864, ), and Output shape (196, 128, ), and the ID is 77
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 432)
	Allocating LowPrecision Activations Tensors with Shape of (196, 432)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 78
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (196, 896, ), and Output shape (196, 128, ), and the ID is 79
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (196, 448)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 80
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (196, 928, ), and Output shape (196, 128, ), and the ID is 81
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 464)
	Allocating LowPrecision Activations Tensors with Shape of (196, 464)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 82
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (196, 960, ), and Output shape (196, 128, ), and the ID is 83
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (196, 480)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 84
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (196, 992, ), and Output shape (196, 128, ), and the ID is 85
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 496)
	Allocating LowPrecision Activations Tensors with Shape of (196, 496)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 86
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (196, 1024, ), and Output shape (196, 128, ), and the ID is 87
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)
	Allocating LowPrecision Activations Tensors with Shape of (196, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 88
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (196, 1056, ), and Output shape (196, 128, ), and the ID is 89
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 528)
	Allocating LowPrecision Activations Tensors with Shape of (196, 528)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 90
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (196, 1088, ), and Output shape (196, 128, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 544)
	Allocating LowPrecision Activations Tensors with Shape of (196, 544)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 92
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (196, 1120, ), and Output shape (196, 128, ), and the ID is 93
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 560)
	Allocating LowPrecision Activations Tensors with Shape of (196, 560)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 94
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (196, 1152, ), and Output shape (196, 128, ), and the ID is 95
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 96
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (196, 1184, ), and Output shape (196, 128, ), and the ID is 97
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 592)
	Allocating LowPrecision Activations Tensors with Shape of (196, 592)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
, and the ID is 98
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (196, 1216, ), and Output shape (196, 128, ), and the ID is 99
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 608)
	Allocating LowPrecision Activations Tensors with Shape of (196, 608)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 100
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (196, 1248, ), and Output shape (196, 128, ), and the ID is 101
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 624)
	Allocating LowPrecision Activations Tensors with Shape of (196, 624)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 102
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (196, 1280, ), and Output shape (196, 128, ), and the ID is 103	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 640)

	Allocating LowPrecision Activations Tensors with Shape of (196, 640)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 104
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (196, 1312, ), and Output shape (196, 128, ), and the ID is 105
	Allocating LowPrecision Weight Tensors with Shape of (128, 656)
	Allocating LowPrecision Activations Tensors with Shape of (196, 656)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 106
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (196, 1344, ), and Output shape (196, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 672)
, and the ID is 107
	Allocating LowPrecision Activations Tensors with Shape of (196, 672)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
, and the ID is 108
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (196, 1376, ), and Output shape (196, 128, ), and the ID is 109
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 688)
	Allocating LowPrecision Activations Tensors with Shape of (196, 688)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 110
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (196, 1408, ), and Output shape (196, 128, ), and the ID is 111	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 704)

	Allocating LowPrecision Activations Tensors with Shape of (196, 704)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 112
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (196, 1440, ), and Output shape (196, 128, ), and the ID is 113
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 720)
	Allocating LowPrecision Activations Tensors with Shape of (196, 720)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 114
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (196, 1472, ), and Output shape (196, 128, ), and the ID is 115
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 736)
	Allocating LowPrecision Activations Tensors with Shape of (196, 736)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 116
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (196, 1504, ), and Output shape (196, 128, ), and the ID is 117
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 752)
	Allocating LowPrecision Activations Tensors with Shape of (196, 752)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 118
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (196, 1536, ), and Output shape (196, 128, ), and the ID is 119
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 768)
	Allocating LowPrecision Activations Tensors with Shape of (196, 768)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 120
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (196, 1568, ), and Output shape (196, 128, ), and the ID is 121
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 784)
	Allocating LowPrecision Activations Tensors with Shape of (196, 784)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 122
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (196, 1600, ), and Output shape (196, 128, ), and the ID is 123
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 800)
	Allocating LowPrecision Activations Tensors with Shape of (196, 800)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 124
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 816)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (196, 1632, ), and Output shape (196, 128, ), and the ID is 125
	Allocating LowPrecision Activations Tensors with Shape of (196, 816)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 126
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (196, 1664, ), and Output shape (196, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 832)
127
	Allocating LowPrecision Activations Tensors with Shape of (196, 832)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 128
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (196, 1696, ), and Output shape (196, 128, ), and the ID is 129
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 848)
	Allocating LowPrecision Activations Tensors with Shape of (196, 848)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 130
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (196, 1728, ), and Output shape (196, 128, ), and the ID is 131
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 864)
	Allocating LowPrecision Activations Tensors with Shape of (196, 864)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 132
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (196, 1760, ), and Output shape (196, 128, ), and the ID is 133
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 880)
	Allocating LowPrecision Activations Tensors with Shape of (196, 880)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (196, 128, ), and Output shape (196, 32, ), and the ID is 134
	Allocating LowPrecision Activations Tensors with Shape of (196, 576)
Applying Conv Low-Precision for Kernel shape (896, 1792, ), Input shape (196, 1792, ), and Output shape (196, 896, ), and the ID is 135
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (896, 896)
	Allocating LowPrecision Activations Tensors with Shape of (196, 896)
Applying Conv Low-Precision for Kernel shape (128, 896, ), Input shape (49, 896, ), and Output shape (49, 128, ), and the ID is 136
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 448)
	Allocating LowPrecision Activations Tensors with Shape of (52, 448)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 137
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 928, ), Input shape (49, 928, ), and Output shape (49, 128, ), and the ID is 138
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 464)
	Allocating LowPrecision Activations Tensors with Shape of (52, 464)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 139
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 960, ), Input shape (49, 960, ), and Output shape (49, 128, ), and the ID is 140
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 480)
	Allocating LowPrecision Activations Tensors with Shape of (52, 480)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 141
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 496)
Applying Conv Low-Precision for Kernel shape (128, 992, ), Input shape (49, 992, ), and Output shape (49, 128, ), and the ID is 142
	Allocating LowPrecision Activations Tensors with Shape of (52, 496)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 143
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1024, ), Input shape (49, 1024, ), and Output shape (49, 128, ), and the ID is 144	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 512)

	Allocating LowPrecision Activations Tensors with Shape of (52, 512)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 145
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1056, ), Input shape (49, 1056, ), and Output shape (49, 128, ), and the ID is 146
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 528)
	Allocating LowPrecision Activations Tensors with Shape of (52, 528)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 147
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1088, ), Input shape (49, 1088, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 544)
, and the ID is 148
	Allocating LowPrecision Activations Tensors with Shape of (52, 544)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
149
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1120, ), Input shape (49, 1120, ), and Output shape (49, 128, ), and the ID is 150
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 560)
	Allocating LowPrecision Activations Tensors with Shape of (52, 560)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 151
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (49, 1152, ), and Output shape (49, 128, ), and the ID is 152
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 153
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1184, ), Input shape (49, 1184, ), and Output shape (49, 128, ), and the ID is 154
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 592)
	Allocating LowPrecision Activations Tensors with Shape of (52, 592)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 155
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 608)
Applying Conv Low-Precision for Kernel shape (128, 1216, ), Input shape (49, 1216, ), and Output shape (49, 128, ), and the ID is 156
	Allocating LowPrecision Activations Tensors with Shape of (52, 608)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 157
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1248, ), Input shape (49, 1248, ), and Output shape (49, 128, ), and the ID is 158
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 624)
	Allocating LowPrecision Activations Tensors with Shape of (52, 624)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 159
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 640)
Applying Conv Low-Precision for Kernel shape (128, 1280, ), Input shape (49, 1280, ), and Output shape (49, 128, ), and the ID is 160
	Allocating LowPrecision Activations Tensors with Shape of (52, 640)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 161
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1312, ), Input shape (49, 1312, ), and Output shape (49, 128, ), and the ID is 162
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 656)
	Allocating LowPrecision Activations Tensors with Shape of (52, 656)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 163
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1344, ), Input shape (49, 1344, ), and Output shape (49, 128, ), and the ID is 164
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 672)
	Allocating LowPrecision Activations Tensors with Shape of (52, 672)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 165
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1376, ), Input shape (49, 1376, ), and Output shape (49, 128, ), and the ID is 166
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 688)
	Allocating LowPrecision Activations Tensors with Shape of (52, 688)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 167
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1408, ), Input shape (49, 1408, ), and Output shape (49, 128, ), and the ID is 168
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 704)
	Allocating LowPrecision Activations Tensors with Shape of (52, 704)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 169
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1440, ), Input shape (49, 1440, ), and Output shape (49, 128, ), and the ID is 170
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 720)
	Allocating LowPrecision Activations Tensors with Shape of (52, 720)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 171
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1472, ), Input shape (49, 1472, ), and Output shape (49, 128, ), and the ID is 172
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 736)
	Allocating LowPrecision Activations Tensors with Shape of (52, 736)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 173
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1504, ), Input shape (49, 1504, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 752)
174
	Allocating LowPrecision Activations Tensors with Shape of (52, 752)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 175
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1536, ), Input shape (49, 1536, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 768)
176
	Allocating LowPrecision Activations Tensors with Shape of (52, 768)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 177	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)

	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1568, ), Input shape (49, 1568, ), and Output shape (49, 128, ), and the ID is 178
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 784)
	Allocating LowPrecision Activations Tensors with Shape of (52, 784)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 179
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1600, ), Input shape (49, 1600, ), and Output shape (49, 128, ), and the ID is 180
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 800)
	Allocating LowPrecision Activations Tensors with Shape of (52, 800)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 181
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1632, ), Input shape (49, 1632, ), and Output shape (49, 128, ), and the ID is 182
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 816)
	Allocating LowPrecision Activations Tensors with Shape of (52, 816)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 183
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1664, ), Input shape (49, 1664, ), and Output shape (49, 128, ), and the ID is 184
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 832)
	Allocating LowPrecision Activations Tensors with Shape of (52, 832)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 185
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1696, ), Input shape (49, 1696, ), and Output shape (49, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 848)
, and the ID is 186
	Allocating LowPrecision Activations Tensors with Shape of (52, 848)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 187
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1728, ), Input shape (49, 1728, ), and Output shape (49, 128, ), and the ID is 188	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 864)

	Allocating LowPrecision Activations Tensors with Shape of (52, 864)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 189
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1760, ), Input shape (49, 1760, ), and Output shape (49, 128, ), and the ID is 190
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 880)
	Allocating LowPrecision Activations Tensors with Shape of (52, 880)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 191
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 896)
Applying Conv Low-Precision for Kernel shape (128, 1792, ), Input shape (49, 1792, ), and Output shape (49, 128, ), and the ID is 192
	Allocating LowPrecision Activations Tensors with Shape of (52, 896)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 193
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1824, ), Input shape (49, 1824, ), and Output shape (49, 128, ), and the ID is 194
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 912)
	Allocating LowPrecision Activations Tensors with Shape of (52, 912)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 195
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1856, ), Input shape (49, 1856, ), and Output shape (49, 128, ), and the ID is 196
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 928)
	Allocating LowPrecision Activations Tensors with Shape of (52, 928)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 197
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Conv Low-Precision for Kernel shape (128, 1888, ), Input shape (49, 1888, ), and Output shape (49, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 944)
198
	Allocating LowPrecision Activations Tensors with Shape of (52, 944)
Applying Conv Low-Precision for Kernel shape (32, 1152, ), Input shape (49, 128, ), and Output shape (49, 32, ), and the ID is 199
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (32, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
Applying Low-Precision for shape (1000, 1920, ) and Input shape (1, 1920, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 960)
	Transformed Activation Shape From: (1, 1920) To: (1, 960)
The input model file size (MB): 20.6146
Initialized session in 246.388ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=4390854 curr=4288694 min=4287378 max=4390854 avg=4.30006e+06 std=30466

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=4305856 curr=4297677 min=4297324 max=4305856 avg=4.29937e+06 std=1745

Inference timings in us: Init: 246388, First inference: 4390854, Warmup (avg): 4.30006e+06, Inference (avg): 4.29937e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=28.9648 overall=43.7734
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  230.607	  230.607	100.000%	100.000%	 17804.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  230.607	  230.607	100.000%	100.000%	 17804.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   230.607	   100.000%	   100.000%	 17804.000	        1

Timings (microseconds): count=1 curr=230607
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.027	    0.436	    0.417	  0.010%	  0.010%	     0.000	        1	[densenet201/zero_padding2d/Pad]:0
	                 CONV_2D	            0.453	  355.301	  355.142	  8.270%	  8.279%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                     PAD	          355.608	    1.596	    1.623	  0.038%	  8.317%	     0.000	        1	[densenet201/zero_padding2d_1/Pad]:2
	             MAX_POOL_2D	          357.241	  109.025	  108.815	  2.534%	 10.851%	     0.000	        1	[densenet201/pool1/MaxPool]:3
	                     MUL	          466.068	    2.198	    2.083	  0.049%	 10.900%	     0.000	        1	[densenet201/conv2_block1_0_bn/FusedBatchNormV31]:4
	                     ADD	          468.162	    2.025	    1.976	  0.046%	 10.946%	     0.000	        1	[densenet201/conv2_block1_0_relu/Relu;densenet201/conv2_block1_0_bn/FusedBatchNormV3]:5
	                 CONV_2D	          470.148	  150.732	  150.892	  3.514%	 14.459%	     0.000	        1	[densenet201/conv2_block1_1_relu/Relu;densenet201/conv2_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block1_1_conv/Conv2D]:6
	                 CONV_2D	          621.053	   71.817	   71.932	  1.675%	 16.134%	     0.000	        1	[densenet201/conv2_block1_2_conv/Conv2D1]:7
	           CONCATENATION	          692.997	    1.158	    1.120	  0.026%	 16.160%	     0.000	        1	[densenet201/conv2_block1_concat/concat]:8
	                     MUL	          694.128	    3.164	    3.179	  0.074%	 16.234%	     0.000	        1	[densenet201/conv2_block2_0_bn/FusedBatchNormV31]:9
	                     ADD	          697.317	    2.869	    2.880	  0.067%	 16.301%	     0.000	        1	[densenet201/conv2_block2_0_relu/Relu;densenet201/conv2_block2_0_bn/FusedBatchNormV3]:10
	                 CONV_2D	          700.207	  152.845	  152.870	  3.560%	 19.861%	     0.000	        1	[densenet201/conv2_block2_1_relu/Relu;densenet201/conv2_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block2_1_conv/Conv2D]:11
	                 CONV_2D	          853.090	   71.795	   71.874	  1.674%	 21.535%	     0.000	        1	[densenet201/conv2_block2_2_conv/Conv2D1]:12
	           CONCATENATION	          924.976	    1.668	    1.562	  0.036%	 21.571%	     0.000	        1	[densenet201/conv2_block2_concat/concat]:13
	                     MUL	          926.549	    4.242	    4.264	  0.099%	 21.670%	     0.000	        1	[densenet201/conv2_block3_0_bn/FusedBatchNormV3]:14
	                     ADD	          930.825	    3.835	    3.837	  0.089%	 21.760%	     0.000	        1	[densenet201/conv2_block3_0_relu/Relu;densenet201/conv2_block3_0_bn/FusedBatchNormV3]:15
	                 CONV_2D	          934.674	  152.102	  151.975	  3.539%	 25.298%	     0.000	        1	[densenet201/conv2_block3_1_relu/Relu;densenet201/conv2_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block3_1_conv/Conv2D]:16
	                 CONV_2D	         1086.661	   71.831	   71.881	  1.674%	 26.972%	     0.000	        1	[densenet201/conv2_block3_2_conv/Conv2D1]:17
	           CONCATENATION	         1158.554	    1.874	    1.915	  0.045%	 27.017%	     0.000	        1	[densenet201/conv2_block3_concat/concat]:18
	                     MUL	         1160.480	    5.206	    5.275	  0.123%	 27.140%	     0.000	        1	[densenet201/conv2_block4_0_bn/FusedBatchNormV3]:19
	                     ADD	         1165.767	    4.843	    4.782	  0.111%	 27.251%	     0.000	        1	[densenet201/conv2_block4_0_relu/Relu;densenet201/conv2_block4_0_bn/FusedBatchNormV3]:20
	                 CONV_2D	         1170.561	  158.187	  156.327	  3.640%	 30.891%	     0.000	        1	[densenet201/conv2_block4_1_relu/Relu;densenet201/conv2_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block4_1_conv/Conv2D]:21
	                 CONV_2D	         1326.900	   72.774	   71.951	  1.675%	 32.567%	     0.000	        1	[densenet201/conv2_block4_2_conv/Conv2D1]:22
	           CONCATENATION	         1398.863	    2.243	    2.293	  0.053%	 32.620%	     0.000	        1	[densenet201/conv2_block4_concat/concat]:23
	                     MUL	         1401.167	    5.785	    5.795	  0.135%	 32.755%	     0.000	        1	[densenet201/conv2_block5_0_bn/FusedBatchNormV3]:24
	                     ADD	         1406.975	    5.584	    5.600	  0.130%	 32.885%	     0.000	        1	[densenet201/conv2_block5_0_relu/Relu;densenet201/conv2_block5_0_bn/FusedBatchNormV3]:25
	                 CONV_2D	         1412.587	  163.069	  161.013	  3.749%	 36.635%	     0.000	        1	[densenet201/conv2_block5_1_relu/Relu;densenet201/conv2_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block5_1_conv/Conv2D]:26
	                 CONV_2D	         1573.612	   73.069	   72.999	  1.700%	 38.334%	     0.000	        1	[densenet201/conv2_block5_2_conv/Conv2D1]:27
	           CONCATENATION	         1646.623	    2.465	    2.545	  0.059%	 38.394%	     0.000	        1	[densenet201/conv2_block5_concat/concat]:28
	                     MUL	         1649.186	    6.698	    6.699	  0.156%	 38.550%	     0.000	        1	[densenet201/conv2_block6_0_bn/FusedBatchNormV3]:29
	                     ADD	         1655.897	    6.717	    6.647	  0.155%	 38.704%	     0.000	        1	[densenet201/conv2_block6_0_relu/Relu;densenet201/conv2_block6_0_bn/FusedBatchNormV3]:30
	                 CONV_2D	         1662.556	  161.948	  160.399	  3.735%	 42.439%	     0.000	        1	[densenet201/conv2_block6_1_relu/Relu;densenet201/conv2_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block6_1_conv/Conv2D]:31
	                 CONV_2D	         1822.967	   73.351	   73.437	  1.710%	 44.149%	     0.000	        1	[densenet201/conv2_block6_2_conv/Conv2D1]:32
	           CONCATENATION	         1896.415	    3.016	    2.995	  0.070%	 44.219%	     0.000	        1	[densenet201/conv2_block6_concat/concat]:33
	                     MUL	         1899.422	    8.471	    8.381	  0.195%	 44.414%	     0.000	        1	[densenet201/pool2_bn/FusedBatchNormV3]:34
	                     ADD	         1907.816	    7.397	    7.489	  0.174%	 44.589%	     0.000	        1	[densenet201/pool2_relu/Relu;densenet201/pool2_bn/FusedBatchNormV3]:35
	                 CONV_2D	         1915.317	  159.911	  159.769	  3.720%	 48.309%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	         AVERAGE_POOL_2D	         2075.098	   27.793	   27.848	  0.648%	 48.958%	     0.000	        1	[densenet201/pool2_pool/AvgPool]:37
	                     MUL	         2102.957	    1.088	    1.076	  0.025%	 48.983%	     0.000	        1	[densenet201/conv3_block1_0_bn/FusedBatchNormV3]:38
	                     ADD	         2104.043	    0.955	    0.980	  0.023%	 49.005%	     0.000	        1	[densenet201/conv3_block1_0_relu/Relu;densenet201/conv3_block1_0_bn/FusedBatchNormV3]:39
	                 CONV_2D	         2105.031	   38.544	   38.563	  0.898%	 49.903%	     0.000	        1	[densenet201/conv3_block1_1_relu/Relu;densenet201/conv3_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block1_1_conv/Conv2D]:40
	                 CONV_2D	         2143.609	   18.045	   18.055	  0.420%	 50.324%	     0.000	        1	[densenet201/conv3_block1_2_conv/Conv2D1]:41
	           CONCATENATION	         2161.676	    0.523	    0.504	  0.012%	 50.336%	     0.000	        1	[densenet201/conv3_block1_concat/concat]:42
	                     MUL	         2162.191	    1.332	    1.347	  0.031%	 50.367%	     0.000	        1	[densenet201/conv3_block2_0_bn/FusedBatchNormV31]:43
	                     ADD	         2163.546	    1.317	    1.229	  0.029%	 50.396%	     0.000	        1	[densenet201/conv3_block2_0_relu/Relu;densenet201/conv3_block2_0_bn/FusedBatchNormV3]:44
	                 CONV_2D	         2164.785	   39.532	   39.494	  0.920%	 51.315%	     0.000	        1	[densenet201/conv3_block2_1_relu/Relu;densenet201/conv3_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block2_1_conv/Conv2D]:45
	                 CONV_2D	         2204.290	   17.921	   17.949	  0.418%	 51.733%	     0.000	        1	[densenet201/conv3_block2_2_conv/Conv2D1]:46
	           CONCATENATION	         2222.251	    0.650	    0.636	  0.015%	 51.748%	     0.000	        1	[densenet201/conv3_block2_concat/concat]:47
	                     MUL	         2222.898	    1.524	    1.488	  0.035%	 51.783%	     0.000	        1	[densenet201/conv3_block3_0_bn/FusedBatchNormV31]:48
	                     ADD	         2224.395	    1.411	    1.423	  0.033%	 51.816%	     0.000	        1	[densenet201/conv3_block3_0_relu/Relu;densenet201/conv3_block3_0_bn/FusedBatchNormV3]:49
	                 CONV_2D	         2225.827	   39.530	   39.507	  0.920%	 52.736%	     0.000	        1	[densenet201/conv3_block3_1_relu/Relu;densenet201/conv3_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block3_1_conv/Conv2D]:50
	                 CONV_2D	         2265.345	   18.032	   18.064	  0.421%	 53.156%	     0.000	        1	[densenet201/conv3_block3_2_conv/Conv2D1]:51
	           CONCATENATION	         2283.421	    0.793	    0.723	  0.017%	 53.173%	     0.000	        1	[densenet201/conv3_block3_concat/concat]:52
	                     MUL	         2284.154	    1.678	    1.700	  0.040%	 53.213%	     0.000	        1	[densenet201/conv3_block4_0_bn/FusedBatchNormV31]:53
	                     ADD	         2285.864	    1.754	    1.678	  0.039%	 53.252%	     0.000	        1	[densenet201/conv3_block4_0_relu/Relu;densenet201/conv3_block4_0_bn/FusedBatchNormV3]:54
	                 CONV_2D	         2287.551	   40.315	   40.329	  0.939%	 54.191%	     0.000	        1	[densenet201/conv3_block4_1_relu/Relu;densenet201/conv3_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block4_1_conv/Conv2D]:55
	                 CONV_2D	         2327.892	   17.925	   17.966	  0.418%	 54.609%	     0.000	        1	[densenet201/conv3_block4_2_conv/Conv2D1]:56
	           CONCATENATION	         2345.871	    0.772	    0.802	  0.019%	 54.628%	     0.000	        1	[densenet201/conv3_block4_concat/concat]:57
	                     MUL	         2346.683	    2.183	    2.133	  0.050%	 54.678%	     0.000	        1	[densenet201/conv3_block5_0_bn/FusedBatchNormV3]:58
	                     ADD	         2348.826	    1.848	    1.921	  0.045%	 54.722%	     0.000	        1	[densenet201/conv3_block5_0_relu/Relu;densenet201/conv3_block5_0_bn/FusedBatchNormV3]:59
	                 CONV_2D	         2350.757	   40.984	   40.959	  0.954%	 55.676%	     0.000	        1	[densenet201/conv3_block5_1_relu/Relu;densenet201/conv3_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block5_1_conv/Conv2D]:60
	                 CONV_2D	         2391.727	   17.943	   17.964	  0.418%	 56.094%	     0.000	        1	[densenet201/conv3_block5_2_conv/Conv2D1]:61
	           CONCATENATION	         2409.704	    0.910	    0.888	  0.021%	 56.115%	     0.000	        1	[densenet201/conv3_block5_concat/concat]:62
	                     MUL	         2410.602	    2.428	    2.369	  0.055%	 56.170%	     0.000	        1	[densenet201/conv3_block6_0_bn/FusedBatchNormV3]:63
	                     ADD	         2412.981	    2.068	    2.155	  0.050%	 56.220%	     0.000	        1	[densenet201/conv3_block6_0_relu/Relu;densenet201/conv3_block6_0_bn/FusedBatchNormV3]:64
	                 CONV_2D	         2415.146	   41.196	   41.162	  0.958%	 57.179%	     0.000	        1	[densenet201/conv3_block6_1_relu/Relu;densenet201/conv3_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block6_1_conv/Conv2D]:65
	                 CONV_2D	         2456.320	   18.001	   18.056	  0.420%	 57.599%	     0.000	        1	[densenet201/conv3_block6_2_conv/Conv2D1]:66
	           CONCATENATION	         2474.390	    0.902	    0.902	  0.021%	 57.620%	     0.000	        1	[densenet201/conv3_block6_concat/concat]:67
	                     MUL	         2475.302	    2.529	    2.566	  0.060%	 57.680%	     0.000	        1	[densenet201/conv3_block7_0_bn/FusedBatchNormV3]:68
	                     ADD	         2477.879	    2.416	    2.394	  0.056%	 57.736%	     0.000	        1	[densenet201/conv3_block7_0_relu/Relu;densenet201/conv3_block7_0_bn/FusedBatchNormV3]:69
	                 CONV_2D	         2480.284	   40.861	   40.952	  0.954%	 58.689%	     0.000	        1	[densenet201/conv3_block7_1_relu/Relu;densenet201/conv3_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block7_1_conv/Conv2D]:70
	                 CONV_2D	         2521.247	   18.019	   18.020	  0.420%	 59.109%	     0.000	        1	[densenet201/conv3_block7_2_conv/Conv2D1]:71
	           CONCATENATION	         2539.279	    1.032	    1.071	  0.025%	 59.134%	     0.000	        1	[densenet201/conv3_block7_concat/concat]:72
	                     MUL	         2540.360	    2.629	    2.658	  0.062%	 59.196%	     0.000	        1	[densenet201/conv3_block8_0_bn/FusedBatchNormV3]:73
	                     ADD	         2543.028	    2.633	    2.619	  0.061%	 59.257%	     0.000	        1	[densenet201/conv3_block8_0_relu/Relu;densenet201/conv3_block8_0_bn/FusedBatchNormV3]:74
	                 CONV_2D	         2545.657	   41.767	   41.703	  0.971%	 60.228%	     0.000	        1	[densenet201/conv3_block8_1_relu/Relu;densenet201/conv3_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block8_1_conv/Conv2D]:75
	                 CONV_2D	         2587.372	   17.927	   17.940	  0.418%	 60.646%	     0.000	        1	[densenet201/conv3_block8_2_conv/Conv2D1]:76
	           CONCATENATION	         2605.323	    1.060	    1.063	  0.025%	 60.670%	     0.000	        1	[densenet201/conv3_block8_concat/concat]:77
	                     MUL	         2606.397	    2.938	    2.890	  0.067%	 60.738%	     0.000	        1	[densenet201/conv3_block9_0_bn/FusedBatchNormV3]:78
	                     ADD	         2609.296	    2.826	    2.795	  0.065%	 60.803%	     0.000	        1	[densenet201/conv3_block9_0_relu/Relu;densenet201/conv3_block9_0_bn/FusedBatchNormV3]:79
	                 CONV_2D	         2612.102	   41.373	   41.377	  0.963%	 61.766%	     0.000	        1	[densenet201/conv3_block9_1_relu/Relu;densenet201/conv3_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block9_1_conv/Conv2D]:80
	                 CONV_2D	         2653.491	   18.100	   18.043	  0.420%	 62.186%	     0.000	        1	[densenet201/conv3_block9_2_conv/Conv2D1]:81
	           CONCATENATION	         2671.545	    1.159	    1.236	  0.029%	 62.215%	     0.000	        1	[densenet201/conv3_block9_concat/concat]:82
	                     MUL	         2672.793	    3.407	    3.385	  0.079%	 62.294%	     0.000	        1	[densenet201/conv3_block10_0_bn/FusedBatchNormV3]:83
	                     ADD	         2676.188	    2.969	    3.042	  0.071%	 62.365%	     0.000	        1	[densenet201/conv3_block10_0_relu/Relu;densenet201/conv3_block10_0_bn/FusedBatchNormV3]:84
	                 CONV_2D	         2679.240	   42.417	   42.364	  0.986%	 63.351%	     0.000	        1	[densenet201/conv3_block10_1_relu/Relu;densenet201/conv3_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block10_1_conv/Conv2D]:85
	                 CONV_2D	         2721.616	   17.948	   17.966	  0.418%	 63.770%	     0.000	        1	[densenet201/conv3_block10_2_conv/Conv2D1]:86
	           CONCATENATION	         2739.594	    1.170	    1.250	  0.029%	 63.799%	     0.000	        1	[densenet201/conv3_block10_concat/concat]:87
	                     MUL	         2740.855	    3.642	    3.580	  0.083%	 63.882%	     0.000	        1	[densenet201/conv3_block11_0_bn/FusedBatchNormV3]:88
	                     ADD	         2744.445	    3.293	    3.324	  0.077%	 63.959%	     0.000	        1	[densenet201/conv3_block11_0_relu/Relu;densenet201/conv3_block11_0_bn/FusedBatchNormV3]:89
	                 CONV_2D	         2747.780	   42.181	   42.213	  0.983%	 64.942%	     0.000	        1	[densenet201/conv3_block11_1_relu/Relu;densenet201/conv3_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block11_1_conv/Conv2D]:90
	                 CONV_2D	         2790.005	   18.028	   18.020	  0.420%	 65.362%	     0.000	        1	[densenet201/conv3_block11_2_conv/Conv2D1]:91
	           CONCATENATION	         2808.037	    1.381	    1.385	  0.032%	 65.394%	     0.000	        1	[densenet201/conv3_block11_concat/concat]:92
	                     MUL	         2809.432	    3.843	    3.783	  0.088%	 65.482%	     0.000	        1	[densenet201/conv3_block12_0_bn/FusedBatchNormV3]:93
	                     ADD	         2813.226	    3.477	    3.510	  0.082%	 65.564%	     0.000	        1	[densenet201/conv3_block12_0_relu/Relu;densenet201/conv3_block12_0_bn/FusedBatchNormV3]:94
	                 CONV_2D	         2816.747	   43.130	   43.150	  1.005%	 66.569%	     0.000	        1	[densenet201/conv3_block12_1_relu/Relu;densenet201/conv3_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv3_block12_1_conv/Conv2D]:95
	                 CONV_2D	         2859.909	   17.913	   17.963	  0.418%	 66.987%	     0.000	        1	[densenet201/conv3_block12_2_conv/Conv2D1]:96
	           CONCATENATION	         2877.884	    1.358	    1.396	  0.032%	 67.020%	     0.000	        1	[densenet201/conv3_block12_concat/concat]:97
	                     MUL	         2879.291	    3.801	    3.798	  0.088%	 67.108%	     0.000	        1	[densenet201/pool3_bn/FusedBatchNormV3]:98
	                     ADD	         2883.100	    3.785	    3.801	  0.089%	 67.197%	     0.000	        1	[densenet201/pool3_relu/Relu;densenet201/pool3_bn/FusedBatchNormV3]:99
	                 CONV_2D	         2886.912	   78.711	   78.667	  1.832%	 69.028%	     0.000	        1	[densenet201/pool3_conv/Conv2D1]:100
	         AVERAGE_POOL_2D	         2965.591	   12.881	   13.032	  0.303%	 69.332%	     0.000	        1	[densenet201/pool3_pool/AvgPool]:101
	                     MUL	         2978.633	    0.573	    0.548	  0.013%	 69.345%	     0.000	        1	[densenet201/conv4_block1_0_bn/FusedBatchNormV31]:102
	                     ADD	         2979.188	    0.469	    0.476	  0.011%	 69.356%	     0.000	        1	[densenet201/conv4_block1_0_relu/Relu;densenet201/conv4_block1_0_bn/FusedBatchNormV3]:103
	                 CONV_2D	         2979.671	   10.070	   10.081	  0.235%	 69.590%	     0.000	        1	[densenet201/conv4_block1_1_relu/Relu;densenet201/conv4_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block1_1_conv/Conv2D]:104
	                 CONV_2D	         2989.762	    4.431	    4.442	  0.103%	 69.694%	     0.000	        1	[densenet201/conv4_block1_2_conv/Conv2D1]:105
	           CONCATENATION	         2994.213	    0.169	    0.183	  0.004%	 69.698%	     0.000	        1	[densenet201/conv4_block1_concat/concat]:106
	                     MUL	         2994.403	    0.594	    0.598	  0.014%	 69.712%	     0.000	        1	[densenet201/conv4_block2_0_bn/FusedBatchNormV31]:107
	                     ADD	         2995.007	    0.526	    0.541	  0.013%	 69.725%	     0.000	        1	[densenet201/conv4_block2_0_relu/Relu;densenet201/conv4_block2_0_bn/FusedBatchNormV3]:108
	                 CONV_2D	         2995.555	   10.231	   10.249	  0.239%	 69.963%	     0.000	        1	[densenet201/conv4_block2_1_relu/Relu;densenet201/conv4_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block2_1_conv/Conv2D]:109
	                 CONV_2D	         3005.815	    4.432	    4.444	  0.103%	 70.067%	     0.000	        1	[densenet201/conv4_block2_2_conv/Conv2D1]:110
	           CONCATENATION	         3010.268	    0.144	    0.156	  0.004%	 70.070%	     0.000	        1	[densenet201/conv4_block2_concat/concat]:111
	                     MUL	         3010.431	    0.645	    0.656	  0.015%	 70.086%	     0.000	        1	[densenet201/conv4_block3_0_bn/FusedBatchNormV31]:112
	                     ADD	         3011.094	    0.601	    0.600	  0.014%	 70.100%	     0.000	        1	[densenet201/conv4_block3_0_relu/Relu;densenet201/conv4_block3_0_bn/FusedBatchNormV3]:113
	                 CONV_2D	         3011.701	   10.319	   10.337	  0.241%	 70.340%	     0.000	        1	[densenet201/conv4_block3_1_relu/Relu;densenet201/conv4_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block3_1_conv/Conv2D]:114
	                 CONV_2D	         3022.049	    4.417	    4.434	  0.103%	 70.444%	     0.000	        1	[densenet201/conv4_block3_2_conv/Conv2D1]:115
	           CONCATENATION	         3026.492	    0.174	    0.188	  0.004%	 70.448%	     0.000	        1	[densenet201/conv4_block3_concat/concat]:116
	                     MUL	         3026.687	    0.669	    0.670	  0.016%	 70.464%	     0.000	        1	[densenet201/conv4_block4_0_bn/FusedBatchNormV31]:117
	                     ADD	         3027.364	    0.724	    0.663	  0.015%	 70.479%	     0.000	        1	[densenet201/conv4_block4_0_relu/Relu;densenet201/conv4_block4_0_bn/FusedBatchNormV3]:118
	                 CONV_2D	         3028.034	   10.484	   10.432	  0.243%	 70.722%	     0.000	        1	[densenet201/conv4_block4_1_relu/Relu;densenet201/conv4_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block4_1_conv/Conv2D]:119
	                 CONV_2D	         3038.477	    4.426	    4.446	  0.104%	 70.825%	     0.000	        1	[densenet201/conv4_block4_2_conv/Conv2D1]:120
	           CONCATENATION	         3042.932	    0.199	    0.208	  0.005%	 70.830%	     0.000	        1	[densenet201/conv4_block4_concat/concat]:121
	                     MUL	         3043.146	    0.715	    0.721	  0.017%	 70.847%	     0.000	        1	[densenet201/conv4_block5_0_bn/FusedBatchNormV31]:122
	                     ADD	         3043.874	    0.727	    0.728	  0.017%	 70.864%	     0.000	        1	[densenet201/conv4_block5_0_relu/Relu;densenet201/conv4_block5_0_bn/FusedBatchNormV3]:123
	                 CONV_2D	         3044.610	   10.372	   10.393	  0.242%	 71.106%	     0.000	        1	[densenet201/conv4_block5_1_relu/Relu;densenet201/conv4_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block5_1_conv/Conv2D]:124
	                 CONV_2D	         3055.013	    4.478	    4.445	  0.104%	 71.210%	     0.000	        1	[densenet201/conv4_block5_2_conv/Conv2D1]:125
	           CONCATENATION	         3059.468	    0.216	    0.239	  0.006%	 71.215%	     0.000	        1	[densenet201/conv4_block5_concat/concat]:126
	                     MUL	         3059.714	    0.865	    0.854	  0.020%	 71.235%	     0.000	        1	[densenet201/conv4_block6_0_bn/FusedBatchNormV31]:127
	                     ADD	         3060.575	    0.755	    0.769	  0.018%	 71.253%	     0.000	        1	[densenet201/conv4_block6_0_relu/Relu;densenet201/conv4_block6_0_bn/FusedBatchNormV3]:128
	                 CONV_2D	         3061.351	   10.690	   10.627	  0.247%	 71.500%	     0.000	        1	[densenet201/conv4_block6_1_relu/Relu;densenet201/conv4_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block6_1_conv/Conv2D]:129
	                 CONV_2D	         3071.989	    4.446	    4.444	  0.103%	 71.604%	     0.000	        1	[densenet201/conv4_block6_2_conv/Conv2D1]:130
	           CONCATENATION	         3076.441	    0.266	    0.275	  0.006%	 71.610%	     0.000	        1	[densenet201/conv4_block6_concat/concat]:131
	                     MUL	         3076.724	    0.940	    0.927	  0.022%	 71.632%	     0.000	        1	[densenet201/conv4_block7_0_bn/FusedBatchNormV31]:132
	                     ADD	         3077.658	    0.827	    0.850	  0.020%	 71.652%	     0.000	        1	[densenet201/conv4_block7_0_relu/Relu;densenet201/conv4_block7_0_bn/FusedBatchNormV3]:133
	                 CONV_2D	         3078.515	   10.567	   10.553	  0.246%	 71.897%	     0.000	        1	[densenet201/conv4_block7_1_relu/Relu;densenet201/conv4_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block7_1_conv/Conv2D]:134
	                 CONV_2D	         3089.079	    4.443	    4.451	  0.104%	 72.001%	     0.000	        1	[densenet201/conv4_block7_2_conv/Conv2D1]:135
	           CONCATENATION	         3093.540	    0.303	    0.300	  0.007%	 72.008%	     0.000	        1	[densenet201/conv4_block7_concat/concat]:136
	                     MUL	         3093.847	    0.939	    0.961	  0.022%	 72.030%	     0.000	        1	[densenet201/conv4_block8_0_bn/FusedBatchNormV31]:137
	                     ADD	         3094.815	    0.879	    0.878	  0.020%	 72.051%	     0.000	        1	[densenet201/conv4_block8_0_relu/Relu;densenet201/conv4_block8_0_bn/FusedBatchNormV3]:138
	                 CONV_2D	         3095.701	   10.825	   10.781	  0.251%	 72.302%	     0.000	        1	[densenet201/conv4_block8_1_relu/Relu;densenet201/conv4_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block8_1_conv/Conv2D]:139
	                 CONV_2D	         3106.493	    4.431	    4.449	  0.104%	 72.405%	     0.000	        1	[densenet201/conv4_block8_2_conv/Conv2D1]:140
	           CONCATENATION	         3110.952	    0.377	    0.338	  0.008%	 72.413%	     0.000	        1	[densenet201/conv4_block8_concat/concat]:141
	                     MUL	         3111.298	    0.989	    0.970	  0.023%	 72.436%	     0.000	        1	[densenet201/conv4_block9_0_bn/FusedBatchNormV31]:142
	                     ADD	         3112.276	    0.942	    0.970	  0.023%	 72.458%	     0.000	        1	[densenet201/conv4_block9_0_relu/Relu;densenet201/conv4_block9_0_bn/FusedBatchNormV3]:143
	                 CONV_2D	         3113.253	   10.676	   10.692	  0.249%	 72.707%	     0.000	        1	[densenet201/conv4_block9_1_relu/Relu;densenet201/conv4_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block9_1_conv/Conv2D]:144
	                 CONV_2D	         3123.957	    4.435	    4.445	  0.104%	 72.811%	     0.000	        1	[densenet201/conv4_block9_2_conv/Conv2D1]:145
	           CONCATENATION	         3128.411	    0.380	    0.377	  0.009%	 72.820%	     0.000	        1	[densenet201/conv4_block9_concat/concat]:146
	                     MUL	         3128.795	    1.095	    1.102	  0.026%	 72.845%	     0.000	        1	[densenet201/conv4_block10_0_bn/FusedBatchNormV31]:147
	                     ADD	         3129.905	    1.023	    1.011	  0.024%	 72.869%	     0.000	        1	[densenet201/conv4_block10_0_relu/Relu;densenet201/conv4_block10_0_bn/FusedBatchNormV3]:148
	                 CONV_2D	         3130.924	   10.912	   10.912	  0.254%	 73.123%	     0.000	        1	[densenet201/conv4_block10_1_relu/Relu;densenet201/conv4_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block10_1_conv/Conv2D]:149
	                 CONV_2D	         3141.848	    4.437	    4.457	  0.104%	 73.227%	     0.000	        1	[densenet201/conv4_block10_2_conv/Conv2D1]:150
	           CONCATENATION	         3146.313	    0.388	    0.397	  0.009%	 73.236%	     0.000	        1	[densenet201/conv4_block10_concat/concat]:151
	                     MUL	         3146.718	    1.047	    1.068	  0.025%	 73.261%	     0.000	        1	[densenet201/conv4_block11_0_bn/FusedBatchNormV31]:152
	                     ADD	         3147.794	    1.089	    1.066	  0.025%	 73.286%	     0.000	        1	[densenet201/conv4_block11_0_relu/Relu;densenet201/conv4_block11_0_bn/FusedBatchNormV3]:153
	                 CONV_2D	         3148.869	   10.850	   10.859	  0.253%	 73.539%	     0.000	        1	[densenet201/conv4_block11_1_relu/Relu;densenet201/conv4_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block11_1_conv/Conv2D]:154
	                 CONV_2D	         3159.739	    4.482	    4.464	  0.104%	 73.643%	     0.000	        1	[densenet201/conv4_block11_2_conv/Conv2D1]:155
	           CONCATENATION	         3164.212	    0.358	    0.403	  0.009%	 73.652%	     0.000	        1	[densenet201/conv4_block11_concat/concat]:156
	                     MUL	         3164.623	    1.260	    1.247	  0.029%	 73.681%	     0.000	        1	[densenet201/conv4_block12_0_bn/FusedBatchNormV31]:157
	                     ADD	         3165.879	    1.111	    1.147	  0.027%	 73.708%	     0.000	        1	[densenet201/conv4_block12_0_relu/Relu;densenet201/conv4_block12_0_bn/FusedBatchNormV3]:158
	                 CONV_2D	         3167.034	   11.114	   11.125	  0.259%	 73.967%	     0.000	        1	[densenet201/conv4_block12_1_relu/Relu;densenet201/conv4_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block12_1_conv/Conv2D]:159
	                 CONV_2D	         3178.171	    4.428	    4.458	  0.104%	 74.071%	     0.000	        1	[densenet201/conv4_block12_2_conv/Conv2D1]:160
	           CONCATENATION	         3182.638	    0.463	    0.437	  0.010%	 74.081%	     0.000	        1	[densenet201/conv4_block12_concat/concat]:161
	                     MUL	         3183.084	    1.288	    1.263	  0.029%	 74.110%	     0.000	        1	[densenet201/conv4_block13_0_bn/FusedBatchNormV31]:162
	                     ADD	         3184.355	    1.205	    1.186	  0.028%	 74.138%	     0.000	        1	[densenet201/conv4_block13_0_relu/Relu;densenet201/conv4_block13_0_bn/FusedBatchNormV3]:163
	                 CONV_2D	         3185.550	   11.031	   11.045	  0.257%	 74.395%	     0.000	        1	[densenet201/conv4_block13_1_relu/Relu;densenet201/conv4_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block13_1_conv/Conv2D]:164
	                 CONV_2D	         3196.607	    4.446	    4.454	  0.104%	 74.499%	     0.000	        1	[densenet201/conv4_block13_2_conv/Conv2D1]:165
	           CONCATENATION	         3201.070	    0.476	    0.479	  0.011%	 74.510%	     0.000	        1	[densenet201/conv4_block13_concat/concat]:166
	                     MUL	         3201.558	    1.343	    1.369	  0.032%	 74.542%	     0.000	        1	[densenet201/conv4_block14_0_bn/FusedBatchNormV31]:167
	                     ADD	         3202.935	    1.303	    1.236	  0.029%	 74.570%	     0.000	        1	[densenet201/conv4_block14_0_relu/Relu;densenet201/conv4_block14_0_bn/FusedBatchNormV3]:168
	                 CONV_2D	         3204.181	   11.334	   11.347	  0.264%	 74.835%	     0.000	        1	[densenet201/conv4_block14_1_relu/Relu;densenet201/conv4_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block14_1_conv/Conv2D]:169
	                 CONV_2D	         3215.540	    4.470	    4.478	  0.104%	 74.939%	     0.000	        1	[densenet201/conv4_block14_2_conv/Conv2D1]:170
	           CONCATENATION	         3220.027	    0.518	    0.515	  0.012%	 74.951%	     0.000	        1	[densenet201/conv4_block14_concat/concat]:171
	                     MUL	         3220.550	    1.303	    1.328	  0.031%	 74.982%	     0.000	        1	[densenet201/conv4_block15_0_bn/FusedBatchNormV31]:172
	                     ADD	         3221.887	    1.265	    1.297	  0.030%	 75.012%	     0.000	        1	[densenet201/conv4_block15_0_relu/Relu;densenet201/conv4_block15_0_bn/FusedBatchNormV3]:173
	                 CONV_2D	         3223.193	   11.197	   11.204	  0.261%	 75.273%	     0.000	        1	[densenet201/conv4_block15_1_relu/Relu;densenet201/conv4_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block15_1_conv/Conv2D]:174
	                 CONV_2D	         3234.408	    4.485	    4.489	  0.105%	 75.377%	     0.000	        1	[densenet201/conv4_block15_2_conv/Conv2D1]:175
	           CONCATENATION	         3238.905	    0.433	    0.497	  0.012%	 75.389%	     0.000	        1	[densenet201/conv4_block15_concat/concat]:176
	                     MUL	         3239.411	    1.378	    1.368	  0.032%	 75.421%	     0.000	        1	[densenet201/conv4_block16_0_bn/FusedBatchNormV31]:177
	                     ADD	         3240.787	    1.341	    1.359	  0.032%	 75.453%	     0.000	        1	[densenet201/conv4_block16_0_relu/Relu;densenet201/conv4_block16_0_bn/FusedBatchNormV3]:178
	                 CONV_2D	         3242.155	   11.432	   11.440	  0.266%	 75.719%	     0.000	        1	[densenet201/conv4_block16_1_relu/Relu;densenet201/conv4_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block16_1_conv/Conv2D]:179
	                 CONV_2D	         3253.607	    4.517	    4.470	  0.104%	 75.823%	     0.000	        1	[densenet201/conv4_block16_2_conv/Conv2D1]:180
	           CONCATENATION	         3258.086	    0.467	    0.520	  0.012%	 75.835%	     0.000	        1	[densenet201/conv4_block16_concat/concat]:181
	                     MUL	         3258.614	    1.587	    1.572	  0.037%	 75.872%	     0.000	        1	[densenet201/conv4_block17_0_bn/FusedBatchNormV31]:182
	                     ADD	         3260.195	    1.385	    1.402	  0.033%	 75.904%	     0.000	        1	[densenet201/conv4_block17_0_relu/Relu;densenet201/conv4_block17_0_bn/FusedBatchNormV3]:183
	                 CONV_2D	         3261.606	   11.390	   11.406	  0.266%	 76.170%	     0.000	        1	[densenet201/conv4_block17_1_relu/Relu;densenet201/conv4_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block17_1_conv/Conv2D]:184
	                 CONV_2D	         3273.024	    4.454	    4.477	  0.104%	 76.274%	     0.000	        1	[densenet201/conv4_block17_2_conv/Conv2D1]:185
	           CONCATENATION	         3277.510	    0.554	    0.506	  0.012%	 76.286%	     0.000	        1	[densenet201/conv4_block17_concat/concat]:186
	                     MUL	         3278.025	    1.582	    1.619	  0.038%	 76.324%	     0.000	        1	[densenet201/conv4_block18_0_bn/FusedBatchNormV31]:187
	                     ADD	         3279.653	    1.528	    1.488	  0.035%	 76.358%	     0.000	        1	[densenet201/conv4_block18_0_relu/Relu;densenet201/conv4_block18_0_bn/FusedBatchNormV3]:188
	                 CONV_2D	         3281.150	   11.639	   11.649	  0.271%	 76.630%	     0.000	        1	[densenet201/conv4_block18_1_relu/Relu;densenet201/conv4_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block18_1_conv/Conv2D]:189
	                 CONV_2D	         3292.810	    4.450	    4.466	  0.104%	 76.734%	     0.000	        1	[densenet201/conv4_block18_2_conv/Conv2D1]:190
	           CONCATENATION	         3297.284	    0.535	    0.537	  0.013%	 76.746%	     0.000	        1	[densenet201/conv4_block18_concat/concat]:191
	                     MUL	         3297.830	    1.608	    1.641	  0.038%	 76.784%	     0.000	        1	[densenet201/conv4_block19_0_bn/FusedBatchNormV31]:192
	                     ADD	         3299.480	    1.545	    1.521	  0.035%	 76.820%	     0.000	        1	[densenet201/conv4_block19_0_relu/Relu;densenet201/conv4_block19_0_bn/FusedBatchNormV3]:193
	                 CONV_2D	         3301.011	   11.574	   11.581	  0.270%	 77.089%	     0.000	        1	[densenet201/conv4_block19_1_relu/Relu;densenet201/conv4_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block19_1_conv/Conv2D]:194
	                 CONV_2D	         3312.604	    4.458	    4.470	  0.104%	 77.194%	     0.000	        1	[densenet201/conv4_block19_2_conv/Conv2D1]:195
	           CONCATENATION	         3317.083	    0.549	    0.546	  0.013%	 77.206%	     0.000	        1	[densenet201/conv4_block19_concat/concat]:196
	                     MUL	         3317.638	    1.597	    1.639	  0.038%	 77.244%	     0.000	        1	[densenet201/conv4_block20_0_bn/FusedBatchNormV31]:197
	                     ADD	         3319.287	    1.611	    1.588	  0.037%	 77.281%	     0.000	        1	[densenet201/conv4_block20_0_relu/Relu;densenet201/conv4_block20_0_bn/FusedBatchNormV3]:198
	                 CONV_2D	         3320.884	   11.837	   11.846	  0.276%	 77.557%	     0.000	        1	[densenet201/conv4_block20_1_relu/Relu;densenet201/conv4_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block20_1_conv/Conv2D]:199
	                 CONV_2D	         3332.741	    4.448	    4.470	  0.104%	 77.661%	     0.000	        1	[densenet201/conv4_block20_2_conv/Conv2D1]:200
	           CONCATENATION	         3337.219	    0.563	    0.575	  0.013%	 77.675%	     0.000	        1	[densenet201/conv4_block20_concat/concat]:201
	                     MUL	         3337.803	    1.617	    1.649	  0.038%	 77.713%	     0.000	        1	[densenet201/conv4_block21_0_bn/FusedBatchNormV3]:202
	                     ADD	         3339.461	    1.673	    1.649	  0.038%	 77.751%	     0.000	        1	[densenet201/conv4_block21_0_relu/Relu;densenet201/conv4_block21_0_bn/FusedBatchNormV3]:203
	                 CONV_2D	         3341.119	   11.775	   11.759	  0.274%	 78.025%	     0.000	        1	[densenet201/conv4_block21_1_relu/Relu;densenet201/conv4_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block21_1_conv/Conv2D]:204
	                 CONV_2D	         3352.889	    4.476	    4.472	  0.104%	 78.129%	     0.000	        1	[densenet201/conv4_block21_2_conv/Conv2D1]:205
	           CONCATENATION	         3357.370	    0.583	    0.569	  0.013%	 78.143%	     0.000	        1	[densenet201/conv4_block21_concat/concat]:206
	                     MUL	         3357.949	    1.833	    1.864	  0.043%	 78.186%	     0.000	        1	[densenet201/conv4_block22_0_bn/FusedBatchNormV3]:207
	                     ADD	         3359.822	    1.749	    1.707	  0.040%	 78.226%	     0.000	        1	[densenet201/conv4_block22_0_relu/Relu;densenet201/conv4_block22_0_bn/FusedBatchNormV3]:208
	                 CONV_2D	         3361.539	   11.964	   11.980	  0.279%	 78.505%	     0.000	        1	[densenet201/conv4_block22_1_relu/Relu;densenet201/conv4_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block22_1_conv/Conv2D]:209
	                 CONV_2D	         3373.531	    4.456	    4.480	  0.104%	 78.609%	     0.000	        1	[densenet201/conv4_block22_2_conv/Conv2D1]:210
	           CONCATENATION	         3378.020	    0.583	    0.581	  0.014%	 78.623%	     0.000	        1	[densenet201/conv4_block22_concat/concat]:211
	                     MUL	         3378.611	    1.987	    1.916	  0.045%	 78.667%	     0.000	        1	[densenet201/conv4_block23_0_bn/FusedBatchNormV3]:212
	                     ADD	         3380.537	    1.717	    1.758	  0.041%	 78.708%	     0.000	        1	[densenet201/conv4_block23_0_relu/Relu;densenet201/conv4_block23_0_bn/FusedBatchNormV3]:213
	                 CONV_2D	         3382.304	   11.959	   11.922	  0.278%	 78.986%	     0.000	        1	[densenet201/conv4_block23_1_relu/Relu;densenet201/conv4_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block23_1_conv/Conv2D]:214
	                 CONV_2D	         3394.238	    4.452	    4.464	  0.104%	 79.090%	     0.000	        1	[densenet201/conv4_block23_2_conv/Conv2D1]:215
	           CONCATENATION	         3398.711	    0.577	    0.585	  0.014%	 79.103%	     0.000	        1	[densenet201/conv4_block23_concat/concat]:216
	                     MUL	         3399.306	    1.989	    1.951	  0.045%	 79.149%	     0.000	        1	[densenet201/conv4_block24_0_bn/FusedBatchNormV3]:217
	                     ADD	         3401.265	    1.801	    1.845	  0.043%	 79.192%	     0.000	        1	[densenet201/conv4_block24_0_relu/Relu;densenet201/conv4_block24_0_bn/FusedBatchNormV3]:218
	                 CONV_2D	         3403.121	   12.150	   12.160	  0.283%	 79.475%	     0.000	        1	[densenet201/conv4_block24_1_relu/Relu;densenet201/conv4_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block24_1_conv/Conv2D]:219
	                 CONV_2D	         3415.292	    4.468	    4.474	  0.104%	 79.579%	     0.000	        1	[densenet201/conv4_block24_2_conv/Conv2D1]:220
	           CONCATENATION	         3419.775	    0.654	    0.628	  0.015%	 79.594%	     0.000	        1	[densenet201/conv4_block24_concat/concat]:221
	                     MUL	         3420.413	    1.872	    1.936	  0.045%	 79.639%	     0.000	        1	[densenet201/conv4_block25_0_bn/FusedBatchNormV3]:222
	                     ADD	         3422.360	    1.907	    1.895	  0.044%	 79.683%	     0.000	        1	[densenet201/conv4_block25_0_relu/Relu;densenet201/conv4_block25_0_bn/FusedBatchNormV3]:223
	                 CONV_2D	         3424.265	   12.196	   12.155	  0.283%	 79.966%	     0.000	        1	[densenet201/conv4_block25_1_relu/Relu;densenet201/conv4_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block25_1_conv/Conv2D]:224
	                 CONV_2D	         3436.434	    4.481	    4.473	  0.104%	 80.070%	     0.000	        1	[densenet201/conv4_block25_2_conv/Conv2D1]:225
	           CONCATENATION	         3440.916	    0.585	    0.620	  0.014%	 80.085%	     0.000	        1	[densenet201/conv4_block25_concat/concat]:226
	                     MUL	         3441.546	    2.070	    2.091	  0.049%	 80.133%	     0.000	        1	[densenet201/conv4_block26_0_bn/FusedBatchNormV3]:227
	                     ADD	         3443.647	    1.999	    1.958	  0.046%	 80.179%	     0.000	        1	[densenet201/conv4_block26_0_relu/Relu;densenet201/conv4_block26_0_bn/FusedBatchNormV3]:228
	                 CONV_2D	         3445.615	   12.344	   12.355	  0.288%	 80.467%	     0.000	        1	[densenet201/conv4_block26_1_relu/Relu;densenet201/conv4_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block26_1_conv/Conv2D]:229
	                 CONV_2D	         3457.981	    4.508	    4.487	  0.104%	 80.571%	     0.000	        1	[densenet201/conv4_block26_2_conv/Conv2D1]:230
	           CONCATENATION	         3462.478	    0.613	    0.645	  0.015%	 80.586%	     0.000	        1	[densenet201/conv4_block26_concat/concat]:231
	                     MUL	         3463.133	    2.067	    2.025	  0.047%	 80.633%	     0.000	        1	[densenet201/conv4_block27_0_bn/FusedBatchNormV3]:232
	                     ADD	         3465.169	    1.965	    1.998	  0.047%	 80.680%	     0.000	        1	[densenet201/conv4_block27_0_relu/Relu;densenet201/conv4_block27_0_bn/FusedBatchNormV3]:233
	                 CONV_2D	         3467.177	   12.313	   12.307	  0.287%	 80.966%	     0.000	        1	[densenet201/conv4_block27_1_relu/Relu;densenet201/conv4_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block27_1_conv/Conv2D]:234
	                 CONV_2D	         3479.496	    4.465	    4.484	  0.104%	 81.071%	     0.000	        1	[densenet201/conv4_block27_2_conv/Conv2D1]:235
	           CONCATENATION	         3483.989	    0.639	    0.644	  0.015%	 81.086%	     0.000	        1	[densenet201/conv4_block27_concat/concat]:236
	                     MUL	         3484.643	    2.222	    2.258	  0.053%	 81.138%	     0.000	        1	[densenet201/conv4_block28_0_bn/FusedBatchNormV3]:237
	                     ADD	         3486.911	    2.091	    2.041	  0.048%	 81.186%	     0.000	        1	[densenet201/conv4_block28_0_relu/Relu;densenet201/conv4_block28_0_bn/FusedBatchNormV3]:238
	                 CONV_2D	         3488.962	   12.513	   12.531	  0.292%	 81.478%	     0.000	        1	[densenet201/conv4_block28_1_relu/Relu;densenet201/conv4_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block28_1_conv/Conv2D]:239
	                 CONV_2D	         3501.504	    4.518	    4.489	  0.105%	 81.582%	     0.000	        1	[densenet201/conv4_block28_2_conv/Conv2D1]:240
	           CONCATENATION	         3506.003	    0.677	    0.676	  0.016%	 81.598%	     0.000	        1	[densenet201/conv4_block28_concat/concat]:241
	                     MUL	         3506.688	    2.281	    2.254	  0.052%	 81.650%	     0.000	        1	[densenet201/conv4_block29_0_bn/FusedBatchNormV3]:242
	                     ADD	         3508.952	    2.133	    2.163	  0.050%	 81.701%	     0.000	        1	[densenet201/conv4_block29_0_relu/Relu;densenet201/conv4_block29_0_bn/FusedBatchNormV3]:243
	                 CONV_2D	         3511.125	   12.479	   12.458	  0.290%	 81.991%	     0.000	        1	[densenet201/conv4_block29_1_relu/Relu;densenet201/conv4_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block29_1_conv/Conv2D]:244
	                 CONV_2D	         3523.595	    4.447	    4.479	  0.104%	 82.095%	     0.000	        1	[densenet201/conv4_block29_2_conv/Conv2D1]:245
	           CONCATENATION	         3528.082	    0.711	    0.691	  0.016%	 82.111%	     0.000	        1	[densenet201/conv4_block29_concat/concat]:246
	                     MUL	         3528.783	    2.364	    2.386	  0.056%	 82.167%	     0.000	        1	[densenet201/conv4_block30_0_bn/FusedBatchNormV3]:247
	                     ADD	         3531.178	    2.253	    2.219	  0.052%	 82.218%	     0.000	        1	[densenet201/conv4_block30_0_relu/Relu;densenet201/conv4_block30_0_bn/FusedBatchNormV3]:248
	                 CONV_2D	         3533.408	   12.641	   12.684	  0.295%	 82.514%	     0.000	        1	[densenet201/conv4_block30_1_relu/Relu;densenet201/conv4_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block30_1_conv/Conv2D]:249
	                 CONV_2D	         3546.103	    4.496	    4.499	  0.105%	 82.619%	     0.000	        1	[densenet201/conv4_block30_2_conv/Conv2D1]:250
	           CONCATENATION	         3550.612	    0.645	    0.692	  0.016%	 82.635%	     0.000	        1	[densenet201/conv4_block30_concat/concat]:251
	                     MUL	         3551.313	    2.341	    2.312	  0.054%	 82.689%	     0.000	        1	[densenet201/conv4_block31_0_bn/FusedBatchNormV3]:252
	                     ADD	         3553.636	    2.187	    2.213	  0.052%	 82.740%	     0.000	        1	[densenet201/conv4_block31_0_relu/Relu;densenet201/conv4_block31_0_bn/FusedBatchNormV3]:253
	                 CONV_2D	         3555.859	   12.640	   12.633	  0.294%	 83.034%	     0.000	        1	[densenet201/conv4_block31_1_relu/Relu;densenet201/conv4_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block31_1_conv/Conv2D]:254
	                 CONV_2D	         3568.504	    4.450	    4.457	  0.104%	 83.138%	     0.000	        1	[densenet201/conv4_block31_2_conv/Conv2D1]:255
	           CONCATENATION	         3572.969	    0.701	    0.712	  0.017%	 83.155%	     0.000	        1	[densenet201/conv4_block31_concat/concat]:256
	                     MUL	         3573.691	    2.254	    2.291	  0.053%	 83.208%	     0.000	        1	[densenet201/conv4_block32_0_bn/FusedBatchNormV3]:257
	                     ADD	         3575.992	    2.348	    2.311	  0.054%	 83.262%	     0.000	        1	[densenet201/conv4_block32_0_relu/Relu;densenet201/conv4_block32_0_bn/FusedBatchNormV3]:258
	                 CONV_2D	         3578.313	   12.832	   12.858	  0.299%	 83.561%	     0.000	        1	[densenet201/conv4_block32_1_relu/Relu;densenet201/conv4_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block32_1_conv/Conv2D]:259
	                 CONV_2D	         3591.182	    4.753	    4.497	  0.105%	 83.666%	     0.000	        1	[densenet201/conv4_block32_2_conv/Conv2D1]:260
	           CONCATENATION	         3595.688	    0.731	    0.744	  0.017%	 83.683%	     0.000	        1	[densenet201/conv4_block32_concat/concat]:261
	                     MUL	         3596.442	    2.503	    2.537	  0.059%	 83.742%	     0.000	        1	[densenet201/conv4_block33_0_bn/FusedBatchNormV3]:262
	                     ADD	         3598.989	    2.396	    2.361	  0.055%	 83.797%	     0.000	        1	[densenet201/conv4_block33_0_relu/Relu;densenet201/conv4_block33_0_bn/FusedBatchNormV3]:263
	                 CONV_2D	         3601.361	   12.787	   12.822	  0.299%	 84.096%	     0.000	        1	[densenet201/conv4_block33_1_relu/Relu;densenet201/conv4_block33_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block33_1_conv/Conv2D]:264
	                 CONV_2D	         3614.194	    4.466	    4.450	  0.104%	 84.199%	     0.000	        1	[densenet201/conv4_block33_2_conv/Conv2D1]:265
	           CONCATENATION	         3618.653	    0.703	    0.744	  0.017%	 84.217%	     0.000	        1	[densenet201/conv4_block33_concat/concat]:266
	                     MUL	         3619.407	    2.708	    2.680	  0.062%	 84.279%	     0.000	        1	[densenet201/conv4_block34_0_bn/FusedBatchNormV3]:267
	                     ADD	         3622.097	    2.410	    2.374	  0.055%	 84.334%	     0.000	        1	[densenet201/conv4_block34_0_relu/Relu;densenet201/conv4_block34_0_bn/FusedBatchNormV3]:268
	                 CONV_2D	         3624.481	   13.058	   13.049	  0.304%	 84.638%	     0.000	        1	[densenet201/conv4_block34_1_relu/Relu;densenet201/conv4_block34_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block34_1_conv/Conv2D]:269
	                 CONV_2D	         3637.542	    4.520	    4.488	  0.105%	 84.743%	     0.000	        1	[densenet201/conv4_block34_2_conv/Conv2D1]:270
	           CONCATENATION	         3642.039	    0.735	    0.759	  0.018%	 84.761%	     0.000	        1	[densenet201/conv4_block34_concat/concat]:271
	                     MUL	         3642.808	    2.658	    2.613	  0.061%	 84.821%	     0.000	        1	[densenet201/conv4_block35_0_bn/FusedBatchNormV3]:272
	                     ADD	         3645.431	    2.474	    2.520	  0.059%	 84.880%	     0.000	        1	[densenet201/conv4_block35_0_relu/Relu;densenet201/conv4_block35_0_bn/FusedBatchNormV3]:273
	                 CONV_2D	         3647.962	   12.989	   12.943	  0.301%	 85.181%	     0.000	        1	[densenet201/conv4_block35_1_relu/Relu;densenet201/conv4_block35_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block35_1_conv/Conv2D]:274
	                 CONV_2D	         3660.916	    4.507	    4.467	  0.104%	 85.285%	     0.000	        1	[densenet201/conv4_block35_2_conv/Conv2D1]:275
	           CONCATENATION	         3665.392	    0.700	    0.803	  0.019%	 85.304%	     0.000	        1	[densenet201/conv4_block35_concat/concat]:276
	                     MUL	         3666.205	    2.612	    2.586	  0.060%	 85.364%	     0.000	        1	[densenet201/conv4_block36_0_bn/FusedBatchNormV3]:277
	                     ADD	         3668.801	    2.521	    2.538	  0.059%	 85.423%	     0.000	        1	[densenet201/conv4_block36_0_relu/Relu;densenet201/conv4_block36_0_bn/FusedBatchNormV3]:278
	                 CONV_2D	         3671.350	   13.231	   13.185	  0.307%	 85.731%	     0.000	        1	[densenet201/conv4_block36_1_relu/Relu;densenet201/conv4_block36_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block36_1_conv/Conv2D]:279
	                 CONV_2D	         3684.546	    4.486	    4.484	  0.104%	 85.835%	     0.000	        1	[densenet201/conv4_block36_2_conv/Conv2D1]:280
	           CONCATENATION	         3689.039	    0.778	    0.798	  0.019%	 85.854%	     0.000	        1	[densenet201/conv4_block36_concat/concat]:281
	                     MUL	         3689.848	    2.605	    2.610	  0.061%	 85.914%	     0.000	        1	[densenet201/conv4_block37_0_bn/FusedBatchNormV3]:282
	                     ADD	         3692.468	    2.611	    2.630	  0.061%	 85.976%	     0.000	        1	[densenet201/conv4_block37_0_relu/Relu;densenet201/conv4_block37_0_bn/FusedBatchNormV3]:283
	                 CONV_2D	         3695.108	   13.116	   13.120	  0.306%	 86.281%	     0.000	        1	[densenet201/conv4_block37_1_relu/Relu;densenet201/conv4_block37_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block37_1_conv/Conv2D]:284
	                 CONV_2D	         3708.239	    4.478	    4.464	  0.104%	 86.385%	     0.000	        1	[densenet201/conv4_block37_2_conv/Conv2D1]:285
	           CONCATENATION	         3712.712	    0.814	    0.824	  0.019%	 86.404%	     0.000	        1	[densenet201/conv4_block37_concat/concat]:286
	                     MUL	         3713.546	    2.850	    2.872	  0.067%	 86.471%	     0.000	        1	[densenet201/conv4_block38_0_bn/FusedBatchNormV3]:287
	                     ADD	         3716.428	    2.628	    2.632	  0.061%	 86.532%	     0.000	        1	[densenet201/conv4_block38_0_relu/Relu;densenet201/conv4_block38_0_bn/FusedBatchNormV3]:288
	                 CONV_2D	         3719.071	   13.349	   13.363	  0.311%	 86.843%	     0.000	        1	[densenet201/conv4_block38_1_relu/Relu;densenet201/conv4_block38_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block38_1_conv/Conv2D]:289
	                 CONV_2D	         3732.445	    4.495	    4.486	  0.104%	 86.948%	     0.000	        1	[densenet201/conv4_block38_2_conv/Conv2D1]:290
	           CONCATENATION	         3736.940	    0.782	    0.819	  0.019%	 86.967%	     0.000	        1	[densenet201/conv4_block38_concat/concat]:291
	                     MUL	         3737.769	    2.904	    2.901	  0.068%	 87.035%	     0.000	        1	[densenet201/conv4_block39_0_bn/FusedBatchNormV3]:292
	                     ADD	         3740.680	    2.664	    2.663	  0.062%	 87.097%	     0.000	        1	[densenet201/conv4_block39_0_relu/Relu;densenet201/conv4_block39_0_bn/FusedBatchNormV3]:293
	                 CONV_2D	         3743.354	   13.323	   13.305	  0.310%	 87.406%	     0.000	        1	[densenet201/conv4_block39_1_relu/Relu;densenet201/conv4_block39_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block39_1_conv/Conv2D]:294
	                 CONV_2D	         3756.670	    4.451	    4.453	  0.104%	 87.510%	     0.000	        1	[densenet201/conv4_block39_2_conv/Conv2D1]:295
	           CONCATENATION	         3761.132	    0.826	    0.860	  0.020%	 87.530%	     0.000	        1	[densenet201/conv4_block39_concat/concat]:296
	                     MUL	         3762.002	    2.984	    2.959	  0.069%	 87.599%	     0.000	        1	[densenet201/conv4_block40_0_bn/FusedBatchNormV3]:297
	                     ADD	         3764.971	    2.796	    2.799	  0.065%	 87.664%	     0.000	        1	[densenet201/conv4_block40_0_relu/Relu;densenet201/conv4_block40_0_bn/FusedBatchNormV3]:298
	                 CONV_2D	         3767.783	   13.555	   13.542	  0.315%	 87.980%	     0.000	        1	[densenet201/conv4_block40_1_relu/Relu;densenet201/conv4_block40_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block40_1_conv/Conv2D]:299
	                 CONV_2D	         3781.336	    4.472	    4.486	  0.104%	 88.084%	     0.000	        1	[densenet201/conv4_block40_2_conv/Conv2D1]:300
	           CONCATENATION	         3785.832	    0.887	    0.861	  0.020%	 88.104%	     0.000	        1	[densenet201/conv4_block40_concat/concat]:301
	                     MUL	         3786.703	    2.915	    2.920	  0.068%	 88.172%	     0.000	        1	[densenet201/conv4_block41_0_bn/FusedBatchNormV3]:302
	                     ADD	         3789.634	    2.847	    2.821	  0.066%	 88.238%	     0.000	        1	[densenet201/conv4_block41_0_relu/Relu;densenet201/conv4_block41_0_bn/FusedBatchNormV3]:303
	                 CONV_2D	         3792.465	   13.416	   13.439	  0.313%	 88.551%	     0.000	        1	[densenet201/conv4_block41_1_relu/Relu;densenet201/conv4_block41_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block41_1_conv/Conv2D]:304
	                 CONV_2D	         3805.916	    4.483	    4.459	  0.104%	 88.654%	     0.000	        1	[densenet201/conv4_block41_2_conv/Conv2D1]:305
	           CONCATENATION	         3810.383	    0.896	    0.869	  0.020%	 88.675%	     0.000	        1	[densenet201/conv4_block41_concat/concat]:306
	                     MUL	         3811.262	    3.140	    3.158	  0.074%	 88.748%	     0.000	        1	[densenet201/conv4_block42_0_bn/FusedBatchNormV3]:307
	                     ADD	         3814.431	    2.900	    2.875	  0.067%	 88.815%	     0.000	        1	[densenet201/conv4_block42_0_relu/Relu;densenet201/conv4_block42_0_bn/FusedBatchNormV3]:308
	                 CONV_2D	         3817.317	   13.675	   13.718	  0.319%	 89.135%	     0.000	        1	[densenet201/conv4_block42_1_relu/Relu;densenet201/conv4_block42_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block42_1_conv/Conv2D]:309
	                 CONV_2D	         3831.046	    4.524	    4.491	  0.105%	 89.239%	     0.000	        1	[densenet201/conv4_block42_2_conv/Conv2D1]:310
	           CONCATENATION	         3835.546	    0.916	    0.889	  0.021%	 89.260%	     0.000	        1	[densenet201/conv4_block42_concat/concat]:311
	                     MUL	         3836.445	    2.875	    2.937	  0.068%	 89.328%	     0.000	        1	[densenet201/conv4_block43_0_bn/FusedBatchNormV3]:312
	                     ADD	         3839.393	    2.957	    2.952	  0.069%	 89.397%	     0.000	        1	[densenet201/conv4_block43_0_relu/Relu;densenet201/conv4_block43_0_bn/FusedBatchNormV3]:313
	                 CONV_2D	         3842.356	   13.618	   13.650	  0.318%	 89.715%	     0.000	        1	[densenet201/conv4_block43_1_relu/Relu;densenet201/conv4_block43_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block43_1_conv/Conv2D]:314
	                 CONV_2D	         3856.017	    4.478	    4.460	  0.104%	 89.819%	     0.000	        1	[densenet201/conv4_block43_2_conv/Conv2D1]:315
	           CONCATENATION	         3860.486	    0.898	    0.933	  0.022%	 89.840%	     0.000	        1	[densenet201/conv4_block43_concat/concat]:316
	                     MUL	         3861.430	    3.188	    3.221	  0.075%	 89.915%	     0.000	        1	[densenet201/conv4_block44_0_bn/FusedBatchNormV3]:317
	                     ADD	         3864.661	    2.952	    2.973	  0.069%	 89.985%	     0.000	        1	[densenet201/conv4_block44_0_relu/Relu;densenet201/conv4_block44_0_bn/FusedBatchNormV3]:318
	                 CONV_2D	         3867.644	   13.853	   13.864	  0.323%	 90.307%	     0.000	        1	[densenet201/conv4_block44_1_relu/Relu;densenet201/conv4_block44_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block44_1_conv/Conv2D]:319
	                 CONV_2D	         3881.520	    4.481	    4.479	  0.104%	 90.412%	     0.000	        1	[densenet201/conv4_block44_2_conv/Conv2D1]:320
	           CONCATENATION	         3886.008	    0.908	    0.937	  0.022%	 90.434%	     0.000	        1	[densenet201/conv4_block44_concat/concat]:321
	                     MUL	         3886.956	    3.266	    3.237	  0.075%	 90.509%	     0.000	        1	[densenet201/conv4_block45_0_bn/FusedBatchNormV3]:322
	                     ADD	         3890.204	    3.040	    3.068	  0.071%	 90.580%	     0.000	        1	[densenet201/conv4_block45_0_relu/Relu;densenet201/conv4_block45_0_bn/FusedBatchNormV3]:323
	                 CONV_2D	         3893.284	   13.844	   13.810	  0.322%	 90.902%	     0.000	        1	[densenet201/conv4_block45_1_relu/Relu;densenet201/conv4_block45_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block45_1_conv/Conv2D]:324
	                 CONV_2D	         3907.106	    4.470	    4.457	  0.104%	 91.006%	     0.000	        1	[densenet201/conv4_block45_2_conv/Conv2D1]:325
	           CONCATENATION	         3911.574	    0.906	    0.941	  0.022%	 91.028%	     0.000	        1	[densenet201/conv4_block45_concat/concat]:326
	                     MUL	         3912.525	    3.400	    3.446	  0.080%	 91.108%	     0.000	        1	[densenet201/conv4_block46_0_bn/FusedBatchNormV3]:327
	                     ADD	         3915.983	    3.157	    3.128	  0.073%	 91.181%	     0.000	        1	[densenet201/conv4_block46_0_relu/Relu;densenet201/conv4_block46_0_bn/FusedBatchNormV3]:328
	                 CONV_2D	         3919.122	   14.055	   14.055	  0.327%	 91.508%	     0.000	        1	[densenet201/conv4_block46_1_relu/Relu;densenet201/conv4_block46_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block46_1_conv/Conv2D]:329
	                 CONV_2D	         3933.188	    4.478	    4.485	  0.104%	 91.613%	     0.000	        1	[densenet201/conv4_block46_2_conv/Conv2D1]:330
	           CONCATENATION	         3937.682	    0.993	    0.977	  0.023%	 91.635%	     0.000	        1	[densenet201/conv4_block46_concat/concat]:331
	                     MUL	         3938.670	    3.241	    3.253	  0.076%	 91.711%	     0.000	        1	[densenet201/conv4_block47_0_bn/FusedBatchNormV3]:332
	                     ADD	         3941.933	    3.177	    3.169	  0.074%	 91.785%	     0.000	        1	[densenet201/conv4_block47_0_relu/Relu;densenet201/conv4_block47_0_bn/FusedBatchNormV3]:333
	                 CONV_2D	         3945.113	   14.054	   14.076	  0.328%	 92.113%	     0.000	        1	[densenet201/conv4_block47_1_relu/Relu;densenet201/conv4_block47_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block47_1_conv/Conv2D]:334
	                 CONV_2D	         3959.200	    4.508	    4.462	  0.104%	 92.216%	     0.000	        1	[densenet201/conv4_block47_2_conv/Conv2D1]:335
	           CONCATENATION	         3963.671	    0.968	    0.979	  0.023%	 92.239%	     0.000	        1	[densenet201/conv4_block47_concat/concat]:336
	                     MUL	         3964.661	    3.197	    3.211	  0.075%	 92.314%	     0.000	        1	[densenet201/conv4_block48_0_bn/FusedBatchNormV3]:337
	                     ADD	         3967.882	    3.184	    3.215	  0.075%	 92.389%	     0.000	        1	[densenet201/conv4_block48_0_relu/Relu;densenet201/conv4_block48_0_bn/FusedBatchNormV3]:338
	                 CONV_2D	         3971.109	   14.179	   14.182	  0.330%	 92.719%	     0.000	        1	[densenet201/conv4_block48_1_relu/Relu;densenet201/conv4_block48_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv4_block48_1_conv/Conv2D]:339
	                 CONV_2D	         3985.302	    4.479	    4.486	  0.104%	 92.824%	     0.000	        1	[densenet201/conv4_block48_2_conv/Conv2D1]:340
	           CONCATENATION	         3989.797	    0.898	    0.991	  0.023%	 92.847%	     0.000	        1	[densenet201/conv4_block48_concat/concat]:341
	                     MUL	         3990.799	    3.494	    3.482	  0.081%	 92.928%	     0.000	        1	[densenet201/pool4_bn/FusedBatchNormV3]:342
	                     ADD	         3994.292	    3.265	    3.235	  0.075%	 93.003%	     0.000	        1	[densenet201/pool4_relu/Relu;densenet201/pool4_bn/FusedBatchNormV3]:343
	                 CONV_2D	         3997.538	   79.892	   79.317	  1.847%	 94.850%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344
	         AVERAGE_POOL_2D	         4076.866	   10.606	   10.540	  0.245%	 95.095%	     0.000	        1	[densenet201/pool4_pool/AvgPool]:345
	                     MUL	         4087.416	    0.467	    0.431	  0.010%	 95.105%	     0.000	        1	[densenet201/conv5_block1_0_bn/FusedBatchNormV31]:346
	                     ADD	         4087.854	    0.419	    0.420	  0.010%	 95.115%	     0.000	        1	[densenet201/conv5_block1_0_relu/Relu;densenet201/conv5_block1_0_bn/FusedBatchNormV3]:347
	                 CONV_2D	         4088.281	    2.978	    3.003	  0.070%	 95.185%	     0.000	        1	[densenet201/conv5_block1_1_relu/Relu;densenet201/conv5_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block1_1_conv/Conv2D]:348
	                 CONV_2D	         4091.294	    1.173	    1.159	  0.027%	 95.212%	     0.000	        1	[densenet201/conv5_block1_2_conv/Conv2D1]:349
	           CONCATENATION	         4092.460	    0.090	    0.086	  0.002%	 95.214%	     0.000	        1	[densenet201/conv5_block1_concat/concat]:350
	                     MUL	         4092.555	    0.469	    0.472	  0.011%	 95.225%	     0.000	        1	[densenet201/conv5_block2_0_bn/FusedBatchNormV31]:351
	                     ADD	         4093.033	    0.423	    0.426	  0.010%	 95.235%	     0.000	        1	[densenet201/conv5_block2_0_relu/Relu;densenet201/conv5_block2_0_bn/FusedBatchNormV3]:352
	                 CONV_2D	         4093.466	    3.036	    3.036	  0.071%	 95.306%	     0.000	        1	[densenet201/conv5_block2_1_relu/Relu;densenet201/conv5_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block2_1_conv/Conv2D]:353
	                 CONV_2D	         4096.511	    1.144	    1.149	  0.027%	 95.333%	     0.000	        1	[densenet201/conv5_block2_2_conv/Conv2D1]:354
	           CONCATENATION	         4097.667	    0.083	    0.083	  0.002%	 95.335%	     0.000	        1	[densenet201/conv5_block2_concat/concat]:355
	                     MUL	         4097.757	    0.491	    0.491	  0.011%	 95.346%	     0.000	        1	[densenet201/conv5_block3_0_bn/FusedBatchNormV31]:356
	                     ADD	         4098.254	    0.439	    0.444	  0.010%	 95.356%	     0.000	        1	[densenet201/conv5_block3_0_relu/Relu;densenet201/conv5_block3_0_bn/FusedBatchNormV3]:357
	                 CONV_2D	         4098.704	    3.035	    3.016	  0.070%	 95.427%	     0.000	        1	[densenet201/conv5_block3_1_relu/Relu;densenet201/conv5_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block3_1_conv/Conv2D]:358
	                 CONV_2D	         4101.729	    1.146	    1.149	  0.027%	 95.453%	     0.000	        1	[densenet201/conv5_block3_2_conv/Conv2D1]:359
	           CONCATENATION	         4102.886	    0.088	    0.084	  0.002%	 95.455%	     0.000	        1	[densenet201/conv5_block3_concat/concat]:360
	                     MUL	         4102.977	    0.493	    0.493	  0.011%	 95.467%	     0.000	        1	[densenet201/conv5_block4_0_bn/FusedBatchNormV31]:361
	                     ADD	         4103.477	    0.476	    0.464	  0.011%	 95.478%	     0.000	        1	[densenet201/conv5_block4_0_relu/Relu;densenet201/conv5_block4_0_bn/FusedBatchNormV3]:362
	                 CONV_2D	         4103.948	    3.098	    3.083	  0.072%	 95.549%	     0.000	        1	[densenet201/conv5_block4_1_relu/Relu;densenet201/conv5_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block4_1_conv/Conv2D]:363
	                 CONV_2D	         4107.040	    1.155	    1.148	  0.027%	 95.576%	     0.000	        1	[densenet201/conv5_block4_2_conv/Conv2D1]:364
	           CONCATENATION	         4108.195	    0.088	    0.092	  0.002%	 95.578%	     0.000	        1	[densenet201/conv5_block4_concat/concat]:365
	                     MUL	         4108.294	    0.479	    0.481	  0.011%	 95.589%	     0.000	        1	[densenet201/conv5_block5_0_bn/FusedBatchNormV31]:366
	                     ADD	         4108.781	    0.499	    0.483	  0.011%	 95.601%	     0.000	        1	[densenet201/conv5_block5_0_relu/Relu;densenet201/conv5_block5_0_bn/FusedBatchNormV3]:367
	                 CONV_2D	         4109.271	    3.065	    3.065	  0.071%	 95.672%	     0.000	        1	[densenet201/conv5_block5_1_relu/Relu;densenet201/conv5_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block5_1_conv/Conv2D]:368
	                 CONV_2D	         4112.345	    1.141	    1.146	  0.027%	 95.699%	     0.000	        1	[densenet201/conv5_block5_2_conv/Conv2D1]:369
	           CONCATENATION	         4113.498	    0.086	    0.092	  0.002%	 95.701%	     0.000	        1	[densenet201/conv5_block5_concat/concat]:370
	                     MUL	         4113.596	    0.534	    0.530	  0.012%	 95.713%	     0.000	        1	[densenet201/conv5_block6_0_bn/FusedBatchNormV31]:371
	                     ADD	         4114.133	    0.480	    0.493	  0.011%	 95.725%	     0.000	        1	[densenet201/conv5_block6_0_relu/Relu;densenet201/conv5_block6_0_bn/FusedBatchNormV3]:372
	                 CONV_2D	         4114.633	    3.116	    3.113	  0.072%	 95.797%	     0.000	        1	[densenet201/conv5_block6_1_relu/Relu;densenet201/conv5_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block6_1_conv/Conv2D]:373
	                 CONV_2D	         4117.755	    1.143	    1.159	  0.027%	 95.824%	     0.000	        1	[densenet201/conv5_block6_2_conv/Conv2D1]:374
	           CONCATENATION	         4118.922	    0.091	    0.103	  0.002%	 95.827%	     0.000	        1	[densenet201/conv5_block6_concat/concat]:375
	                     MUL	         4119.032	    0.500	    0.503	  0.012%	 95.838%	     0.000	        1	[densenet201/conv5_block7_0_bn/FusedBatchNormV31]:376
	                     ADD	         4119.541	    0.514	    0.505	  0.012%	 95.850%	     0.000	        1	[densenet201/conv5_block7_0_relu/Relu;densenet201/conv5_block7_0_bn/FusedBatchNormV3]:377
	                 CONV_2D	         4120.053	    3.064	    3.100	  0.072%	 95.922%	     0.000	        1	[densenet201/conv5_block7_1_relu/Relu;densenet201/conv5_block7_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block7_1_conv/Conv2D]:378
	                 CONV_2D	         4123.163	    1.155	    1.145	  0.027%	 95.949%	     0.000	        1	[densenet201/conv5_block7_2_conv/Conv2D1]:379
	           CONCATENATION	         4124.316	    0.097	    0.098	  0.002%	 95.951%	     0.000	        1	[densenet201/conv5_block7_concat/concat]:380
	                     MUL	         4124.420	    0.569	    0.572	  0.013%	 95.964%	     0.000	        1	[densenet201/conv5_block8_0_bn/FusedBatchNormV31]:381
	                     ADD	         4124.998	    0.512	    0.520	  0.012%	 95.977%	     0.000	        1	[densenet201/conv5_block8_0_relu/Relu;densenet201/conv5_block8_0_bn/FusedBatchNormV3]:382
	                 CONV_2D	         4125.525	    3.164	    3.167	  0.074%	 96.050%	     0.000	        1	[densenet201/conv5_block8_1_relu/Relu;densenet201/conv5_block8_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block8_1_conv/Conv2D]:383
	                 CONV_2D	         4128.702	    1.142	    1.150	  0.027%	 96.077%	     0.000	        1	[densenet201/conv5_block8_2_conv/Conv2D1]:384
	           CONCATENATION	         4129.860	    0.096	    0.108	  0.003%	 96.080%	     0.000	        1	[densenet201/conv5_block8_concat/concat]:385
	                     MUL	         4129.974	    0.561	    0.574	  0.013%	 96.093%	     0.000	        1	[densenet201/conv5_block9_0_bn/FusedBatchNormV31]:386
	                     ADD	         4130.554	    0.539	    0.546	  0.013%	 96.106%	     0.000	        1	[densenet201/conv5_block9_0_relu/Relu;densenet201/conv5_block9_0_bn/FusedBatchNormV3]:387
	                 CONV_2D	         4131.107	    3.147	    3.149	  0.073%	 96.179%	     0.000	        1	[densenet201/conv5_block9_1_relu/Relu;densenet201/conv5_block9_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block9_1_conv/Conv2D]:388
	                 CONV_2D	         4134.266	    1.152	    1.166	  0.027%	 96.206%	     0.000	        1	[densenet201/conv5_block9_2_conv/Conv2D1]:389
	           CONCATENATION	         4135.439	    0.099	    0.108	  0.003%	 96.209%	     0.000	        1	[densenet201/conv5_block9_concat/concat]:390
	                     MUL	         4135.554	    0.616	    0.603	  0.014%	 96.223%	     0.000	        1	[densenet201/conv5_block10_0_bn/FusedBatchNormV31]:391
	                     ADD	         4136.163	    0.556	    0.562	  0.013%	 96.236%	     0.000	        1	[densenet201/conv5_block10_0_relu/Relu;densenet201/conv5_block10_0_bn/FusedBatchNormV3]:392
	                 CONV_2D	         4136.732	    3.213	    3.219	  0.075%	 96.311%	     0.000	        1	[densenet201/conv5_block10_1_relu/Relu;densenet201/conv5_block10_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block10_1_conv/Conv2D]:393
	                 CONV_2D	         4139.961	    1.141	    1.154	  0.027%	 96.338%	     0.000	        1	[densenet201/conv5_block10_2_conv/Conv2D1]:394
	           CONCATENATION	         4141.122	    0.110	    0.119	  0.003%	 96.340%	     0.000	        1	[densenet201/conv5_block10_concat/concat]:395
	                     MUL	         4141.248	    0.575	    0.578	  0.013%	 96.354%	     0.000	        1	[densenet201/conv5_block11_0_bn/FusedBatchNormV31]:396
	                     ADD	         4141.832	    0.551	    0.559	  0.013%	 96.367%	     0.000	        1	[densenet201/conv5_block11_0_relu/Relu;densenet201/conv5_block11_0_bn/FusedBatchNormV3]:397
	                 CONV_2D	         4142.398	    3.185	    3.180	  0.074%	 96.441%	     0.000	        1	[densenet201/conv5_block11_1_relu/Relu;densenet201/conv5_block11_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block11_1_conv/Conv2D]:398
	                 CONV_2D	         4145.587	    1.143	    1.157	  0.027%	 96.468%	     0.000	        1	[densenet201/conv5_block11_2_conv/Conv2D1]:399
	           CONCATENATION	         4146.752	    0.105	    0.118	  0.003%	 96.471%	     0.000	        1	[densenet201/conv5_block11_concat/concat]:400
	                     MUL	         4146.877	    0.572	    0.576	  0.013%	 96.484%	     0.000	        1	[densenet201/conv5_block12_0_bn/FusedBatchNormV31]:401
	                     ADD	         4147.459	    0.612	    0.581	  0.014%	 96.498%	     0.000	        1	[densenet201/conv5_block12_0_relu/Relu;densenet201/conv5_block12_0_bn/FusedBatchNormV3]:402
	                 CONV_2D	         4148.047	    3.239	    3.258	  0.076%	 96.573%	     0.000	        1	[densenet201/conv5_block12_1_relu/Relu;densenet201/conv5_block12_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block12_1_conv/Conv2D]:403
	                 CONV_2D	         4151.316	    1.168	    1.161	  0.027%	 96.600%	     0.000	        1	[densenet201/conv5_block12_2_conv/Conv2D1]:404
	           CONCATENATION	         4152.484	    0.155	    0.137	  0.003%	 96.604%	     0.000	        1	[densenet201/conv5_block12_concat/concat]:405
	                     MUL	         4152.628	    0.680	    0.653	  0.015%	 96.619%	     0.000	        1	[densenet201/conv5_block13_0_bn/FusedBatchNormV31]:406
	                     ADD	         4153.287	    0.593	    0.602	  0.014%	 96.633%	     0.000	        1	[densenet201/conv5_block13_0_relu/Relu;densenet201/conv5_block13_0_bn/FusedBatchNormV3]:407
	                 CONV_2D	         4153.895	    3.250	    3.240	  0.075%	 96.708%	     0.000	        1	[densenet201/conv5_block13_1_relu/Relu;densenet201/conv5_block13_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block13_1_conv/Conv2D]:408
	                 CONV_2D	         4157.145	    1.143	    1.154	  0.027%	 96.735%	     0.000	        1	[densenet201/conv5_block13_2_conv/Conv2D1]:409
	           CONCATENATION	         4158.307	    0.126	    0.137	  0.003%	 96.738%	     0.000	        1	[densenet201/conv5_block13_concat/concat]:410
	                     MUL	         4158.450	    0.664	    0.666	  0.015%	 96.754%	     0.000	        1	[densenet201/conv5_block14_0_bn/FusedBatchNormV31]:411
	                     ADD	         4159.122	    0.627	    0.606	  0.014%	 96.768%	     0.000	        1	[densenet201/conv5_block14_0_relu/Relu;densenet201/conv5_block14_0_bn/FusedBatchNormV3]:412
	                 CONV_2D	         4159.734	    3.289	    3.310	  0.077%	 96.845%	     0.000	        1	[densenet201/conv5_block14_1_relu/Relu;densenet201/conv5_block14_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block14_1_conv/Conv2D]:413
	                 CONV_2D	         4163.055	    1.170	    1.152	  0.027%	 96.872%	     0.000	        1	[densenet201/conv5_block14_2_conv/Conv2D1]:414
	           CONCATENATION	         4164.215	    0.149	    0.155	  0.004%	 96.875%	     0.000	        1	[densenet201/conv5_block14_concat/concat]:415
	                     MUL	         4164.377	    0.661	    0.674	  0.016%	 96.891%	     0.000	        1	[densenet201/conv5_block15_0_bn/FusedBatchNormV31]:416
	                     ADD	         4165.057	    0.629	    0.635	  0.015%	 96.906%	     0.000	        1	[densenet201/conv5_block15_0_relu/Relu;densenet201/conv5_block15_0_bn/FusedBatchNormV3]:417
	                 CONV_2D	         4165.699	    3.293	    3.287	  0.077%	 96.982%	     0.000	        1	[densenet201/conv5_block15_1_relu/Relu;densenet201/conv5_block15_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block15_1_conv/Conv2D]:418
	                 CONV_2D	         4168.997	    1.145	    1.155	  0.027%	 97.009%	     0.000	        1	[densenet201/conv5_block15_2_conv/Conv2D1]:419
	           CONCATENATION	         4170.160	    0.132	    0.143	  0.003%	 97.013%	     0.000	        1	[densenet201/conv5_block15_concat/concat]:420
	                     MUL	         4170.309	    0.645	    0.659	  0.015%	 97.028%	     0.000	        1	[densenet201/conv5_block16_0_bn/FusedBatchNormV31]:421
	                     ADD	         4170.975	    0.624	    0.630	  0.015%	 97.043%	     0.000	        1	[densenet201/conv5_block16_0_relu/Relu;densenet201/conv5_block16_0_bn/FusedBatchNormV3]:422
	                 CONV_2D	         4171.612	    3.363	    3.358	  0.078%	 97.121%	     0.000	        1	[densenet201/conv5_block16_1_relu/Relu;densenet201/conv5_block16_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block16_1_conv/Conv2D]:423
	                 CONV_2D	         4174.981	    1.162	    1.151	  0.027%	 97.148%	     0.000	        1	[densenet201/conv5_block16_2_conv/Conv2D1]:424
	           CONCATENATION	         4176.140	    0.157	    0.165	  0.004%	 97.152%	     0.000	        1	[densenet201/conv5_block16_concat/concat]:425
	                     MUL	         4176.312	    0.650	    0.654	  0.015%	 97.167%	     0.000	        1	[densenet201/conv5_block17_0_bn/FusedBatchNormV31]:426
	                     ADD	         4176.972	    0.650	    0.660	  0.015%	 97.182%	     0.000	        1	[densenet201/conv5_block17_0_relu/Relu;densenet201/conv5_block17_0_bn/FusedBatchNormV3]:427
	                 CONV_2D	         4177.639	    3.381	    3.339	  0.078%	 97.260%	     0.000	        1	[densenet201/conv5_block17_1_relu/Relu;densenet201/conv5_block17_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block17_1_conv/Conv2D]:428
	                 CONV_2D	         4180.989	    1.143	    1.154	  0.027%	 97.287%	     0.000	        1	[densenet201/conv5_block17_2_conv/Conv2D1]:429
	           CONCATENATION	         4182.151	    0.151	    0.155	  0.004%	 97.290%	     0.000	        1	[densenet201/conv5_block17_concat/concat]:430
	                     MUL	         4182.314	    0.730	    0.726	  0.017%	 97.307%	     0.000	        1	[densenet201/conv5_block18_0_bn/FusedBatchNormV31]:431
	                     ADD	         4183.047	    0.689	    0.668	  0.016%	 97.323%	     0.000	        1	[densenet201/conv5_block18_0_relu/Relu;densenet201/conv5_block18_0_bn/FusedBatchNormV3]:432
	                 CONV_2D	         4183.722	    3.384	    3.412	  0.079%	 97.402%	     0.000	        1	[densenet201/conv5_block18_1_relu/Relu;densenet201/conv5_block18_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block18_1_conv/Conv2D]:433
	                 CONV_2D	         4187.144	    1.172	    1.161	  0.027%	 97.429%	     0.000	        1	[densenet201/conv5_block18_2_conv/Conv2D1]:434
	           CONCATENATION	         4188.313	    0.170	    0.186	  0.004%	 97.434%	     0.000	        1	[densenet201/conv5_block18_concat/concat]:435
	                     MUL	         4188.506	    0.737	    0.755	  0.018%	 97.451%	     0.000	        1	[densenet201/conv5_block19_0_bn/FusedBatchNormV31]:436
	                     ADD	         4189.268	    0.671	    0.683	  0.016%	 97.467%	     0.000	        1	[densenet201/conv5_block19_0_relu/Relu;densenet201/conv5_block19_0_bn/FusedBatchNormV3]:437
	                 CONV_2D	         4189.959	    3.405	    3.395	  0.079%	 97.546%	     0.000	        1	[densenet201/conv5_block19_1_relu/Relu;densenet201/conv5_block19_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block19_1_conv/Conv2D]:438
	                 CONV_2D	         4193.364	    1.153	    1.161	  0.027%	 97.573%	     0.000	        1	[densenet201/conv5_block19_2_conv/Conv2D1]:439
	           CONCATENATION	         4194.533	    0.164	    0.174	  0.004%	 97.577%	     0.000	        1	[densenet201/conv5_block19_concat/concat]:440
	                     MUL	         4194.714	    0.736	    0.743	  0.017%	 97.595%	     0.000	        1	[densenet201/conv5_block20_0_bn/FusedBatchNormV31]:441
	                     ADD	         4195.464	    0.732	    0.715	  0.017%	 97.611%	     0.000	        1	[densenet201/conv5_block20_0_relu/Relu;densenet201/conv5_block20_0_bn/FusedBatchNormV3]:442
	                 CONV_2D	         4196.187	    3.426	    3.445	  0.080%	 97.692%	     0.000	        1	[densenet201/conv5_block20_1_relu/Relu;densenet201/conv5_block20_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block20_1_conv/Conv2D]:443
	                 CONV_2D	         4199.643	    1.174	    1.159	  0.027%	 97.719%	     0.000	        1	[densenet201/conv5_block20_2_conv/Conv2D1]:444
	           CONCATENATION	         4200.809	    0.188	    0.202	  0.005%	 97.723%	     0.000	        1	[densenet201/conv5_block20_concat/concat]:445
	                     MUL	         4201.018	    0.720	    0.726	  0.017%	 97.740%	     0.000	        1	[densenet201/conv5_block21_0_bn/FusedBatchNormV31]:446
	                     ADD	         4201.751	    0.741	    0.721	  0.017%	 97.757%	     0.000	        1	[densenet201/conv5_block21_0_relu/Relu;densenet201/conv5_block21_0_bn/FusedBatchNormV3]:447
	                 CONV_2D	         4202.479	    3.423	    3.430	  0.080%	 97.837%	     0.000	        1	[densenet201/conv5_block21_1_relu/Relu;densenet201/conv5_block21_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block21_1_conv/Conv2D]:448
	                 CONV_2D	         4205.920	    1.150	    1.161	  0.027%	 97.864%	     0.000	        1	[densenet201/conv5_block21_2_conv/Conv2D1]:449
	           CONCATENATION	         4207.089	    0.181	    0.194	  0.005%	 97.868%	     0.000	        1	[densenet201/conv5_block21_concat/concat]:450
	                     MUL	         4207.290	    0.808	    0.788	  0.018%	 97.887%	     0.000	        1	[densenet201/conv5_block22_0_bn/FusedBatchNormV31]:451
	                     ADD	         4208.085	    0.720	    0.734	  0.017%	 97.904%	     0.000	        1	[densenet201/conv5_block22_0_relu/Relu;densenet201/conv5_block22_0_bn/FusedBatchNormV3]:452
	                 CONV_2D	         4208.827	    3.502	    3.512	  0.082%	 97.986%	     0.000	        1	[densenet201/conv5_block22_1_relu/Relu;densenet201/conv5_block22_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block22_1_conv/Conv2D]:453
	                 CONV_2D	         4212.350	    1.150	    1.160	  0.027%	 98.013%	     0.000	        1	[densenet201/conv5_block22_2_conv/Conv2D1]:454
	           CONCATENATION	         4213.517	    0.210	    0.224	  0.005%	 98.018%	     0.000	        1	[densenet201/conv5_block22_concat/concat]:455
	                     MUL	         4213.749	    0.735	    0.748	  0.017%	 98.035%	     0.000	        1	[densenet201/conv5_block23_0_bn/FusedBatchNormV31]:456
	                     ADD	         4214.505	    0.728	    0.742	  0.017%	 98.052%	     0.000	        1	[densenet201/conv5_block23_0_relu/Relu;densenet201/conv5_block23_0_bn/FusedBatchNormV3]:457
	                 CONV_2D	         4215.254	    3.563	    3.475	  0.081%	 98.133%	     0.000	        1	[densenet201/conv5_block23_1_relu/Relu;densenet201/conv5_block23_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block23_1_conv/Conv2D]:458
	                 CONV_2D	         4218.740	    1.182	    1.163	  0.027%	 98.160%	     0.000	        1	[densenet201/conv5_block23_2_conv/Conv2D1]:459
	           CONCATENATION	         4219.911	    0.226	    0.210	  0.005%	 98.165%	     0.000	        1	[densenet201/conv5_block23_concat/concat]:460
	                     MUL	         4220.128	    0.814	    0.821	  0.019%	 98.184%	     0.000	        1	[densenet201/conv5_block24_0_bn/FusedBatchNormV31]:461
	                     ADD	         4220.956	    0.783	    0.756	  0.018%	 98.202%	     0.000	        1	[densenet201/conv5_block24_0_relu/Relu;densenet201/conv5_block24_0_bn/FusedBatchNormV3]:462
	                 CONV_2D	         4221.719	    3.562	    3.555	  0.083%	 98.285%	     0.000	        1	[densenet201/conv5_block24_1_relu/Relu;densenet201/conv5_block24_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block24_1_conv/Conv2D]:463
	                 CONV_2D	         4225.285	    1.171	    1.159	  0.027%	 98.312%	     0.000	        1	[densenet201/conv5_block24_2_conv/Conv2D1]:464
	           CONCATENATION	         4226.453	    0.276	    0.240	  0.006%	 98.317%	     0.000	        1	[densenet201/conv5_block24_concat/concat]:465
	                     MUL	         4226.700	    0.807	    0.816	  0.019%	 98.336%	     0.000	        1	[densenet201/conv5_block25_0_bn/FusedBatchNormV31]:466
	                     ADD	         4227.523	    0.799	    0.784	  0.018%	 98.355%	     0.000	        1	[densenet201/conv5_block25_0_relu/Relu;densenet201/conv5_block25_0_bn/FusedBatchNormV3]:467
	                 CONV_2D	         4228.315	    3.580	    3.596	  0.084%	 98.438%	     0.000	        1	[densenet201/conv5_block25_1_relu/Relu;densenet201/conv5_block25_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block25_1_conv/Conv2D]:468
	                 CONV_2D	         4231.922	    1.153	    1.160	  0.027%	 98.465%	     0.000	        1	[densenet201/conv5_block25_2_conv/Conv2D1]:469
	           CONCATENATION	         4233.090	    0.222	    0.223	  0.005%	 98.471%	     0.000	        1	[densenet201/conv5_block25_concat/concat]:470
	                     MUL	         4233.321	    0.844	    0.850	  0.020%	 98.490%	     0.000	        1	[densenet201/conv5_block26_0_bn/FusedBatchNormV31]:471
	                     ADD	         4234.178	    0.784	    0.806	  0.019%	 98.509%	     0.000	        1	[densenet201/conv5_block26_0_relu/Relu;densenet201/conv5_block26_0_bn/FusedBatchNormV3]:472
	                 CONV_2D	         4234.994	    3.632	    3.599	  0.084%	 98.593%	     0.000	        1	[densenet201/conv5_block26_1_relu/Relu;densenet201/conv5_block26_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block26_1_conv/Conv2D]:473
	                 CONV_2D	         4238.604	    1.163	    1.159	  0.027%	 98.620%	     0.000	        1	[densenet201/conv5_block26_2_conv/Conv2D1]:474
	           CONCATENATION	         4239.771	    0.246	    0.240	  0.006%	 98.626%	     0.000	        1	[densenet201/conv5_block26_concat/concat]:475
	                     MUL	         4240.019	    0.810	    0.821	  0.019%	 98.645%	     0.000	        1	[densenet201/conv5_block27_0_bn/FusedBatchNormV31]:476
	                     ADD	         4240.847	    0.821	    0.804	  0.019%	 98.663%	     0.000	        1	[densenet201/conv5_block27_0_relu/Relu;densenet201/conv5_block27_0_bn/FusedBatchNormV3]:477
	                 CONV_2D	         4241.658	    3.572	    3.564	  0.083%	 98.746%	     0.000	        1	[densenet201/conv5_block27_1_relu/Relu;densenet201/conv5_block27_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block27_1_conv/Conv2D]:478
	                 CONV_2D	         4245.232	    1.150	    1.157	  0.027%	 98.773%	     0.000	        1	[densenet201/conv5_block27_2_conv/Conv2D1]:479
	           CONCATENATION	         4246.397	    0.230	    0.228	  0.005%	 98.779%	     0.000	        1	[densenet201/conv5_block27_concat/concat]:480
	                     MUL	         4246.632	    0.803	    0.814	  0.019%	 98.798%	     0.000	        1	[densenet201/conv5_block28_0_bn/FusedBatchNormV31]:481
	                     ADD	         4247.453	    0.835	    0.823	  0.019%	 98.817%	     0.000	        1	[densenet201/conv5_block28_0_relu/Relu;densenet201/conv5_block28_0_bn/FusedBatchNormV3]:482
	                 CONV_2D	         4248.283	    3.638	    3.655	  0.085%	 98.902%	     0.000	        1	[densenet201/conv5_block28_1_relu/Relu;densenet201/conv5_block28_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block28_1_conv/Conv2D]:483
	                 CONV_2D	         4251.949	    1.158	    1.158	  0.027%	 98.929%	     0.000	        1	[densenet201/conv5_block28_2_conv/Conv2D1]:484
	           CONCATENATION	         4253.115	    0.262	    0.263	  0.006%	 98.935%	     0.000	        1	[densenet201/conv5_block28_concat/concat]:485
	                     MUL	         4253.385	    0.880	    0.890	  0.021%	 98.956%	     0.000	        1	[densenet201/conv5_block29_0_bn/FusedBatchNormV31]:486
	                     ADD	         4254.282	    0.809	    0.822	  0.019%	 98.975%	     0.000	        1	[densenet201/conv5_block29_0_relu/Relu;densenet201/conv5_block29_0_bn/FusedBatchNormV3]:487
	                 CONV_2D	         4255.112	    3.613	    3.622	  0.084%	 99.059%	     0.000	        1	[densenet201/conv5_block29_1_relu/Relu;densenet201/conv5_block29_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block29_1_conv/Conv2D]:488
	                 CONV_2D	         4258.745	    1.168	    1.156	  0.027%	 99.086%	     0.000	        1	[densenet201/conv5_block29_2_conv/Conv2D1]:489
	           CONCATENATION	         4259.909	    0.254	    0.247	  0.006%	 99.092%	     0.000	        1	[densenet201/conv5_block29_concat/concat]:490
	                     MUL	         4260.163	    0.956	    0.933	  0.022%	 99.114%	     0.000	        1	[densenet201/conv5_block30_0_bn/FusedBatchNormV31]:491
	                     ADD	         4261.104	    0.845	    0.850	  0.020%	 99.133%	     0.000	        1	[densenet201/conv5_block30_0_relu/Relu;densenet201/conv5_block30_0_bn/FusedBatchNormV3]:492
	                 CONV_2D	         4261.961	    3.728	    3.701	  0.086%	 99.220%	     0.000	        1	[densenet201/conv5_block30_1_relu/Relu;densenet201/conv5_block30_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block30_1_conv/Conv2D]:493
	                 CONV_2D	         4265.674	    1.155	    1.162	  0.027%	 99.247%	     0.000	        1	[densenet201/conv5_block30_2_conv/Conv2D1]:494
	           CONCATENATION	         4266.844	    0.279	    0.277	  0.006%	 99.253%	     0.000	        1	[densenet201/conv5_block30_concat/concat]:495
	                     MUL	         4267.128	    0.921	    0.920	  0.021%	 99.275%	     0.000	        1	[densenet201/conv5_block31_0_bn/FusedBatchNormV31]:496
	                     ADD	         4268.056	    0.846	    0.857	  0.020%	 99.295%	     0.000	        1	[densenet201/conv5_block31_0_relu/Relu;densenet201/conv5_block31_0_bn/FusedBatchNormV3]:497
	                 CONV_2D	         4268.921	    3.687	    3.666	  0.085%	 99.380%	     0.000	        1	[densenet201/conv5_block31_1_relu/Relu;densenet201/conv5_block31_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block31_1_conv/Conv2D]:498
	                 CONV_2D	         4272.598	    1.154	    1.160	  0.027%	 99.407%	     0.000	        1	[densenet201/conv5_block31_2_conv/Conv2D1]:499
	           CONCATENATION	         4273.766	    0.271	    0.267	  0.006%	 99.413%	     0.000	        1	[densenet201/conv5_block31_concat/concat]:500
	                     MUL	         4274.040	    0.884	    0.898	  0.021%	 99.434%	     0.000	        1	[densenet201/conv5_block32_0_bn/FusedBatchNormV31]:501
	                     ADD	         4274.945	    0.953	    0.890	  0.021%	 99.455%	     0.000	        1	[densenet201/conv5_block32_0_relu/Relu;densenet201/conv5_block32_0_bn/FusedBatchNormV3]:502
	                 CONV_2D	         4275.843	    3.710	    3.752	  0.087%	 99.542%	     0.000	        1	[densenet201/conv5_block32_1_relu/Relu;densenet201/conv5_block32_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv5_block32_1_conv/Conv2D]:503
	                 CONV_2D	         4279.608	    1.150	    1.158	  0.027%	 99.569%	     0.000	        1	[densenet201/conv5_block32_2_conv/Conv2D2]:504
	           CONCATENATION	         4280.773	    0.276	    0.278	  0.006%	 99.576%	     0.000	        1	[densenet201/conv5_block32_concat/concat]:505
	                     MUL	         4281.059	    0.878	    0.886	  0.021%	 99.596%	     0.000	        1	[densenet201/bn/FusedBatchNormV31]:506
	                     ADD	         4281.953	    0.872	    0.899	  0.021%	 99.617%	     0.000	        1	[densenet201/relu/Relu;densenet201/bn/FusedBatchNormV3]:507
	                    MEAN	         4282.859	   15.418	   15.425	  0.359%	 99.976%	     0.000	        1	[densenet201/avg_pool/Mean]:508
	         FULLY_CONNECTED	         4298.292	    0.434	    0.435	  0.010%	 99.986%	     0.000	        1	[densenet201/predictions/MatMul;densenet201/predictions/BiasAdd]:509
	                 SOFTMAX	         4298.735	    0.580	    0.583	  0.014%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:510

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.453	  355.301	  355.142	  8.270%	  8.270%	     0.000	        1	[densenet201/conv1/relu/Relu;densenet201/conv1/bn/FusedBatchNormV3;densenet201/conv1/bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv1/conv/Conv2D]:1
	                 CONV_2D	         1412.587	  163.069	  161.013	  3.749%	 12.019%	     0.000	        1	[densenet201/conv2_block5_1_relu/Relu;densenet201/conv2_block5_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block5_1_conv/Conv2D]:26
	                 CONV_2D	         1662.556	  161.948	  160.399	  3.735%	 15.754%	     0.000	        1	[densenet201/conv2_block6_1_relu/Relu;densenet201/conv2_block6_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block6_1_conv/Conv2D]:31
	                 CONV_2D	         1915.317	  159.911	  159.769	  3.720%	 19.474%	     0.000	        1	[densenet201/pool2_conv/Conv2D1]:36
	                 CONV_2D	         1170.561	  158.187	  156.327	  3.640%	 23.114%	     0.000	        1	[densenet201/conv2_block4_1_relu/Relu;densenet201/conv2_block4_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block4_1_conv/Conv2D]:21
	                 CONV_2D	          700.207	  152.845	  152.870	  3.560%	 26.674%	     0.000	        1	[densenet201/conv2_block2_1_relu/Relu;densenet201/conv2_block2_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block2_1_conv/Conv2D]:11
	                 CONV_2D	          934.674	  152.102	  151.975	  3.539%	 30.213%	     0.000	        1	[densenet201/conv2_block3_1_relu/Relu;densenet201/conv2_block3_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block3_1_conv/Conv2D]:16
	                 CONV_2D	          470.148	  150.732	  150.892	  3.514%	 33.727%	     0.000	        1	[densenet201/conv2_block1_1_relu/Relu;densenet201/conv2_block1_1_bn/FusedBatchNormV3;densenet201/conv2_block1_1_bn/FusedBatchNormV3/ReadVariableOp;densenet201/conv2_block1_1_conv/Conv2D]:6
	             MAX_POOL_2D	          357.241	  109.025	  108.815	  2.534%	 36.260%	     0.000	        1	[densenet201/pool1/MaxPool]:3
	                 CONV_2D	         3997.538	   79.892	   79.317	  1.847%	 38.107%	     0.000	        1	[densenet201/pool4_conv/Conv2D1]:344

Number of nodes executed: 511
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	      200	  3689.399	    85.915%	    85.915%	     0.000	      200
	                     MUL	      102	   188.822	     4.397%	    90.312%	     0.000	      102
	                     ADD	      102	   178.208	     4.150%	    94.461%	     0.000	      102
	             MAX_POOL_2D	        1	   108.814	     2.534%	    96.995%	     0.000	        1
	           CONCATENATION	       98	    59.125	     1.377%	    98.372%	     0.000	       98
	         AVERAGE_POOL_2D	        3	    51.419	     1.197%	    99.570%	     0.000	        3
	                    MEAN	        1	    15.424	     0.359%	    99.929%	     0.000	        1
	                     PAD	        2	     2.039	     0.047%	    99.976%	     0.000	        2
	                 SOFTMAX	        1	     0.583	     0.014%	    99.990%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.435	     0.010%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=4300969 curr=4292819 min=4292430 max=4300969 avg=4.2945e+06 std=1740
Memory (bytes): count=0
511 nodes observed



