STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/f32i8/ResNet152V2.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/f32i8/ResNet152V2.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 147, ), Input shape (52900, 3, ), and Output shape (12544, 64, ), and the ID is 0
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (12544, 32)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 1
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 16)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
Applying Conv Low-Precision for Kernel shape (64, 64, ), Input shape (3136, 64, ), and Output shape (3136, 64, ), and the ID is 2
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 16)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3364, 64, ), and Output shape (3136, 64, ), and the ID is 3	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 80)

	Allocating LowPrecision Activations Tensors with Shape of (3136, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 16)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 4
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
Applying Conv Low-Precision for Kernel shape (64, 256, ), Input shape (3136, 256, ), and Output shape (3136, 64, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 80)
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3364, 64, ), and Output shape (3136, 64, ), and the ID is 6
	Allocating LowPrecision Activations Tensors with Shape of (3136, 80)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (3136, 64, ), and Output shape (3136, 256, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 16)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 16)
Applying Conv Low-Precision for Kernel shape (64, 256, ), Input shape (3136, 256, ), and Output shape (3136, 64, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 32)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 32)
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (3364, 64, ), and Output shape (784, 64, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 80)
	Allocating LowPrecision Activations Tensors with Shape of (784, 80)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 16)
Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (784, 64, ), and Output shape (784, 256, ), and the ID is 10
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (512, 256, ), Input shape (784, 256, ), and Output shape (784, 512, ), and the ID is 11
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 32)
	Allocating LowPrecision Activations Tensors with Shape of (784, 32)
Applying Conv Low-Precision for Kernel shape (128, 256, ), Input shape (784, 256, ), and Output shape (784, 128, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 32)
	Allocating LowPrecision Activations Tensors with Shape of (784, 32)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 13
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 14
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 15
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
, and the ID is 18
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
22
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 24
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
(784, 128, ), and the ID is 25
	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 26
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 27
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 28	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)

	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape (784, 128, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape (784, 128, ), and the ID is 31	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)

	Allocating LowPrecision Activations Tensors with Shape of (784, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (784, 128, ), and Output shape (784, 512, ), and the ID is 32
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (784, 16)
Applying Conv Low-Precision for Kernel shape (128, 512, ), Input shape (784, 512, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 64)
(784, 128, ), and the ID is 33
	Allocating LowPrecision Activations Tensors with Shape of (784, 64)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (900, 128, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
(196, 128, ), and the ID is 34
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
	Allocating LowPrecision Activations Tensors with Shape of (196, 144)
Applying Conv Low-Precision for Kernel shape (512, 128, ), Input shape (196, 128, ), and Output shape (196, 512, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 16)
	Allocating LowPrecision Activations Tensors with Shape of (196, 16)
Applying Conv Low-Precision for Kernel shape (1024, 512, ), Input shape (196, 512, ), and Output shape (196, 1024, ), and the ID is 36
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 64)
	Allocating LowPrecision Activations Tensors with Shape of (196, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 512, ), Input shape (196, 512, ), and Output shape (196, 256, ), and the ID is 37
	Allocating LowPrecision Weight Tensors with Shape of (256, 64)
	Allocating LowPrecision Activations Tensors with Shape of (196, 64)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 39
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 40
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 41
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 42
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 43
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 44
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 45
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 46
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 47
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 48
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 49
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 50
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 51
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 52
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 53
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 54
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 55
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 56
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 57
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 58
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 59
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 60
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 61
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 62
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 63
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 64
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 65
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 66
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 67
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 68
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 69
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
, and the ID is 70
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 71	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)

	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 72
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 73
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 74
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 75
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 76
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 77
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 78
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 79
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 80
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 81
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 82
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 83
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 84
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 85
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 86
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 87
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 88
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 89
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 90
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 91
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 92
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 93
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 94
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 95
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 96
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 97
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 98	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)

	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 99
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 100
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 101
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 102
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 103
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 104
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 105
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 106
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 107
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 108
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 109
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 110
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 111
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 112
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 113
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 114
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 115
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 116	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)

	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 117
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 118
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 119
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 120
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 121
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 122
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 123
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 124
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 125
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 126
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 127
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 128
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 129
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 130
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 131
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 132
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 133
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 134
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 135
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 136
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 137
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 138
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 139
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (196, 256, ), and the ID is 140
	Allocating LowPrecision Activations Tensors with Shape of (196, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (196, 256, ), and Output shape (196, 1024, ), and the ID is 141
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (196, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 128)
Applying Conv Low-Precision for Kernel shape (256, 1024, ), Input shape (196, 1024, ), and Output shape (196, 256, ), and the ID is 142
	Allocating LowPrecision Activations Tensors with Shape of (196, 128)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (256, 256, ), and Output shape (49, 256, ), and the ID is 143
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (52, 288)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (1024, 256, ), Input shape (49, 256, ), and Output shape (49, 1024, ), and the ID is 144
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 32)
	Allocating LowPrecision Activations Tensors with Shape of (52, 32)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 128)
Applying Conv Low-Precision for Kernel shape (2048, 1024, ), Input shape (49, 1024, ), and Output shape (49, 2048, ), and the ID is 145
	Allocating LowPrecision Activations Tensors with Shape of (52, 128)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 128)
Applying Conv Low-Precision for Kernel shape (512, 1024, ), Input shape (49, 1024, ), and Output shape (49, 512, ), and the ID is 146
	Allocating LowPrecision Activations Tensors with Shape of (52, 128)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (81, 512, ), and Output shape (49, 512, ), and the ID is 147
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 64)
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 148
	Allocating LowPrecision Activations Tensors with Shape of (52, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 2048, ), Input shape (49, 2048, ), and Output shape (49, 512, ), and the ID is 149
	Allocating LowPrecision Weight Tensors with Shape of (512, 256)
	Allocating LowPrecision Activations Tensors with Shape of (52, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (81, 512, ), and Output shape (49, 512, ), and the ID is 150
	Allocating LowPrecision Weight Tensors with Shape of (512, 576)
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 64)
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 151
	Allocating LowPrecision Activations Tensors with Shape of (52, 64)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 2048, ), Input shape (49, 2048, ), and Output shape (49, 512, ), and the ID is 152
	Allocating LowPrecision Weight Tensors with Shape of (512, 256)
	Allocating LowPrecision Activations Tensors with Shape of (52, 256)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 576)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (81, 512, ), and Output shape (49, 512, ), and the ID is 153
	Allocating LowPrecision Activations Tensors with Shape of (52, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (2048, 512, ), Input shape (49, 512, ), and Output shape (49, 2048, ), and the ID is 154
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 64)
	Allocating LowPrecision Activations Tensors with Shape of (52, 64)
Applying Low-Precision for shape (1000, 2048, ) and Input shape (1, 2048, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 256)
	Transformed Activation Shape From: (1, 2048) To: (1, 256)
The input model file size (MB): 61.0954
Initialized session in 200.064ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=8567062 curr=8500051 min=8498083 max=8567062 avg=8.50704e+06 std=20060

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=18 first=8510633 curr=8506432 min=8504642 max=8510633 avg=8.50784e+06 std=1628

Inference timings in us: Init: 200064, First inference: 8567062, Warmup (avg): 8.50704e+06, Inference (avg): 8.50784e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=64.6484 overall=166.984
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  174.757	  174.757	100.000%	100.000%	 53768.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  174.757	  174.757	100.000%	100.000%	 53768.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   174.757	   100.000%	   100.000%	 53768.000	        1

Timings (microseconds): count=1 curr=174757
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	            0.110	    0.376	    0.396	  0.005%	  0.005%	     0.000	        1	[resnet152v2/conv1_pad/Pad]:0
	                 CONV_2D	            0.514	  375.075	  374.825	  4.408%	  4.412%	     0.000	        1	[resnet152v2/conv1_conv/BiasAdd;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv1_conv/Conv2D]:1
	                     PAD	          375.351	    1.602	    1.709	  0.020%	  4.433%	     0.000	        1	[resnet152v2/pool1_pad/Pad]:2
	             MAX_POOL_2D	          377.071	  110.055	  109.827	  1.292%	  5.724%	     0.000	        1	[resnet152v2/pool1_pool/MaxPool]:3
	                     MUL	          486.911	    2.251	    2.220	  0.026%	  5.750%	     0.000	        1	[resnet152v2/conv2_block1_preact_bn/FusedBatchNormV3]:4
	                     ADD	          489.140	    1.968	    1.944	  0.023%	  5.773%	     0.000	        1	[resnet152v2/conv2_block1_preact_relu/Relu;resnet152v2/conv2_block1_preact_bn/FusedBatchNormV3]:5
	                 CONV_2D	          491.095	  298.469	  296.950	  3.492%	  9.265%	     0.000	        1	[resnet152v2/conv2_block1_0_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_0_conv/Conv2D]:6
	                 CONV_2D	          788.058	   93.732	   93.864	  1.104%	 10.369%	     0.000	        1	[resnet152v2/conv2_block1_1_relu/Relu;resnet152v2/conv2_block1_1_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_1_conv/Conv2D]:7
	                     PAD	          881.934	    0.589	    0.576	  0.007%	 10.376%	     0.000	        1	[resnet152v2/conv2_block1_2_pad/Pad]:8
	                 CONV_2D	          882.518	   92.029	   92.148	  1.084%	 11.459%	     0.000	        1	[resnet152v2/conv2_block1_2_relu/Relu;resnet152v2/conv2_block1_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_2_conv/Conv2D]:9
	                 CONV_2D	          974.678	  298.721	  298.743	  3.513%	 14.972%	     0.000	        1	[resnet152v2/conv2_block1_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_3_conv/Conv2D]:10
	                     ADD	         1273.433	    7.564	    7.487	  0.088%	 15.060%	     0.000	        1	[resnet152v2/conv2_block1_out/add]:11
	                     MUL	         1280.932	    7.542	    7.607	  0.089%	 15.150%	     0.000	        1	[resnet152v2/conv2_block2_preact_bn/FusedBatchNormV3]:12
	                     ADD	         1288.551	    7.934	    7.816	  0.092%	 15.242%	     0.000	        1	[resnet152v2/conv2_block2_preact_relu/Relu;resnet152v2/conv2_block2_preact_bn/FusedBatchNormV3]:13
	                 CONV_2D	         1296.379	   95.117	   95.252	  1.120%	 16.362%	     0.000	        1	[resnet152v2/conv2_block2_1_relu/Relu;resnet152v2/conv2_block2_1_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_1_conv/Conv2D]:14
	                     PAD	         1391.643	    0.612	    0.569	  0.007%	 16.368%	     0.000	        1	[resnet152v2/conv2_block2_2_pad/Pad]:15
	                 CONV_2D	         1392.220	   92.449	   92.574	  1.089%	 17.457%	     0.000	        1	[resnet152v2/conv2_block2_2_relu/Relu;resnet152v2/conv2_block2_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_2_conv/Conv2D]:16
	                 CONV_2D	         1484.806	  296.708	  296.990	  3.492%	 20.950%	     0.000	        1	[resnet152v2/conv2_block2_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_3_conv/Conv2D]:17
	                     ADD	         1781.809	    7.466	    7.452	  0.088%	 21.037%	     0.000	        1	[resnet152v2/conv2_block2_out/add]:18
	             MAX_POOL_2D	         1789.272	   14.106	   14.188	  0.167%	 21.204%	     0.000	        1	[resnet152v2/max_pooling2d_8/MaxPool]:19
	                     MUL	         1803.472	    7.590	    7.592	  0.089%	 21.293%	     0.000	        1	[resnet152v2/conv2_block3_preact_bn/FusedBatchNormV3]:20
	                     ADD	         1811.077	    7.805	    7.792	  0.092%	 21.385%	     0.000	        1	[resnet152v2/conv2_block3_preact_relu/Relu;resnet152v2/conv2_block3_preact_bn/FusedBatchNormV3]:21
	                 CONV_2D	         1818.882	   95.515	   95.690	  1.125%	 22.510%	     0.000	        1	[resnet152v2/conv2_block3_1_relu/Relu;resnet152v2/conv2_block3_1_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block3_1_conv/Conv2D]:22
	                     PAD	         1914.584	    0.638	    0.596	  0.007%	 22.517%	     0.000	        1	[resnet152v2/conv2_block3_2_pad/Pad]:23
	                 CONV_2D	         1915.188	   23.661	   23.652	  0.278%	 22.795%	     0.000	        1	[resnet152v2/conv2_block3_2_relu/Relu;resnet152v2/conv2_block3_2_bn/FusedBatchNormV3;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block3_2_conv/Conv2D]:24
	                 CONV_2D	         1938.852	   73.861	   73.969	  0.870%	 23.665%	     0.000	        1	[resnet152v2/conv2_block3_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block3_3_conv/Conv2D]:25
	                     ADD	         2012.834	    1.921	    1.890	  0.022%	 23.687%	     0.000	        1	[resnet152v2/conv2_block3_out/add]:26
	                     MUL	         2014.734	    1.897	    1.919	  0.023%	 23.710%	     0.000	        1	[resnet152v2/conv3_block1_preact_bn/FusedBatchNormV3]:27
	                     ADD	         2016.662	    1.968	    1.973	  0.023%	 23.733%	     0.000	        1	[resnet152v2/conv3_block1_preact_relu/Relu;resnet152v2/conv3_block1_preact_bn/FusedBatchNormV3]:28
	                 CONV_2D	         2018.646	  144.314	  144.319	  1.697%	 25.430%	     0.000	        1	[resnet152v2/conv3_block1_0_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block1_0_conv/Conv2D]:29
	                 CONV_2D	         2162.977	   41.547	   41.544	  0.489%	 25.919%	     0.000	        1	[resnet152v2/conv3_block1_1_relu/Relu;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block1_1_conv/Conv2D]:30
	                     PAD	         2204.532	    0.277	    0.236	  0.003%	 25.922%	     0.000	        1	[resnet152v2/conv3_block1_2_pad/Pad]:31
	                 CONV_2D	         2204.775	   44.000	   44.078	  0.518%	 26.440%	     0.000	        1	[resnet152v2/conv3_block1_2_relu/Relu;resnet152v2/conv3_block1_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block1_2_conv/Conv2D]:32
	                 CONV_2D	         2248.865	  147.559	  147.668	  1.737%	 28.176%	     0.000	        1	[resnet152v2/conv3_block1_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block1_3_conv/Conv2D]:33
	                     ADD	         2396.546	    3.714	    3.733	  0.044%	 28.220%	     0.000	        1	[resnet152v2/conv3_block1_out/add]:34
	                     MUL	         2400.290	    4.177	    4.169	  0.049%	 28.269%	     0.000	        1	[resnet152v2/conv3_block2_preact_bn/FusedBatchNormV3]:35
	                     ADD	         2404.471	    3.849	    3.814	  0.045%	 28.314%	     0.000	        1	[resnet152v2/conv3_block2_preact_relu/Relu;resnet152v2/conv3_block2_preact_bn/FusedBatchNormV3]:36
	                 CONV_2D	         2408.297	   41.306	   41.335	  0.486%	 28.800%	     0.000	        1	[resnet152v2/conv3_block2_1_relu/Relu;resnet152v2/conv3_block2_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block2_1_conv/Conv2D]:37
	                     PAD	         2449.644	    0.224	    0.194	  0.002%	 28.803%	     0.000	        1	[resnet152v2/conv3_block2_2_pad/Pad]:38
	                 CONV_2D	         2449.845	   44.019	   44.115	  0.519%	 29.321%	     0.000	        1	[resnet152v2/conv3_block2_2_relu/Relu;resnet152v2/conv3_block2_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block2_2_conv/Conv2D]:39
	                 CONV_2D	         2493.972	  145.548	  145.823	  1.715%	 31.036%	     0.000	        1	[resnet152v2/conv3_block2_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block2_3_conv/Conv2D]:40
	                     ADD	         2639.807	    3.765	    3.745	  0.044%	 31.080%	     0.000	        1	[resnet152v2/conv3_block2_out/add]:41
	                     MUL	         2643.563	    4.189	    4.170	  0.049%	 31.129%	     0.000	        1	[resnet152v2/conv3_block3_preact_bn/FusedBatchNormV3]:42
	                     ADD	         2647.745	    3.821	    3.810	  0.045%	 31.174%	     0.000	        1	[resnet152v2/conv3_block3_preact_relu/Relu;resnet152v2/conv3_block3_preact_bn/FusedBatchNormV3]:43
	                 CONV_2D	         2651.566	   41.621	   41.694	  0.490%	 31.664%	     0.000	        1	[resnet152v2/conv3_block3_1_relu/Relu;resnet152v2/conv3_block3_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block3_1_conv/Conv2D]:44
	                     PAD	         2693.272	    0.176	    0.201	  0.002%	 31.667%	     0.000	        1	[resnet152v2/conv3_block3_2_pad/Pad]:45
	                 CONV_2D	         2693.481	   45.122	   44.831	  0.527%	 32.194%	     0.000	        1	[resnet152v2/conv3_block3_2_relu/Relu;resnet152v2/conv3_block3_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block3_2_conv/Conv2D]:46
	                 CONV_2D	         2738.324	  148.327	  148.023	  1.741%	 33.935%	     0.000	        1	[resnet152v2/conv3_block3_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block3_3_conv/Conv2D]:47
	                     ADD	         2886.359	    3.775	    3.756	  0.044%	 33.979%	     0.000	        1	[resnet152v2/conv3_block3_out/add]:48
	                     MUL	         2890.126	    4.021	    4.149	  0.049%	 34.028%	     0.000	        1	[resnet152v2/conv3_block4_preact_bn/FusedBatchNormV3]:49
	                     ADD	         2894.287	    3.822	    3.822	  0.045%	 34.072%	     0.000	        1	[resnet152v2/conv3_block4_preact_relu/Relu;resnet152v2/conv3_block4_preact_bn/FusedBatchNormV3]:50
	                 CONV_2D	         2898.120	   42.226	   41.880	  0.492%	 34.565%	     0.000	        1	[resnet152v2/conv3_block4_1_relu/Relu;resnet152v2/conv3_block4_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block4_1_conv/Conv2D]:51
	                     PAD	         2940.012	    0.277	    0.250	  0.003%	 34.568%	     0.000	        1	[resnet152v2/conv3_block4_2_pad/Pad]:52
	                 CONV_2D	         2940.270	   44.593	   44.239	  0.520%	 35.088%	     0.000	        1	[resnet152v2/conv3_block4_2_relu/Relu;resnet152v2/conv3_block4_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block4_2_conv/Conv2D]:53
	                 CONV_2D	         2984.521	  147.673	  146.364	  1.721%	 36.809%	     0.000	        1	[resnet152v2/conv3_block4_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block4_3_conv/Conv2D]:54
	                     ADD	         3130.897	    3.742	    3.749	  0.044%	 36.853%	     0.000	        1	[resnet152v2/conv3_block4_out/add]:55
	                     MUL	         3134.656	    4.158	    4.143	  0.049%	 36.902%	     0.000	        1	[resnet152v2/conv3_block5_preact_bn/FusedBatchNormV3]:56
	                     ADD	         3138.810	    3.860	    3.837	  0.045%	 36.947%	     0.000	        1	[resnet152v2/conv3_block5_preact_relu/Relu;resnet152v2/conv3_block5_preact_bn/FusedBatchNormV3]:57
	                 CONV_2D	         3142.659	   41.450	   41.339	  0.486%	 37.433%	     0.000	        1	[resnet152v2/conv3_block5_1_relu/Relu;resnet152v2/conv3_block5_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block5_1_conv/Conv2D]:58
	                     PAD	         3184.009	    0.230	    0.235	  0.003%	 37.436%	     0.000	        1	[resnet152v2/conv3_block5_2_pad/Pad]:59
	                 CONV_2D	         3184.250	   44.588	   44.635	  0.525%	 37.961%	     0.000	        1	[resnet152v2/conv3_block5_2_relu/Relu;resnet152v2/conv3_block5_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block5_2_conv/Conv2D]:60
	                 CONV_2D	         3228.898	  144.901	  144.979	  1.705%	 39.666%	     0.000	        1	[resnet152v2/conv3_block5_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block5_3_conv/Conv2D]:61
	                     ADD	         3373.889	    3.763	    3.763	  0.044%	 39.710%	     0.000	        1	[resnet152v2/conv3_block5_out/add]:62
	                     MUL	         3377.663	    4.147	    4.149	  0.049%	 39.759%	     0.000	        1	[resnet152v2/conv3_block6_preact_bn/FusedBatchNormV3]:63
	                     ADD	         3381.822	    3.817	    3.813	  0.045%	 39.804%	     0.000	        1	[resnet152v2/conv3_block6_preact_relu/Relu;resnet152v2/conv3_block6_preact_bn/FusedBatchNormV3]:64
	                 CONV_2D	         3385.646	   41.680	   41.702	  0.490%	 40.294%	     0.000	        1	[resnet152v2/conv3_block6_1_relu/Relu;resnet152v2/conv3_block6_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block6_1_conv/Conv2D]:65
	                     PAD	         3427.360	    0.238	    0.198	  0.002%	 40.296%	     0.000	        1	[resnet152v2/conv3_block6_2_pad/Pad]:66
	                 CONV_2D	         3427.566	   44.273	   44.350	  0.522%	 40.818%	     0.000	        1	[resnet152v2/conv3_block6_2_relu/Relu;resnet152v2/conv3_block6_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block6_2_conv/Conv2D]:67
	                 CONV_2D	         3471.929	  145.372	  145.558	  1.712%	 42.530%	     0.000	        1	[resnet152v2/conv3_block6_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block6_3_conv/Conv2D]:68
	                     ADD	         3617.499	    3.724	    3.754	  0.044%	 42.574%	     0.000	        1	[resnet152v2/conv3_block6_out/add]:69
	                     MUL	         3621.264	    4.177	    4.149	  0.049%	 42.623%	     0.000	        1	[resnet152v2/conv3_block7_preact_bn/FusedBatchNormV3]:70
	                     ADD	         3625.425	    3.813	    3.831	  0.045%	 42.668%	     0.000	        1	[resnet152v2/conv3_block7_preact_relu/Relu;resnet152v2/conv3_block7_preact_bn/FusedBatchNormV3]:71
	                 CONV_2D	         3629.267	   41.659	   41.732	  0.491%	 43.158%	     0.000	        1	[resnet152v2/conv3_block7_1_relu/Relu;resnet152v2/conv3_block7_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block7_1_conv/Conv2D]:72
	                     PAD	         3671.012	    0.240	    0.242	  0.003%	 43.161%	     0.000	        1	[resnet152v2/conv3_block7_2_pad/Pad]:73
	                 CONV_2D	         3671.262	   44.219	   44.313	  0.521%	 43.682%	     0.000	        1	[resnet152v2/conv3_block7_2_relu/Relu;resnet152v2/conv3_block7_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block7_2_conv/Conv2D]:74
	                 CONV_2D	         3715.587	  147.322	  147.536	  1.735%	 45.417%	     0.000	        1	[resnet152v2/conv3_block7_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block7_3_conv/Conv2D]:75
	                     ADD	         3863.136	    3.720	    3.753	  0.044%	 45.461%	     0.000	        1	[resnet152v2/conv3_block7_out/add]:76
	             MAX_POOL_2D	         3866.900	    6.647	    6.699	  0.079%	 45.540%	     0.000	        1	[resnet152v2/max_pooling2d_9/MaxPool]:77
	                     MUL	         3873.609	    4.186	    4.161	  0.049%	 45.589%	     0.000	        1	[resnet152v2/conv3_block8_preact_bn/FusedBatchNormV3]:78
	                     ADD	         3877.782	    3.775	    3.825	  0.045%	 45.634%	     0.000	        1	[resnet152v2/conv3_block8_preact_relu/Relu;resnet152v2/conv3_block8_preact_bn/FusedBatchNormV3]:79
	                 CONV_2D	         3881.618	   42.119	   42.202	  0.496%	 46.130%	     0.000	        1	[resnet152v2/conv3_block8_1_relu/Relu;resnet152v2/conv3_block8_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block8_1_conv/Conv2D]:80
	                     PAD	         3923.832	    0.227	    0.234	  0.003%	 46.133%	     0.000	        1	[resnet152v2/conv3_block8_2_pad/Pad]:81
	                 CONV_2D	         3924.073	   11.362	   11.376	  0.134%	 46.267%	     0.000	        1	[resnet152v2/conv3_block8_2_relu/Relu;resnet152v2/conv3_block8_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_1_bn/FusedBatchNormV3/ReadVariableOp;resnet152v2/conv3_block8_2_conv/Conv2D]:82
	                 CONV_2D	         3935.460	   37.593	   37.627	  0.442%	 46.709%	     0.000	        1	[resnet152v2/conv3_block8_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block8_3_conv/Conv2D]:83
	                     ADD	         3973.098	    0.938	    0.955	  0.011%	 46.721%	     0.000	        1	[resnet152v2/conv3_block8_out/add]:84
	                     MUL	         3974.061	    1.064	    1.061	  0.012%	 46.733%	     0.000	        1	[resnet152v2/conv4_block1_preact_bn/FusedBatchNormV3]:85
	                     ADD	         3975.131	    0.941	    0.954	  0.011%	 46.744%	     0.000	        1	[resnet152v2/conv4_block1_preact_relu/Relu;resnet152v2/conv4_block1_preact_bn/FusedBatchNormV3]:86
	                 CONV_2D	         3976.094	   72.748	   72.927	  0.858%	 47.602%	     0.000	        1	[resnet152v2/conv4_block1_0_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_0_conv/Conv2D]:87
	                 CONV_2D	         4049.032	   19.365	   19.405	  0.228%	 47.830%	     0.000	        1	[resnet152v2/conv4_block1_1_relu/Relu;resnet152v2/conv4_block1_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_1_conv/Conv2D]:88
	                     PAD	         4068.449	    0.132	    0.121	  0.001%	 47.832%	     0.000	        1	[resnet152v2/conv4_block1_2_pad/Pad]:89
	                 CONV_2D	         4068.577	   21.679	   21.678	  0.255%	 48.086%	     0.000	        1	[resnet152v2/conv4_block1_2_relu/Relu;resnet152v2/conv4_block1_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_2_conv/Conv2D]:90
	                 CONV_2D	         4090.266	   71.771	   71.819	  0.845%	 48.931%	     0.000	        1	[resnet152v2/conv4_block1_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block1_3_conv/Conv2D]:91
	                     ADD	         4162.098	    1.852	    1.885	  0.022%	 48.953%	     0.000	        1	[resnet152v2/conv4_block1_out/add]:92
	                     MUL	         4163.992	    2.034	    2.021	  0.024%	 48.977%	     0.000	        1	[resnet152v2/conv4_block2_preact_bn/FusedBatchNormV3]:93
	                     ADD	         4166.023	    1.872	    1.869	  0.022%	 48.999%	     0.000	        1	[resnet152v2/conv4_block2_preact_relu/Relu;resnet152v2/conv4_block2_preact_bn/FusedBatchNormV3]:94
	                 CONV_2D	         4167.902	   19.469	   19.516	  0.229%	 49.228%	     0.000	        1	[resnet152v2/conv4_block2_1_relu/Relu;resnet152v2/conv4_block2_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block2_1_conv/Conv2D]:95
	                     PAD	         4187.430	    0.105	    0.095	  0.001%	 49.230%	     0.000	        1	[resnet152v2/conv4_block2_2_pad/Pad]:96
	                 CONV_2D	         4187.531	   21.895	   21.921	  0.258%	 49.487%	     0.000	        1	[resnet152v2/conv4_block2_2_relu/Relu;resnet152v2/conv4_block2_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block2_2_conv/Conv2D]:97
	                 CONV_2D	         4209.464	   71.328	   71.550	  0.841%	 50.329%	     0.000	        1	[resnet152v2/conv4_block2_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block2_3_conv/Conv2D]:98
	                     ADD	         4281.029	    1.847	    1.881	  0.022%	 50.351%	     0.000	        1	[resnet152v2/conv4_block2_out/add]:99
	                     MUL	         4282.920	    2.069	    2.027	  0.024%	 50.375%	     0.000	        1	[resnet152v2/conv4_block3_preact_bn/FusedBatchNormV3]:100
	                     ADD	         4284.956	    1.817	    1.850	  0.022%	 50.396%	     0.000	        1	[resnet152v2/conv4_block3_preact_relu/Relu;resnet152v2/conv4_block3_preact_bn/FusedBatchNormV3]:101
	                 CONV_2D	         4286.816	   19.372	   19.383	  0.228%	 50.624%	     0.000	        1	[resnet152v2/conv4_block3_1_relu/Relu;resnet152v2/conv4_block3_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block3_1_conv/Conv2D]:102
	                     PAD	         4306.210	    0.119	    0.125	  0.001%	 50.626%	     0.000	        1	[resnet152v2/conv4_block3_2_pad/Pad]:103
	                 CONV_2D	         4306.341	   22.123	   22.156	  0.261%	 50.886%	     0.000	        1	[resnet152v2/conv4_block3_2_relu/Relu;resnet152v2/conv4_block3_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block3_2_conv/Conv2D]:104
	                 CONV_2D	         4328.510	   71.677	   71.737	  0.844%	 51.730%	     0.000	        1	[resnet152v2/conv4_block3_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block3_3_conv/Conv2D]:105
	                     ADD	         4400.258	    1.927	    1.890	  0.022%	 51.752%	     0.000	        1	[resnet152v2/conv4_block3_out/add]:106
	                     MUL	         4402.158	    1.991	    2.021	  0.024%	 51.776%	     0.000	        1	[resnet152v2/conv4_block4_preact_bn/FusedBatchNormV3]:107
	                     ADD	         4404.189	    1.865	    1.863	  0.022%	 51.798%	     0.000	        1	[resnet152v2/conv4_block4_preact_relu/Relu;resnet152v2/conv4_block4_preact_bn/FusedBatchNormV3]:108
	                 CONV_2D	         4406.062	   19.491	   19.496	  0.229%	 52.027%	     0.000	        1	[resnet152v2/conv4_block4_1_relu/Relu;resnet152v2/conv4_block4_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block4_1_conv/Conv2D]:109
	                     PAD	         4425.571	    0.116	    0.123	  0.001%	 52.029%	     0.000	        1	[resnet152v2/conv4_block4_2_pad/Pad]:110
	                 CONV_2D	         4425.700	   21.622	   21.606	  0.254%	 52.283%	     0.000	        1	[resnet152v2/conv4_block4_2_relu/Relu;resnet152v2/conv4_block4_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block4_2_conv/Conv2D]:111
	                 CONV_2D	         4447.317	   71.801	   71.767	  0.844%	 53.127%	     0.000	        1	[resnet152v2/conv4_block4_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block4_3_conv/Conv2D]:112
	                     ADD	         4519.096	    1.882	    1.864	  0.022%	 53.148%	     0.000	        1	[resnet152v2/conv4_block4_out/add]:113
	                     MUL	         4520.970	    1.991	    2.022	  0.024%	 53.172%	     0.000	        1	[resnet152v2/conv4_block5_preact_bn/FusedBatchNormV3]:114
	                     ADD	         4523.004	    1.850	    1.864	  0.022%	 53.194%	     0.000	        1	[resnet152v2/conv4_block5_preact_relu/Relu;resnet152v2/conv4_block5_preact_bn/FusedBatchNormV3]:115
	                 CONV_2D	         4524.879	   19.337	   19.353	  0.228%	 53.422%	     0.000	        1	[resnet152v2/conv4_block5_1_relu/Relu;resnet152v2/conv4_block5_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block5_1_conv/Conv2D]:116
	                     PAD	         4544.243	    0.091	    0.096	  0.001%	 53.423%	     0.000	        1	[resnet152v2/conv4_block5_2_pad/Pad]:117
	                 CONV_2D	         4544.346	   22.096	   22.080	  0.260%	 53.683%	     0.000	        1	[resnet152v2/conv4_block5_2_relu/Relu;resnet152v2/conv4_block5_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block5_2_conv/Conv2D]:118
	                 CONV_2D	         4566.438	   72.375	   72.361	  0.851%	 54.533%	     0.000	        1	[resnet152v2/conv4_block5_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block5_3_conv/Conv2D]:119
	                     ADD	         4638.811	    1.854	    1.884	  0.022%	 54.556%	     0.000	        1	[resnet152v2/conv4_block5_out/add]:120
	                     MUL	         4640.704	    2.073	    2.033	  0.024%	 54.580%	     0.000	        1	[resnet152v2/conv4_block6_preact_bn/FusedBatchNormV3]:121
	                     ADD	         4642.747	    1.838	    1.857	  0.022%	 54.601%	     0.000	        1	[resnet152v2/conv4_block6_preact_relu/Relu;resnet152v2/conv4_block6_preact_bn/FusedBatchNormV3]:122
	                 CONV_2D	         4644.614	   19.336	   19.339	  0.227%	 54.829%	     0.000	        1	[resnet152v2/conv4_block6_1_relu/Relu;resnet152v2/conv4_block6_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block6_1_conv/Conv2D]:123
	                     PAD	         4663.964	    0.120	    0.119	  0.001%	 54.830%	     0.000	        1	[resnet152v2/conv4_block6_2_pad/Pad]:124
	                 CONV_2D	         4664.090	   21.734	   21.712	  0.255%	 55.086%	     0.000	        1	[resnet152v2/conv4_block6_2_relu/Relu;resnet152v2/conv4_block6_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block6_2_conv/Conv2D]:125
	                 CONV_2D	         4685.815	   72.727	   72.838	  0.857%	 55.942%	     0.000	        1	[resnet152v2/conv4_block6_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block6_3_conv/Conv2D]:126
	                     ADD	         4758.667	    1.847	    1.881	  0.022%	 55.964%	     0.000	        1	[resnet152v2/conv4_block6_out/add]:127
	                     MUL	         4760.558	    2.034	    2.024	  0.024%	 55.988%	     0.000	        1	[resnet152v2/conv4_block7_preact_bn/FusedBatchNormV3]:128
	                     ADD	         4762.592	    1.877	    1.853	  0.022%	 56.010%	     0.000	        1	[resnet152v2/conv4_block7_preact_relu/Relu;resnet152v2/conv4_block7_preact_bn/FusedBatchNormV3]:129
	                 CONV_2D	         4764.455	   19.227	   19.255	  0.226%	 56.236%	     0.000	        1	[resnet152v2/conv4_block7_1_relu/Relu;resnet152v2/conv4_block7_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block7_1_conv/Conv2D]:130
	                     PAD	         4783.721	    0.127	    0.123	  0.001%	 56.238%	     0.000	        1	[resnet152v2/conv4_block7_2_pad/Pad]:131
	                 CONV_2D	         4783.851	   21.706	   21.673	  0.255%	 56.493%	     0.000	        1	[resnet152v2/conv4_block7_2_relu/Relu;resnet152v2/conv4_block7_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block7_2_conv/Conv2D]:132
	                 CONV_2D	         4805.535	   71.577	   71.652	  0.843%	 57.335%	     0.000	        1	[resnet152v2/conv4_block7_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block7_3_conv/Conv2D]:133
	                     ADD	         4877.199	    1.844	    1.877	  0.022%	 57.357%	     0.000	        1	[resnet152v2/conv4_block7_out/add]:134
	                     MUL	         4879.087	    2.024	    2.019	  0.024%	 57.381%	     0.000	        1	[resnet152v2/conv4_block8_preact_bn/FusedBatchNormV3]:135
	                     ADD	         4881.118	    1.860	    1.868	  0.022%	 57.403%	     0.000	        1	[resnet152v2/conv4_block8_preact_relu/Relu;resnet152v2/conv4_block8_preact_bn/FusedBatchNormV3]:136
	                 CONV_2D	         4882.995	   19.230	   19.267	  0.227%	 57.629%	     0.000	        1	[resnet152v2/conv4_block8_1_relu/Relu;resnet152v2/conv4_block8_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block8_1_conv/Conv2D]:137
	                     PAD	         4902.273	    0.114	    0.130	  0.002%	 57.631%	     0.000	        1	[resnet152v2/conv4_block8_2_pad/Pad]:138
	                 CONV_2D	         4902.410	   21.714	   21.801	  0.256%	 57.887%	     0.000	        1	[resnet152v2/conv4_block8_2_relu/Relu;resnet152v2/conv4_block8_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block8_2_conv/Conv2D]:139
	                 CONV_2D	         4924.226	   72.766	   72.750	  0.856%	 58.743%	     0.000	        1	[resnet152v2/conv4_block8_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block8_3_conv/Conv2D]:140
	                     ADD	         4996.988	    1.911	    1.894	  0.022%	 58.765%	     0.000	        1	[resnet152v2/conv4_block8_out/add]:141
	                     MUL	         4998.891	    1.992	    2.021	  0.024%	 58.789%	     0.000	        1	[resnet152v2/conv4_block9_preact_bn/FusedBatchNormV3]:142
	                     ADD	         5000.922	    1.856	    1.858	  0.022%	 58.811%	     0.000	        1	[resnet152v2/conv4_block9_preact_relu/Relu;resnet152v2/conv4_block9_preact_bn/FusedBatchNormV3]:143
	                 CONV_2D	         5002.790	   19.489	   19.466	  0.229%	 59.040%	     0.000	        1	[resnet152v2/conv4_block9_1_relu/Relu;resnet152v2/conv4_block9_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block9_1_conv/Conv2D]:144
	                     PAD	         5022.267	    0.115	    0.124	  0.001%	 59.041%	     0.000	        1	[resnet152v2/conv4_block9_2_pad/Pad]:145
	                 CONV_2D	         5022.398	   21.603	   21.629	  0.254%	 59.295%	     0.000	        1	[resnet152v2/conv4_block9_2_relu/Relu;resnet152v2/conv4_block9_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block9_2_conv/Conv2D]:146
	                 CONV_2D	         5044.039	   71.995	   71.832	  0.845%	 60.140%	     0.000	        1	[resnet152v2/conv4_block9_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block9_3_conv/Conv2D]:147
	                     ADD	         5115.883	    1.930	    1.873	  0.022%	 60.162%	     0.000	        1	[resnet152v2/conv4_block9_out/add]:148
	                     MUL	         5117.766	    1.980	    2.019	  0.024%	 60.186%	     0.000	        1	[resnet152v2/conv4_block10_preact_bn/FusedBatchNormV3]:149
	                     ADD	         5119.795	    1.911	    1.878	  0.022%	 60.208%	     0.000	        1	[resnet152v2/conv4_block10_preact_relu/Relu;resnet152v2/conv4_block10_preact_bn/FusedBatchNormV3]:150
	                 CONV_2D	         5121.685	   19.941	   19.914	  0.234%	 60.442%	     0.000	        1	[resnet152v2/conv4_block10_1_relu/Relu;resnet152v2/conv4_block10_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block10_1_conv/Conv2D]:151
	                     PAD	         5141.611	    0.143	    0.122	  0.001%	 60.444%	     0.000	        1	[resnet152v2/conv4_block10_2_pad/Pad]:152
	                 CONV_2D	         5141.740	   21.744	   21.738	  0.256%	 60.699%	     0.000	        1	[resnet152v2/conv4_block10_2_relu/Relu;resnet152v2/conv4_block10_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block10_2_conv/Conv2D]:153
	                 CONV_2D	         5163.490	   71.472	   71.358	  0.839%	 61.538%	     0.000	        1	[resnet152v2/conv4_block10_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block10_3_conv/Conv2D]:154
	                     ADD	         5234.860	    1.845	    1.885	  0.022%	 61.561%	     0.000	        1	[resnet152v2/conv4_block10_out/add]:155
	                     MUL	         5236.755	    2.050	    2.029	  0.024%	 61.584%	     0.000	        1	[resnet152v2/conv4_block11_preact_bn/FusedBatchNormV3]:156
	                     ADD	         5238.793	    1.821	    1.862	  0.022%	 61.606%	     0.000	        1	[resnet152v2/conv4_block11_preact_relu/Relu;resnet152v2/conv4_block11_preact_bn/FusedBatchNormV3]:157
	                 CONV_2D	         5240.666	   19.338	   19.328	  0.227%	 61.834%	     0.000	        1	[resnet152v2/conv4_block11_1_relu/Relu;resnet152v2/conv4_block11_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block11_1_conv/Conv2D]:158
	                     PAD	         5260.006	    0.108	    0.103	  0.001%	 61.835%	     0.000	        1	[resnet152v2/conv4_block11_2_pad/Pad]:159
	                 CONV_2D	         5260.115	   22.028	   21.949	  0.258%	 62.093%	     0.000	        1	[resnet152v2/conv4_block11_2_relu/Relu;resnet152v2/conv4_block11_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block11_2_conv/Conv2D]:160
	                 CONV_2D	         5282.076	   72.476	   71.972	  0.846%	 62.939%	     0.000	        1	[resnet152v2/conv4_block11_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block11_3_conv/Conv2D]:161
	                     ADD	         5354.060	    1.898	    1.887	  0.022%	 62.961%	     0.000	        1	[resnet152v2/conv4_block11_out/add]:162
	                     MUL	         5355.957	    2.024	    2.025	  0.024%	 62.985%	     0.000	        1	[resnet152v2/conv4_block12_preact_bn/FusedBatchNormV3]:163
	                     ADD	         5357.992	    1.871	    1.868	  0.022%	 63.007%	     0.000	        1	[resnet152v2/conv4_block12_preact_relu/Relu;resnet152v2/conv4_block12_preact_bn/FusedBatchNormV3]:164
	                 CONV_2D	         5359.870	   19.290	   19.088	  0.224%	 63.232%	     0.000	        1	[resnet152v2/conv4_block12_1_relu/Relu;resnet152v2/conv4_block12_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block12_1_conv/Conv2D]:165
	                     PAD	         5378.970	    0.155	    0.134	  0.002%	 63.233%	     0.000	        1	[resnet152v2/conv4_block12_2_pad/Pad]:166
	                 CONV_2D	         5379.110	   21.995	   21.786	  0.256%	 63.489%	     0.000	        1	[resnet152v2/conv4_block12_2_relu/Relu;resnet152v2/conv4_block12_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block12_2_conv/Conv2D]:167
	                 CONV_2D	         5400.908	   71.396	   71.100	  0.836%	 64.326%	     0.000	        1	[resnet152v2/conv4_block12_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block12_3_conv/Conv2D]:168
	                     ADD	         5472.019	    1.895	    1.879	  0.022%	 64.348%	     0.000	        1	[resnet152v2/conv4_block12_out/add]:169
	                     MUL	         5473.908	    1.995	    2.025	  0.024%	 64.371%	     0.000	        1	[resnet152v2/conv4_block13_preact_bn/FusedBatchNormV3]:170
	                     ADD	         5475.943	    1.884	    1.870	  0.022%	 64.393%	     0.000	        1	[resnet152v2/conv4_block13_preact_relu/Relu;resnet152v2/conv4_block13_preact_bn/FusedBatchNormV3]:171
	                 CONV_2D	         5477.825	   19.682	   19.490	  0.229%	 64.623%	     0.000	        1	[resnet152v2/conv4_block13_1_relu/Relu;resnet152v2/conv4_block13_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block13_1_conv/Conv2D]:172
	                     PAD	         5497.327	    0.170	    0.123	  0.001%	 64.624%	     0.000	        1	[resnet152v2/conv4_block13_2_pad/Pad]:173
	                 CONV_2D	         5497.457	   21.804	   21.700	  0.255%	 64.879%	     0.000	        1	[resnet152v2/conv4_block13_2_relu/Relu;resnet152v2/conv4_block13_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block13_2_conv/Conv2D]:174
	                 CONV_2D	         5519.169	   71.185	   71.087	  0.836%	 65.715%	     0.000	        1	[resnet152v2/conv4_block13_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block13_3_conv/Conv2D]:175
	                     ADD	         5590.270	    1.899	    1.889	  0.022%	 65.737%	     0.000	        1	[resnet152v2/conv4_block13_out/add]:176
	                     MUL	         5592.168	    2.029	    2.020	  0.024%	 65.761%	     0.000	        1	[resnet152v2/conv4_block14_preact_bn/FusedBatchNormV3]:177
	                     ADD	         5594.198	    1.867	    1.864	  0.022%	 65.783%	     0.000	        1	[resnet152v2/conv4_block14_preact_relu/Relu;resnet152v2/conv4_block14_preact_bn/FusedBatchNormV3]:178
	                 CONV_2D	         5596.072	   19.151	   19.188	  0.226%	 66.009%	     0.000	        1	[resnet152v2/conv4_block14_1_relu/Relu;resnet152v2/conv4_block14_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block14_1_conv/Conv2D]:179
	                     PAD	         5615.271	    0.111	    0.122	  0.001%	 66.010%	     0.000	        1	[resnet152v2/conv4_block14_2_pad/Pad]:180
	                 CONV_2D	         5615.400	   21.598	   21.645	  0.255%	 66.265%	     0.000	        1	[resnet152v2/conv4_block14_2_relu/Relu;resnet152v2/conv4_block14_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block14_2_conv/Conv2D]:181
	                 CONV_2D	         5637.056	   72.819	   72.778	  0.856%	 67.121%	     0.000	        1	[resnet152v2/conv4_block14_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block14_3_conv/Conv2D]:182
	                     ADD	         5709.845	    1.919	    1.876	  0.022%	 67.143%	     0.000	        1	[resnet152v2/conv4_block14_out/add]:183
	                     MUL	         5711.731	    1.987	    2.022	  0.024%	 67.166%	     0.000	        1	[resnet152v2/conv4_block15_preact_bn/FusedBatchNormV3]:184
	                     ADD	         5713.763	    1.853	    1.867	  0.022%	 67.188%	     0.000	        1	[resnet152v2/conv4_block15_preact_relu/Relu;resnet152v2/conv4_block15_preact_bn/FusedBatchNormV3]:185
	                 CONV_2D	         5715.640	   19.397	   19.361	  0.228%	 67.416%	     0.000	        1	[resnet152v2/conv4_block15_1_relu/Relu;resnet152v2/conv4_block15_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block15_1_conv/Conv2D]:186
	                     PAD	         5735.013	    0.121	    0.128	  0.002%	 67.418%	     0.000	        1	[resnet152v2/conv4_block15_2_pad/Pad]:187
	                 CONV_2D	         5735.148	   21.610	   21.638	  0.254%	 67.672%	     0.000	        1	[resnet152v2/conv4_block15_2_relu/Relu;resnet152v2/conv4_block15_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block15_2_conv/Conv2D]:188
	                 CONV_2D	         5756.797	   73.354	   73.433	  0.864%	 68.535%	     0.000	        1	[resnet152v2/conv4_block15_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block15_3_conv/Conv2D]:189
	                     ADD	         5830.241	    1.890	    1.890	  0.022%	 68.558%	     0.000	        1	[resnet152v2/conv4_block15_out/add]:190
	                     MUL	         5832.141	    2.031	    2.033	  0.024%	 68.582%	     0.000	        1	[resnet152v2/conv4_block16_preact_bn/FusedBatchNormV3]:191
	                     ADD	         5834.184	    1.854	    1.856	  0.022%	 68.603%	     0.000	        1	[resnet152v2/conv4_block16_preact_relu/Relu;resnet152v2/conv4_block16_preact_bn/FusedBatchNormV3]:192
	                 CONV_2D	         5836.052	   19.430	   19.493	  0.229%	 68.833%	     0.000	        1	[resnet152v2/conv4_block16_1_relu/Relu;resnet152v2/conv4_block16_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block16_1_conv/Conv2D]:193
	                     PAD	         5855.557	    0.111	    0.121	  0.001%	 68.834%	     0.000	        1	[resnet152v2/conv4_block16_2_pad/Pad]:194
	                 CONV_2D	         5855.684	   21.931	   21.968	  0.258%	 69.092%	     0.000	        1	[resnet152v2/conv4_block16_2_relu/Relu;resnet152v2/conv4_block16_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block16_2_conv/Conv2D]:195
	                 CONV_2D	         5877.664	   71.811	   71.785	  0.844%	 69.937%	     0.000	        1	[resnet152v2/conv4_block16_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block16_3_conv/Conv2D]:196
	                     ADD	         5949.462	    1.885	    1.876	  0.022%	 69.959%	     0.000	        1	[resnet152v2/conv4_block16_out/add]:197
	                     MUL	         5951.350	    1.993	    2.013	  0.024%	 69.982%	     0.000	        1	[resnet152v2/conv4_block17_preact_bn/FusedBatchNormV3]:198
	                     ADD	         5953.372	    1.857	    1.877	  0.022%	 70.004%	     0.000	        1	[resnet152v2/conv4_block17_preact_relu/Relu;resnet152v2/conv4_block17_preact_bn/FusedBatchNormV3]:199
	                 CONV_2D	         5955.260	   19.481	   19.426	  0.228%	 70.233%	     0.000	        1	[resnet152v2/conv4_block17_1_relu/Relu;resnet152v2/conv4_block17_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block17_1_conv/Conv2D]:200
	                     PAD	         5974.697	    0.133	    0.127	  0.001%	 70.234%	     0.000	        1	[resnet152v2/conv4_block17_2_pad/Pad]:201
	                 CONV_2D	         5974.831	   21.659	   21.709	  0.255%	 70.490%	     0.000	        1	[resnet152v2/conv4_block17_2_relu/Relu;resnet152v2/conv4_block17_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block17_2_conv/Conv2D]:202
	                 CONV_2D	         5996.552	   71.573	   71.703	  0.843%	 71.333%	     0.000	        1	[resnet152v2/conv4_block17_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block17_3_conv/Conv2D]:203
	                     ADD	         6068.267	    1.889	    1.888	  0.022%	 71.355%	     0.000	        1	[resnet152v2/conv4_block17_out/add]:204
	                     MUL	         6070.165	    2.023	    2.025	  0.024%	 71.379%	     0.000	        1	[resnet152v2/conv4_block18_preact_bn/FusedBatchNormV3]:205
	                     ADD	         6072.200	    1.875	    1.858	  0.022%	 71.401%	     0.000	        1	[resnet152v2/conv4_block18_preact_relu/Relu;resnet152v2/conv4_block18_preact_bn/FusedBatchNormV3]:206
	                 CONV_2D	         6074.068	   19.204	   19.264	  0.227%	 71.627%	     0.000	        1	[resnet152v2/conv4_block18_1_relu/Relu;resnet152v2/conv4_block18_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block18_1_conv/Conv2D]:207
	                     PAD	         6093.343	    0.087	    0.101	  0.001%	 71.628%	     0.000	        1	[resnet152v2/conv4_block18_2_pad/Pad]:208
	                 CONV_2D	         6093.450	   22.126	   22.191	  0.261%	 71.889%	     0.000	        1	[resnet152v2/conv4_block18_2_relu/Relu;resnet152v2/conv4_block18_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block18_2_conv/Conv2D]:209
	                 CONV_2D	         6115.653	   71.320	   71.513	  0.841%	 72.730%	     0.000	        1	[resnet152v2/conv4_block18_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block18_3_conv/Conv2D]:210
	                     ADD	         6187.177	    1.881	    1.874	  0.022%	 72.752%	     0.000	        1	[resnet152v2/conv4_block18_out/add]:211
	                     MUL	         6189.061	    1.992	    2.026	  0.024%	 72.776%	     0.000	        1	[resnet152v2/conv4_block19_preact_bn/FusedBatchNormV3]:212
	                     ADD	         6191.097	    1.893	    1.871	  0.022%	 72.798%	     0.000	        1	[resnet152v2/conv4_block19_preact_relu/Relu;resnet152v2/conv4_block19_preact_bn/FusedBatchNormV3]:213
	                 CONV_2D	         6192.978	   19.103	   19.147	  0.225%	 73.023%	     0.000	        1	[resnet152v2/conv4_block19_1_relu/Relu;resnet152v2/conv4_block19_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block19_1_conv/Conv2D]:214
	                     PAD	         6212.137	    0.105	    0.119	  0.001%	 73.025%	     0.000	        1	[resnet152v2/conv4_block19_2_pad/Pad]:215
	                 CONV_2D	         6212.262	   21.520	   21.550	  0.253%	 73.278%	     0.000	        1	[resnet152v2/conv4_block19_2_relu/Relu;resnet152v2/conv4_block19_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block19_2_conv/Conv2D]:216
	                 CONV_2D	         6233.824	   72.154	   72.231	  0.849%	 74.128%	     0.000	        1	[resnet152v2/conv4_block19_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block19_3_conv/Conv2D]:217
	                     ADD	         6306.067	    1.846	    1.879	  0.022%	 74.150%	     0.000	        1	[resnet152v2/conv4_block19_out/add]:218
	                     MUL	         6307.959	    2.079	    2.030	  0.024%	 74.174%	     0.000	        1	[resnet152v2/conv4_block20_preact_bn/FusedBatchNormV3]:219
	                     ADD	         6309.999	    1.819	    1.856	  0.022%	 74.195%	     0.000	        1	[resnet152v2/conv4_block20_preact_relu/Relu;resnet152v2/conv4_block20_preact_bn/FusedBatchNormV3]:220
	                 CONV_2D	         6311.865	   19.260	   19.298	  0.227%	 74.422%	     0.000	        1	[resnet152v2/conv4_block20_1_relu/Relu;resnet152v2/conv4_block20_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block20_1_conv/Conv2D]:221
	                     PAD	         6331.175	    0.110	    0.127	  0.001%	 74.424%	     0.000	        1	[resnet152v2/conv4_block20_2_pad/Pad]:222
	                 CONV_2D	         6331.308	   21.796	   21.785	  0.256%	 74.680%	     0.000	        1	[resnet152v2/conv4_block20_2_relu/Relu;resnet152v2/conv4_block20_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block20_2_conv/Conv2D]:223
	                 CONV_2D	         6353.105	   71.682	   71.963	  0.846%	 75.526%	     0.000	        1	[resnet152v2/conv4_block20_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block20_3_conv/Conv2D]:224
	                     ADD	         6425.080	    1.893	    1.887	  0.022%	 75.548%	     0.000	        1	[resnet152v2/conv4_block20_out/add]:225
	                     MUL	         6426.977	    2.059	    2.021	  0.024%	 75.572%	     0.000	        1	[resnet152v2/conv4_block21_preact_bn/FusedBatchNormV3]:226
	                     ADD	         6429.008	    1.844	    1.866	  0.022%	 75.594%	     0.000	        1	[resnet152v2/conv4_block21_preact_relu/Relu;resnet152v2/conv4_block21_preact_bn/FusedBatchNormV3]:227
	                 CONV_2D	         6430.884	   19.326	   19.368	  0.228%	 75.822%	     0.000	        1	[resnet152v2/conv4_block21_1_relu/Relu;resnet152v2/conv4_block21_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block21_1_conv/Conv2D]:228
	                     PAD	         6450.264	    0.121	    0.131	  0.002%	 75.823%	     0.000	        1	[resnet152v2/conv4_block21_2_pad/Pad]:229
	                 CONV_2D	         6450.401	   21.638	   21.650	  0.255%	 76.078%	     0.000	        1	[resnet152v2/conv4_block21_2_relu/Relu;resnet152v2/conv4_block21_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block21_2_conv/Conv2D]:230
	                 CONV_2D	         6472.063	   73.128	   73.186	  0.861%	 76.939%	     0.000	        1	[resnet152v2/conv4_block21_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block21_3_conv/Conv2D]:231
	                     ADD	         6545.261	    1.844	    1.877	  0.022%	 76.961%	     0.000	        1	[resnet152v2/conv4_block21_out/add]:232
	                     MUL	         6547.147	    2.066	    2.031	  0.024%	 76.985%	     0.000	        1	[resnet152v2/conv4_block22_preact_bn/FusedBatchNormV3]:233
	                     ADD	         6549.187	    1.858	    1.864	  0.022%	 77.007%	     0.000	        1	[resnet152v2/conv4_block22_preact_relu/Relu;resnet152v2/conv4_block22_preact_bn/FusedBatchNormV3]:234
	                 CONV_2D	         6551.062	   19.407	   19.454	  0.229%	 77.235%	     0.000	        1	[resnet152v2/conv4_block22_1_relu/Relu;resnet152v2/conv4_block22_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block22_1_conv/Conv2D]:235
	                     PAD	         6570.527	    0.117	    0.122	  0.001%	 77.237%	     0.000	        1	[resnet152v2/conv4_block22_2_pad/Pad]:236
	                 CONV_2D	         6570.656	   21.771	   21.801	  0.256%	 77.493%	     0.000	        1	[resnet152v2/conv4_block22_2_relu/Relu;resnet152v2/conv4_block22_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block22_2_conv/Conv2D]:237
	                 CONV_2D	         6592.469	   71.914	   72.093	  0.848%	 78.341%	     0.000	        1	[resnet152v2/conv4_block22_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block22_3_conv/Conv2D]:238
	                     ADD	         6664.573	    1.872	    1.882	  0.022%	 78.363%	     0.000	        1	[resnet152v2/conv4_block22_out/add]:239
	                     MUL	         6666.464	    2.029	    2.030	  0.024%	 78.387%	     0.000	        1	[resnet152v2/conv4_block23_preact_bn/FusedBatchNormV3]:240
	                     ADD	         6668.506	    1.864	    1.862	  0.022%	 78.409%	     0.000	        1	[resnet152v2/conv4_block23_preact_relu/Relu;resnet152v2/conv4_block23_preact_bn/FusedBatchNormV3]:241
	                 CONV_2D	         6670.378	   19.172	   19.186	  0.226%	 78.634%	     0.000	        1	[resnet152v2/conv4_block23_1_relu/Relu;resnet152v2/conv4_block23_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block23_1_conv/Conv2D]:242
	                     PAD	         6689.576	    0.091	    0.100	  0.001%	 78.636%	     0.000	        1	[resnet152v2/conv4_block23_2_pad/Pad]:243
	                 CONV_2D	         6689.682	   21.500	   21.500	  0.253%	 78.888%	     0.000	        1	[resnet152v2/conv4_block23_2_relu/Relu;resnet152v2/conv4_block23_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block23_2_conv/Conv2D]:244
	                 CONV_2D	         6711.194	   71.763	   71.754	  0.844%	 79.732%	     0.000	        1	[resnet152v2/conv4_block23_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block23_3_conv/Conv2D]:245
	                     ADD	         6782.960	    1.931	    1.886	  0.022%	 79.754%	     0.000	        1	[resnet152v2/conv4_block23_out/add]:246
	                     MUL	         6784.855	    1.993	    2.021	  0.024%	 79.778%	     0.000	        1	[resnet152v2/conv4_block24_preact_bn/FusedBatchNormV3]:247
	                     ADD	         6786.886	    1.861	    1.858	  0.022%	 79.800%	     0.000	        1	[resnet152v2/conv4_block24_preact_relu/Relu;resnet152v2/conv4_block24_preact_bn/FusedBatchNormV3]:248
	                 CONV_2D	         6788.755	   19.447	   19.394	  0.228%	 80.028%	     0.000	        1	[resnet152v2/conv4_block24_1_relu/Relu;resnet152v2/conv4_block24_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block24_1_conv/Conv2D]:249
	                     PAD	         6808.160	    0.112	    0.125	  0.001%	 80.029%	     0.000	        1	[resnet152v2/conv4_block24_2_pad/Pad]:250
	                 CONV_2D	         6808.292	   21.742	   21.663	  0.255%	 80.284%	     0.000	        1	[resnet152v2/conv4_block24_2_relu/Relu;resnet152v2/conv4_block24_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block24_2_conv/Conv2D]:251
	                 CONV_2D	         6829.968	   71.904	   72.118	  0.848%	 81.132%	     0.000	        1	[resnet152v2/conv4_block24_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block24_3_conv/Conv2D]:252
	                     ADD	         6902.098	    1.847	    1.885	  0.022%	 81.154%	     0.000	        1	[resnet152v2/conv4_block24_out/add]:253
	                     MUL	         6903.993	    2.032	    2.026	  0.024%	 81.178%	     0.000	        1	[resnet152v2/conv4_block25_preact_bn/FusedBatchNormV3]:254
	                     ADD	         6906.029	    1.857	    1.861	  0.022%	 81.200%	     0.000	        1	[resnet152v2/conv4_block25_preact_relu/Relu;resnet152v2/conv4_block25_preact_bn/FusedBatchNormV3]:255
	                 CONV_2D	         6907.902	   19.441	   19.404	  0.228%	 81.428%	     0.000	        1	[resnet152v2/conv4_block25_1_relu/Relu;resnet152v2/conv4_block25_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block25_1_conv/Conv2D]:256
	                     PAD	         6927.318	    0.091	    0.102	  0.001%	 81.430%	     0.000	        1	[resnet152v2/conv4_block25_2_pad/Pad]:257
	                 CONV_2D	         6927.426	   21.953	   21.984	  0.259%	 81.688%	     0.000	        1	[resnet152v2/conv4_block25_2_relu/Relu;resnet152v2/conv4_block25_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block25_2_conv/Conv2D]:258
	                 CONV_2D	         6949.422	   72.357	   72.370	  0.851%	 82.539%	     0.000	        1	[resnet152v2/conv4_block25_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block25_3_conv/Conv2D]:259
	                     ADD	         7021.803	    1.849	    1.881	  0.022%	 82.561%	     0.000	        1	[resnet152v2/conv4_block25_out/add]:260
	                     MUL	         7023.694	    2.076	    2.024	  0.024%	 82.585%	     0.000	        1	[resnet152v2/conv4_block26_preact_bn/FusedBatchNormV3]:261
	                     ADD	         7025.727	    1.816	    1.863	  0.022%	 82.607%	     0.000	        1	[resnet152v2/conv4_block26_preact_relu/Relu;resnet152v2/conv4_block26_preact_bn/FusedBatchNormV3]:262
	                 CONV_2D	         7027.600	   19.302	   19.355	  0.228%	 82.835%	     0.000	        1	[resnet152v2/conv4_block26_1_relu/Relu;resnet152v2/conv4_block26_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block26_1_conv/Conv2D]:263
	                     PAD	         7046.968	    0.106	    0.120	  0.001%	 82.836%	     0.000	        1	[resnet152v2/conv4_block26_2_pad/Pad]:264
	                 CONV_2D	         7047.094	   21.656	   21.689	  0.255%	 83.091%	     0.000	        1	[resnet152v2/conv4_block26_2_relu/Relu;resnet152v2/conv4_block26_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block26_2_conv/Conv2D]:265
	                 CONV_2D	         7068.795	   72.556	   72.549	  0.853%	 83.944%	     0.000	        1	[resnet152v2/conv4_block26_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block26_3_conv/Conv2D]:266
	                     ADD	         7141.356	    1.885	    1.885	  0.022%	 83.966%	     0.000	        1	[resnet152v2/conv4_block26_out/add]:267
	                     MUL	         7143.251	    2.034	    2.031	  0.024%	 83.990%	     0.000	        1	[resnet152v2/conv4_block27_preact_bn/FusedBatchNormV3]:268
	                     ADD	         7145.291	    1.898	    1.874	  0.022%	 84.012%	     0.000	        1	[resnet152v2/conv4_block27_preact_relu/Relu;resnet152v2/conv4_block27_preact_bn/FusedBatchNormV3]:269
	                 CONV_2D	         7147.176	   19.666	   19.739	  0.232%	 84.244%	     0.000	        1	[resnet152v2/conv4_block27_1_relu/Relu;resnet152v2/conv4_block27_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block27_1_conv/Conv2D]:270
	                     PAD	         7166.926	    0.121	    0.125	  0.001%	 84.246%	     0.000	        1	[resnet152v2/conv4_block27_2_pad/Pad]:271
	                 CONV_2D	         7167.058	   21.694	   21.700	  0.255%	 84.501%	     0.000	        1	[resnet152v2/conv4_block27_2_relu/Relu;resnet152v2/conv4_block27_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block27_2_conv/Conv2D]:272
	                 CONV_2D	         7188.769	   71.843	   71.887	  0.845%	 85.346%	     0.000	        1	[resnet152v2/conv4_block27_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block27_3_conv/Conv2D]:273
	                     ADD	         7260.668	    1.870	    1.882	  0.022%	 85.368%	     0.000	        1	[resnet152v2/conv4_block27_out/add]:274
	                     MUL	         7262.560	    2.029	    2.033	  0.024%	 85.392%	     0.000	        1	[resnet152v2/conv4_block28_preact_bn/FusedBatchNormV3]:275
	                     ADD	         7264.604	    1.857	    1.854	  0.022%	 85.414%	     0.000	        1	[resnet152v2/conv4_block28_preact_relu/Relu;resnet152v2/conv4_block28_preact_bn/FusedBatchNormV3]:276
	                 CONV_2D	         7266.468	   19.417	   19.407	  0.228%	 85.642%	     0.000	        1	[resnet152v2/conv4_block28_1_relu/Relu;resnet152v2/conv4_block28_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block28_1_conv/Conv2D]:277
	                     PAD	         7285.888	    0.091	    0.095	  0.001%	 85.644%	     0.000	        1	[resnet152v2/conv4_block28_2_pad/Pad]:278
	                 CONV_2D	         7285.990	   21.578	   21.643	  0.255%	 85.898%	     0.000	        1	[resnet152v2/conv4_block28_2_relu/Relu;resnet152v2/conv4_block28_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block28_2_conv/Conv2D]:279
	                 CONV_2D	         7307.647	   72.854	   73.026	  0.859%	 86.757%	     0.000	        1	[resnet152v2/conv4_block28_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block28_3_conv/Conv2D]:280
	                     ADD	         7380.685	    1.916	    1.881	  0.022%	 86.779%	     0.000	        1	[resnet152v2/conv4_block28_out/add]:281
	                     MUL	         7382.576	    1.992	    2.022	  0.024%	 86.803%	     0.000	        1	[resnet152v2/conv4_block29_preact_bn/FusedBatchNormV3]:282
	                     ADD	         7384.607	    1.878	    1.869	  0.022%	 86.825%	     0.000	        1	[resnet152v2/conv4_block29_preact_relu/Relu;resnet152v2/conv4_block29_preact_bn/FusedBatchNormV3]:283
	                 CONV_2D	         7386.487	   19.839	   19.835	  0.233%	 87.058%	     0.000	        1	[resnet152v2/conv4_block29_1_relu/Relu;resnet152v2/conv4_block29_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block29_1_conv/Conv2D]:284
	                     PAD	         7406.334	    0.109	    0.125	  0.001%	 87.059%	     0.000	        1	[resnet152v2/conv4_block29_2_pad/Pad]:285
	                 CONV_2D	         7406.466	   21.768	   21.767	  0.256%	 87.315%	     0.000	        1	[resnet152v2/conv4_block29_2_relu/Relu;resnet152v2/conv4_block29_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block29_2_conv/Conv2D]:286
	                 CONV_2D	         7428.245	   71.998	   71.930	  0.846%	 88.161%	     0.000	        1	[resnet152v2/conv4_block29_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block29_3_conv/Conv2D]:287
	                     ADD	         7500.186	    1.893	    1.878	  0.022%	 88.183%	     0.000	        1	[resnet152v2/conv4_block29_out/add]:288
	                     MUL	         7502.074	    2.014	    2.030	  0.024%	 88.207%	     0.000	        1	[resnet152v2/conv4_block30_preact_bn/FusedBatchNormV3]:289
	                     ADD	         7504.113	    1.871	    1.855	  0.022%	 88.229%	     0.000	        1	[resnet152v2/conv4_block30_preact_relu/Relu;resnet152v2/conv4_block30_preact_bn/FusedBatchNormV3]:290
	                 CONV_2D	         7505.979	   19.381	   19.128	  0.225%	 88.454%	     0.000	        1	[resnet152v2/conv4_block30_1_relu/Relu;resnet152v2/conv4_block30_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block30_1_conv/Conv2D]:291
	                     PAD	         7525.118	    0.153	    0.122	  0.001%	 88.455%	     0.000	        1	[resnet152v2/conv4_block30_2_pad/Pad]:292
	                 CONV_2D	         7525.246	   21.893	   21.864	  0.257%	 88.712%	     0.000	        1	[resnet152v2/conv4_block30_2_relu/Relu;resnet152v2/conv4_block30_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block30_2_conv/Conv2D]:293
	                 CONV_2D	         7547.123	   72.104	   72.119	  0.848%	 89.561%	     0.000	        1	[resnet152v2/conv4_block30_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block30_3_conv/Conv2D]:294
	                     ADD	         7619.254	    1.937	    1.890	  0.022%	 89.583%	     0.000	        1	[resnet152v2/conv4_block30_out/add]:295
	                     MUL	         7621.154	    2.014	    2.021	  0.024%	 89.607%	     0.000	        1	[resnet152v2/conv4_block31_preact_bn/FusedBatchNormV3]:296
	                     ADD	         7623.184	    1.881	    1.873	  0.022%	 89.629%	     0.000	        1	[resnet152v2/conv4_block31_preact_relu/Relu;resnet152v2/conv4_block31_preact_bn/FusedBatchNormV3]:297
	                 CONV_2D	         7625.074	   19.395	   19.358	  0.228%	 89.856%	     0.000	        1	[resnet152v2/conv4_block31_1_relu/Relu;resnet152v2/conv4_block31_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block31_1_conv/Conv2D]:298
	                     PAD	         7644.444	    0.094	    0.095	  0.001%	 89.857%	     0.000	        1	[resnet152v2/conv4_block31_2_pad/Pad]:299
	                 CONV_2D	         7644.545	   21.685	   21.583	  0.254%	 90.111%	     0.000	        1	[resnet152v2/conv4_block31_2_relu/Relu;resnet152v2/conv4_block31_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block31_2_conv/Conv2D]:300
	                 CONV_2D	         7666.140	   73.205	   72.705	  0.855%	 90.966%	     0.000	        1	[resnet152v2/conv4_block31_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block31_3_conv/Conv2D]:301
	                     ADD	         7738.856	    1.896	    1.881	  0.022%	 90.988%	     0.000	        1	[resnet152v2/conv4_block31_out/add]:302
	                     MUL	         7740.747	    1.966	    2.027	  0.024%	 91.012%	     0.000	        1	[resnet152v2/conv4_block32_preact_bn/FusedBatchNormV3]:303
	                     ADD	         7742.784	    1.909	    1.855	  0.022%	 91.034%	     0.000	        1	[resnet152v2/conv4_block32_preact_relu/Relu;resnet152v2/conv4_block32_preact_bn/FusedBatchNormV3]:304
	                 CONV_2D	         7744.650	   19.315	   19.163	  0.225%	 91.259%	     0.000	        1	[resnet152v2/conv4_block32_1_relu/Relu;resnet152v2/conv4_block32_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block32_1_conv/Conv2D]:305
	                     PAD	         7763.824	    0.162	    0.126	  0.001%	 91.261%	     0.000	        1	[resnet152v2/conv4_block32_2_pad/Pad]:306
	                 CONV_2D	         7763.956	   22.038	   21.887	  0.257%	 91.518%	     0.000	        1	[resnet152v2/conv4_block32_2_relu/Relu;resnet152v2/conv4_block32_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block32_2_conv/Conv2D]:307
	                 CONV_2D	         7785.855	   72.921	   72.672	  0.855%	 92.373%	     0.000	        1	[resnet152v2/conv4_block32_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block32_3_conv/Conv2D]:308
	                     ADD	         7858.539	    1.937	    1.892	  0.022%	 92.395%	     0.000	        1	[resnet152v2/conv4_block32_out/add]:309
	                     MUL	         7860.441	    2.024	    2.028	  0.024%	 92.419%	     0.000	        1	[resnet152v2/conv4_block33_preact_bn/FusedBatchNormV3]:310
	                     ADD	         7862.479	    1.873	    1.857	  0.022%	 92.441%	     0.000	        1	[resnet152v2/conv4_block33_preact_relu/Relu;resnet152v2/conv4_block33_preact_bn/FusedBatchNormV3]:311
	                 CONV_2D	         7864.346	   19.974	   19.577	  0.230%	 92.671%	     0.000	        1	[resnet152v2/conv4_block33_1_relu/Relu;resnet152v2/conv4_block33_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block33_1_conv/Conv2D]:312
	                     PAD	         7883.936	    0.189	    0.102	  0.001%	 92.672%	     0.000	        1	[resnet152v2/conv4_block33_2_pad/Pad]:313
	                 CONV_2D	         7884.045	   21.853	   21.551	  0.253%	 92.925%	     0.000	        1	[resnet152v2/conv4_block33_2_relu/Relu;resnet152v2/conv4_block33_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block33_2_conv/Conv2D]:314
	                 CONV_2D	         7905.608	   72.123	   72.046	  0.847%	 93.773%	     0.000	        1	[resnet152v2/conv4_block33_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block33_3_conv/Conv2D]:315
	                     ADD	         7977.665	    1.879	    1.875	  0.022%	 93.795%	     0.000	        1	[resnet152v2/conv4_block33_out/add]:316
	                     MUL	         7979.549	    2.028	    2.023	  0.024%	 93.818%	     0.000	        1	[resnet152v2/conv4_block34_preact_bn/FusedBatchNormV3]:317
	                     ADD	         7981.582	    1.874	    1.874	  0.022%	 93.841%	     0.000	        1	[resnet152v2/conv4_block34_preact_relu/Relu;resnet152v2/conv4_block34_preact_bn/FusedBatchNormV3]:318
	                 CONV_2D	         7983.466	   19.536	   19.541	  0.230%	 94.070%	     0.000	        1	[resnet152v2/conv4_block34_1_relu/Relu;resnet152v2/conv4_block34_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block34_1_conv/Conv2D]:319
	                     PAD	         8003.018	    0.087	    0.101	  0.001%	 94.072%	     0.000	        1	[resnet152v2/conv4_block34_2_pad/Pad]:320
	                 CONV_2D	         8003.126	   21.717	   21.784	  0.256%	 94.328%	     0.000	        1	[resnet152v2/conv4_block34_2_relu/Relu;resnet152v2/conv4_block34_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block34_2_conv/Conv2D]:321
	                 CONV_2D	         8024.922	   71.067	   71.115	  0.836%	 95.164%	     0.000	        1	[resnet152v2/conv4_block34_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block34_3_conv/Conv2D]:322
	                     ADD	         8096.049	    1.848	    1.886	  0.022%	 95.186%	     0.000	        1	[resnet152v2/conv4_block34_out/add]:323
	                     MUL	         8097.945	    2.076	    2.027	  0.024%	 95.210%	     0.000	        1	[resnet152v2/conv4_block35_preact_bn/FusedBatchNormV3]:324
	                     ADD	         8099.982	    1.817	    1.856	  0.022%	 95.232%	     0.000	        1	[resnet152v2/conv4_block35_preact_relu/Relu;resnet152v2/conv4_block35_preact_bn/FusedBatchNormV3]:325
	                 CONV_2D	         8101.849	   19.495	   19.455	  0.229%	 95.461%	     0.000	        1	[resnet152v2/conv4_block35_1_relu/Relu;resnet152v2/conv4_block35_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block35_1_conv/Conv2D]:326
	                     PAD	         8121.315	    0.082	    0.096	  0.001%	 95.462%	     0.000	        1	[resnet152v2/conv4_block35_2_pad/Pad]:327
	                 CONV_2D	         8121.418	   21.516	   21.550	  0.253%	 95.715%	     0.000	        1	[resnet152v2/conv4_block35_2_relu/Relu;resnet152v2/conv4_block35_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block35_2_conv/Conv2D]:328
	                 CONV_2D	         8142.980	   72.797	   72.839	  0.857%	 96.572%	     0.000	        1	[resnet152v2/conv4_block35_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block35_3_conv/Conv2D]:329
	                     ADD	         8215.834	    1.889	    1.903	  0.022%	 96.594%	     0.000	        1	[resnet152v2/conv4_block35_out/add]:330
	                     MUL	         8217.747	    2.032	    2.012	  0.024%	 96.618%	     0.000	        1	[resnet152v2/conv4_block36_preact_bn/FusedBatchNormV3]:331
	                     ADD	         8219.768	    1.875	    1.866	  0.022%	 96.640%	     0.000	        1	[resnet152v2/conv4_block36_preact_relu/Relu;resnet152v2/conv4_block36_preact_bn/FusedBatchNormV3]:332
	                 CONV_2D	         8221.644	   19.597	   19.673	  0.231%	 96.871%	     0.000	        1	[resnet152v2/conv4_block36_1_relu/Relu;resnet152v2/conv4_block36_1_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block36_1_conv/Conv2D]:333
	                     PAD	         8241.329	    0.114	    0.121	  0.001%	 96.872%	     0.000	        1	[resnet152v2/conv4_block36_2_pad/Pad]:334
	                 CONV_2D	         8241.456	    5.699	    5.714	  0.067%	 96.940%	     0.000	        1	[resnet152v2/conv4_block36_2_relu/Relu;resnet152v2/conv4_block36_2_bn/FusedBatchNormV3;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block36_2_conv/Conv2D]:335
	                 CONV_2D	         8247.180	   18.372	   18.362	  0.216%	 97.156%	     0.000	        1	[resnet152v2/conv4_block36_3_conv/BiasAdd;resnet152v2/conv4_block10_3_conv/BiasAdd/ReadVariableOp;resnet152v2/conv4_block36_3_conv/Conv2D]:336
	             MAX_POOL_2D	         8265.551	    3.221	    3.249	  0.038%	 97.194%	     0.000	        1	[resnet152v2/max_pooling2d_10/MaxPool]:337
	                     ADD	         8268.808	    0.473	    0.488	  0.006%	 97.199%	     0.000	        1	[resnet152v2/conv4_block36_out/add]:338
	                     MUL	         8269.303	    0.510	    0.517	  0.006%	 97.206%	     0.000	        1	[resnet152v2/conv5_block1_preact_bn/FusedBatchNormV31]:339
	                     ADD	         8269.827	    0.482	    0.466	  0.005%	 97.211%	     0.000	        1	[resnet152v2/conv5_block1_preact_relu/Relu;resnet152v2/conv5_block1_preact_bn/FusedBatchNormV3]:340
	                 CONV_2D	         8270.299	   36.358	   36.383	  0.428%	 97.639%	     0.000	        1	[resnet152v2/conv5_block1_0_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_0_conv/Conv2D]:341
	                 CONV_2D	         8306.694	    9.366	    9.366	  0.110%	 97.749%	     0.000	        1	[resnet152v2/conv5_block1_1_relu/Relu;resnet152v2/conv5_block1_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_1_conv/Conv2D]:342
	                     PAD	         8316.070	    0.050	    0.061	  0.001%	 97.750%	     0.000	        1	[resnet152v2/conv5_block1_2_pad/Pad]:343
	                 CONV_2D	         8316.137	   11.541	   11.560	  0.136%	 97.886%	     0.000	        1	[resnet152v2/conv5_block1_2_relu/Relu;resnet152v2/conv5_block1_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_2_conv/Conv2D]:344
	                 CONV_2D	         8327.707	   36.600	   36.715	  0.432%	 98.317%	     0.000	        1	[resnet152v2/conv5_block1_3_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block1_3_conv/Conv2D]:345
	                     ADD	         8364.432	    0.938	    0.951	  0.011%	 98.329%	     0.000	        1	[resnet152v2/conv5_block1_out/add]:346
	                     MUL	         8365.392	    1.018	    1.039	  0.012%	 98.341%	     0.000	        1	[resnet152v2/conv5_block2_preact_bn/FusedBatchNormV3]:347
	                     ADD	         8366.438	    0.972	    0.944	  0.011%	 98.352%	     0.000	        1	[resnet152v2/conv5_block2_preact_relu/Relu;resnet152v2/conv5_block2_preact_bn/FusedBatchNormV3]:348
	                 CONV_2D	         8367.390	   10.001	   10.039	  0.118%	 98.470%	     0.000	        1	[resnet152v2/conv5_block2_1_relu/Relu;resnet152v2/conv5_block2_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_1_conv/Conv2D]:349
	                     PAD	         8377.440	    0.050	    0.060	  0.001%	 98.471%	     0.000	        1	[resnet152v2/conv5_block2_2_pad/Pad]:350
	                 CONV_2D	         8377.506	   11.645	   11.655	  0.137%	 98.608%	     0.000	        1	[resnet152v2/conv5_block2_2_relu/Relu;resnet152v2/conv5_block2_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_2_conv/Conv2D]:351
	                 CONV_2D	         8389.171	   36.412	   36.500	  0.429%	 99.037%	     0.000	        1	[resnet152v2/conv5_block2_3_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block2_3_conv/Conv2D]:352
	                     ADD	         8425.682	    0.932	    0.943	  0.011%	 99.048%	     0.000	        1	[resnet152v2/conv5_block2_out/add]:353
	                     MUL	         8426.633	    1.089	    1.041	  0.012%	 99.060%	     0.000	        1	[resnet152v2/conv5_block3_preact_bn/FusedBatchNormV3]:354
	                     ADD	         8427.682	    0.928	    0.944	  0.011%	 99.071%	     0.000	        1	[resnet152v2/conv5_block3_preact_relu/Relu;resnet152v2/conv5_block3_preact_bn/FusedBatchNormV3]:355
	                 CONV_2D	         8428.634	   10.049	   10.104	  0.119%	 99.190%	     0.000	        1	[resnet152v2/conv5_block3_1_relu/Relu;resnet152v2/conv5_block3_1_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_1_conv/Conv2D]:356
	                     PAD	         8438.749	    0.053	    0.066	  0.001%	 99.191%	     0.000	        1	[resnet152v2/conv5_block3_2_pad/Pad]:357
	                 CONV_2D	         8438.821	   11.651	   11.684	  0.137%	 99.328%	     0.000	        1	[resnet152v2/conv5_block3_2_relu/Relu;resnet152v2/conv5_block3_2_bn/FusedBatchNormV3;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_2_conv/Conv2D]:358
	                 CONV_2D	         8450.516	   36.569	   36.622	  0.431%	 99.759%	     0.000	        1	[resnet152v2/conv5_block3_3_conv/BiasAdd;resnet152v2/conv5_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv5_block3_3_conv/Conv2D]:359
	                     ADD	         8487.149	    0.961	    0.949	  0.011%	 99.770%	     0.000	        1	[resnet152v2/conv5_block3_out/add]:360
	                     MUL	         8488.107	    1.026	    1.034	  0.012%	 99.782%	     0.000	        1	[resnet152v2/post_bn/FusedBatchNormV31]:361
	                     ADD	         8489.149	    0.924	    0.951	  0.011%	 99.794%	     0.000	        1	[resnet152v2/post_relu/Relu;resnet152v2/post_bn/FusedBatchNormV3]:362
	                    MEAN	         8490.108	   16.409	   16.417	  0.193%	 99.987%	     0.000	        1	[resnet152v2/avg_pool/Mean]:363
	         FULLY_CONNECTED	         8506.535	    0.547	    0.550	  0.006%	 99.993%	     0.000	        1	[resnet152v2/predictions/MatMul;resnet152v2/predictions/BiasAdd]:364
	                 SOFTMAX	         8507.092	    0.604	    0.589	  0.007%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:365

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.514	  375.075	  374.825	  4.408%	  4.408%	     0.000	        1	[resnet152v2/conv1_conv/BiasAdd;resnet152v2/conv1_conv/BiasAdd/ReadVariableOp;resnet152v2/conv1_conv/Conv2D]:1
	                 CONV_2D	          974.678	  298.721	  298.743	  3.513%	  7.921%	     0.000	        1	[resnet152v2/conv2_block1_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_3_conv/Conv2D]:10
	                 CONV_2D	         1484.806	  296.708	  296.990	  3.492%	 11.413%	     0.000	        1	[resnet152v2/conv2_block2_3_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block2_3_conv/Conv2D]:17
	                 CONV_2D	          491.095	  298.469	  296.950	  3.492%	 14.905%	     0.000	        1	[resnet152v2/conv2_block1_0_conv/BiasAdd;resnet152v2/conv2_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv2_block1_0_conv/Conv2D]:6
	                 CONV_2D	         2738.324	  148.327	  148.023	  1.741%	 16.646%	     0.000	        1	[resnet152v2/conv3_block3_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block3_3_conv/Conv2D]:47
	                 CONV_2D	         2248.865	  147.559	  147.668	  1.737%	 18.382%	     0.000	        1	[resnet152v2/conv3_block1_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block1_3_conv/Conv2D]:33
	                 CONV_2D	         3715.587	  147.322	  147.536	  1.735%	 20.117%	     0.000	        1	[resnet152v2/conv3_block7_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block7_3_conv/Conv2D]:75
	                 CONV_2D	         2984.521	  147.673	  146.364	  1.721%	 21.839%	     0.000	        1	[resnet152v2/conv3_block4_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block4_3_conv/Conv2D]:54
	                 CONV_2D	         2493.972	  145.548	  145.823	  1.715%	 23.553%	     0.000	        1	[resnet152v2/conv3_block2_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block2_3_conv/Conv2D]:40
	                 CONV_2D	         3471.929	  145.372	  145.558	  1.712%	 25.265%	     0.000	        1	[resnet152v2/conv3_block6_3_conv/BiasAdd;resnet152v2/conv3_block1_0_conv/BiasAdd/ReadVariableOp;resnet152v2/conv3_block6_3_conv/Conv2D]:68

Number of nodes executed: 366
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	      155	  7989.167	    93.950%	    93.950%	     0.000	      155
	                     ADD	      101	   228.977	     2.693%	    96.643%	     0.000	      101
	             MAX_POOL_2D	        4	   133.961	     1.575%	    98.218%	     0.000	        4
	                     MUL	       51	   123.957	     1.458%	    99.676%	     0.000	       51
	                    MEAN	        1	    16.417	     0.193%	    99.869%	     0.000	        1
	                     PAD	       52	     9.987	     0.117%	    99.987%	     0.000	       52
	                 SOFTMAX	        1	     0.588	     0.007%	    99.994%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.550	     0.006%	   100.000%	     0.000	        1

Timings (microseconds): count=18 first=8506575 curr=8502245 min=8500641 max=8506575 avg=8.50377e+06 std=1636
Memory (bytes): count=0
366 nodes observed



