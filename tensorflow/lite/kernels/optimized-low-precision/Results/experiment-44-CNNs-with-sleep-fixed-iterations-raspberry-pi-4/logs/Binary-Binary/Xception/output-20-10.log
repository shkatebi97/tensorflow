STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/f32i8/Xception.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/f32i8/Xception.tflite
INFO: Initialized TensorFlow Lite runtime.
NOT Applying Conv Low-Precision for Kernel shape (32, 27, ), Input shape (89401, 3, ), and Output shape (22201, 32, ), and the ID is 0
	Changing Input Shape
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (64, 288, ), Input shape (22201, 32, ), and Output shape (21609, 64, ), and the ID is 1
	Allocating LowPrecision Weight Tensors with Shape of (64, 48)
	Allocating LowPrecision Activations Tensors with Shape of (21612, 48)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (21609, 64, ), and Output shape (21609, 128, ), and the ID is 2
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
	Allocating LowPrecision Activations Tensors with Shape of (21612, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (128, 128, ), Input shape (21609, 128, ), and Output shape (21609, 128, ), and the ID is 3
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
	Allocating LowPrecision Activations Tensors with Shape of (21612, 16)
Applying Conv Low-Precision for Kernel shape (128, 64, ), Input shape (21609, 64, ), and Output shape (5476, 128, )	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 16)
, and the ID is 4
	Allocating LowPrecision Activations Tensors with Shape of (5476, 16)
Applying Conv Low-Precision for Kernel shape (256, 128, ), Input shape (5476, 128, ), and Output shape (5476, 256, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 16)
	Allocating LowPrecision Activations Tensors with Shape of (5476, 16)
Applying Conv Low-Precision for Kernel shape (256, 256, ), Input shape (5476, 256, ), and Output shape (5476, 256, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 32)
6
	Allocating LowPrecision Activations Tensors with Shape of (5476, 32)
Applying Conv Low-Precision for Kernel shape (256, 128, ), Input shape (5476, 128, ), and Output shape (1369, 256, ), and the ID is 7
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 16)
	Allocating LowPrecision Activations Tensors with Shape of (1372, 16)
Applying Conv Low-Precision for Kernel shape (728, 256, ), Input shape (1369, 256, ), and Output shape (1369, 728, ), and the ID is 8
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 32)
	Allocating LowPrecision Activations Tensors with Shape of (1372, 32)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (1369, 728, ), and Output shape (1369, 728, ), and the ID is 9
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (1372, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 32)
Applying Conv Low-Precision for Kernel shape (728, 256, ), Input shape (1369, 256, ), and Output shape (361, 728, ), and the ID is 10
	Allocating LowPrecision Activations Tensors with Shape of (364, 32)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 11
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 12
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 13
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 15
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 16
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 17
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 18
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 19
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 20
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 21
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 22
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 23
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 24
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 25
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 26
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 27
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 28
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 29
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 30
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 31
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 32
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 33
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 34
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (728, 728, ), Input shape (361, 728, ), and Output shape (361, 728, ), and the ID is 35
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (728, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
Applying Conv Low-Precision for Kernel shape (1024, 728, ), Input shape (361, 728, ), and Output shape (361, 1024, ), and the ID is 36
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 96)
	Allocating LowPrecision Activations Tensors with Shape of (364, 96)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1024, 96)
Applying Conv Low-Precision for Kernel shape (1024, 728, ), Input shape (361, 728, ), and Output shape (100, 1024, ), and the ID is 37
	Allocating LowPrecision Activations Tensors with Shape of (100, 96)
Applying Conv Low-Precision for Kernel shape (1536, 1024, ), Input shape (100, 1024, ), and Output shape (100, 1536, ), and the ID is 38
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (1536, 128)
	Allocating LowPrecision Activations Tensors with Shape of (100, 128)
Applying Conv Low-Precision for Kernel shape (2048, 1536, ), Input shape (100, 1536, ), and Output shape (100, 2048, ), and the ID is 39
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (2048, 192)
	Allocating LowPrecision Activations Tensors with Shape of (100, 192)
Applying Low-Precision for shape (1000, 2048, ) and Input shape (1, 2048, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 256)
	Transformed Activation Shape From: (1, 2048) To: (1, 256)
The input model file size (MB): 23.4686
Initialized session in 98.253ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=8927864 curr=8812042 min=8790694 max=8927864 avg=8.81526e+06 std=38177

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=18 first=8790358 curr=8795544 min=8786271 max=8823860 avg=8.80459e+06 std=10286

Inference timings in us: Init: 98253, First inference: 8927864, Warmup (avg): 8.81526e+06, Inference (avg): 8.80459e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=26.5664 overall=154.344
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   83.107	   83.107	100.000%	100.000%	 22300.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   83.107	   83.107	100.000%	100.000%	 22300.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	    83.107	   100.000%	   100.000%	 22300.000	        1

Timings (microseconds): count=1 curr=83107
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.027	   14.376	   14.390	  0.163%	  0.163%	     0.000	        1	[xception/block1_conv1_act/Relu;xception/block1_conv1_bn/FusedBatchNormV3;xception/block1_conv1_bn/FusedBatchNormV3/ReadVariableOp;xception/block1_conv1/Conv2D]:0
	                 CONV_2D	           14.430	  646.862	  651.517	  7.401%	  7.564%	     0.000	        1	[xception/block1_conv2_act/Relu;xception/block1_conv2_bn/FusedBatchNormV3;xception/block1_conv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block1_conv2/Conv2D]:1
	       DEPTHWISE_CONV_2D	          665.959	  123.644	  123.347	  1.401%	  8.965%	     0.000	        1	[xception/block2_sepconv1/separable_conv2d/depthwise1]:2
	                 CONV_2D	          789.319	 1133.128	 1129.765	 12.833%	 21.799%	     0.000	        1	[xception/block2_sepconv2_act/Relu;xception/block2_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv1/separable_conv2d]:3
	       DEPTHWISE_CONV_2D	         1919.095	   23.331	   23.470	  0.267%	 22.066%	     0.000	        1	[xception/block2_sepconv2/separable_conv2d/depthwise1]:4
	                 CONV_2D	         1942.579	 1111.628	 1113.516	 12.649%	 34.714%	     0.000	        1	[xception/block2_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv2/separable_conv2d]:5
	             MAX_POOL_2D	         3056.108	  282.817	  280.886	  3.191%	 37.905%	     0.000	        1	[xception/block2_pool/MaxPool]:6
	                 CONV_2D	         3337.007	  286.078	  285.542	  3.244%	 41.149%	     0.000	        1	[xception/batch_normalization_297/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/conv2d_297/Conv2D]:7
	                     ADD	         3622.561	    6.567	    6.499	  0.074%	 41.223%	     0.000	        1	[xception/add_4/add]:8
	                    RELU	         3629.071	   22.281	   22.746	  0.258%	 41.481%	     0.000	        1	[xception/block3_sepconv1_act/Relu]:9
	       DEPTHWISE_CONV_2D	         3651.830	    6.150	    6.083	  0.069%	 41.550%	     0.000	        1	[xception/block3_sepconv1/separable_conv2d/depthwise1]:10
	                 CONV_2D	         3657.929	  527.010	  529.462	  6.014%	 47.564%	     0.000	        1	[xception/block3_sepconv2_act/Relu;xception/block3_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv1/separable_conv2d]:11
	       DEPTHWISE_CONV_2D	         4187.403	   11.802	   11.891	  0.135%	 47.699%	     0.000	        1	[xception/block3_sepconv2/separable_conv2d/depthwise1]:12
	                 CONV_2D	         4199.307	  521.436	  523.010	  5.941%	 53.641%	     0.000	        1	[xception/block3_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv2/separable_conv2d]:13
	             MAX_POOL_2D	         4722.330	  116.085	  117.751	  1.338%	 54.978%	     0.000	        1	[xception/block3_pool/MaxPool]:14
	                 CONV_2D	         4840.094	  132.162	  132.293	  1.503%	 56.481%	     0.000	        1	[xception/batch_normalization_298/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/conv2d_298/Conv2D]:15
	                     ADD	         4972.400	    3.248	    3.228	  0.037%	 56.518%	     0.000	        1	[xception/add_5/add]:16
	                    RELU	         4975.639	   11.184	   11.599	  0.132%	 56.649%	     0.000	        1	[xception/block4_sepconv1_act/Relu]:17
	       DEPTHWISE_CONV_2D	         4987.249	    3.007	    3.037	  0.035%	 56.684%	     0.000	        1	[xception/block4_sepconv1/separable_conv2d/depthwise1]:18
	                 CONV_2D	         4990.299	  358.539	  361.225	  4.103%	 60.787%	     0.000	        1	[xception/block4_sepconv2_act/Relu;xception/block4_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv1/separable_conv2d]:19
	       DEPTHWISE_CONV_2D	         5351.537	    8.279	    8.412	  0.096%	 60.883%	     0.000	        1	[xception/block4_sepconv2/separable_conv2d/depthwise1]:20
	                 CONV_2D	         5359.962	  353.939	  353.025	  4.010%	 64.893%	     0.000	        1	[xception/block4_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv2/separable_conv2d]:21
	             MAX_POOL_2D	         5712.999	   72.367	   72.953	  0.829%	 65.722%	     0.000	        1	[xception/block4_pool/MaxPool]:22
	                 CONV_2D	         5785.966	   94.436	   94.305	  1.071%	 66.793%	     0.000	        1	[xception/batch_normalization_299/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/conv2d_299/Conv2D]:23
	                     ADD	         5880.283	    2.461	    2.448	  0.028%	 66.821%	     0.000	        1	[xception/add_6/add]:24
	                    RELU	         5882.742	    8.453	    8.639	  0.098%	 66.919%	     0.000	        1	[xception/block5_sepconv1_act/Relu]:25
	       DEPTHWISE_CONV_2D	         5891.395	    2.210	    2.217	  0.025%	 66.944%	     0.000	        1	[xception/block5_sepconv1/separable_conv2d/depthwise1]:26
	                 CONV_2D	         5893.623	   95.359	   95.004	  1.079%	 68.023%	     0.000	        1	[xception/block5_sepconv2_act/Relu;xception/block5_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block5_sepconv1/separable_conv2d]:27
	       DEPTHWISE_CONV_2D	         5988.640	    2.263	    2.236	  0.025%	 68.049%	     0.000	        1	[xception/block5_sepconv2/separable_conv2d/depthwise1]:28
	                 CONV_2D	         5990.888	   94.850	   94.779	  1.077%	 69.125%	     0.000	        1	[xception/block5_sepconv3_act/Relu;xception/block5_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block5_sepconv2/separable_conv2d]:29
	       DEPTHWISE_CONV_2D	         6085.679	    2.209	    2.218	  0.025%	 69.150%	     0.000	        1	[xception/block5_sepconv3/separable_conv2d/depthwise1]:30
	                 CONV_2D	         6087.909	   93.079	   93.273	  1.060%	 70.210%	     0.000	        1	[xception/block5_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block5_sepconv3/separable_conv2d]:31
	                     ADD	         6181.194	    2.412	    2.429	  0.028%	 70.237%	     0.000	        1	[xception/add_7/add]:32
	                    RELU	         6183.633	    8.698	    8.646	  0.098%	 70.336%	     0.000	        1	[xception/block6_sepconv1_act/Relu]:33
	       DEPTHWISE_CONV_2D	         6192.290	    2.212	    2.221	  0.025%	 70.361%	     0.000	        1	[xception/block6_sepconv1/separable_conv2d/depthwise1]:34
	                 CONV_2D	         6194.523	   94.270	   94.315	  1.071%	 71.432%	     0.000	        1	[xception/block6_sepconv2_act/Relu;xception/block6_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv1/separable_conv2d]:35
	       DEPTHWISE_CONV_2D	         6288.850	    2.232	    2.235	  0.025%	 71.458%	     0.000	        1	[xception/block6_sepconv2/separable_conv2d/depthwise1]:36
	                 CONV_2D	         6291.097	   95.174	   96.327	  1.094%	 72.552%	     0.000	        1	[xception/block6_sepconv3_act/Relu;xception/block6_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv2/separable_conv2d]:37
	       DEPTHWISE_CONV_2D	         6387.438	    2.175	    2.223	  0.025%	 72.577%	     0.000	        1	[xception/block6_sepconv3/separable_conv2d/depthwise1]:38
	                 CONV_2D	         6389.673	   96.383	   97.252	  1.105%	 73.682%	     0.000	        1	[xception/block6_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block6_sepconv3/separable_conv2d]:39
	                     ADD	         6486.937	    2.429	    2.434	  0.028%	 73.709%	     0.000	        1	[xception/add_8/add]:40
	                    RELU	         6489.381	    7.685	    8.473	  0.096%	 73.806%	     0.000	        1	[xception/block7_sepconv1_act/Relu]:41
	       DEPTHWISE_CONV_2D	         6497.866	    2.189	    2.272	  0.026%	 73.832%	     0.000	        1	[xception/block7_sepconv1/separable_conv2d/depthwise1]:42
	                 CONV_2D	         6500.150	   95.421	   96.472	  1.096%	 74.927%	     0.000	        1	[xception/block7_sepconv2_act/Relu;xception/block7_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block7_sepconv1/separable_conv2d]:43
	       DEPTHWISE_CONV_2D	         6596.634	    2.230	    2.246	  0.026%	 74.953%	     0.000	        1	[xception/block7_sepconv2/separable_conv2d/depthwise1]:44
	                 CONV_2D	         6598.892	   98.396	   98.599	  1.120%	 76.073%	     0.000	        1	[xception/block7_sepconv3_act/Relu;xception/block7_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block7_sepconv2/separable_conv2d]:45
	       DEPTHWISE_CONV_2D	         6697.504	    2.275	    2.256	  0.026%	 76.099%	     0.000	        1	[xception/block7_sepconv3/separable_conv2d/depthwise1]:46
	                 CONV_2D	         6699.772	   96.375	   97.177	  1.104%	 77.202%	     0.000	        1	[xception/block7_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block7_sepconv3/separable_conv2d]:47
	                     ADD	         6796.961	    2.473	    2.423	  0.028%	 77.230%	     0.000	        1	[xception/add_9/add]:48
	                    RELU	         6799.394	    7.885	    8.516	  0.097%	 77.327%	     0.000	        1	[xception/block8_sepconv1_act/Relu]:49
	       DEPTHWISE_CONV_2D	         6807.923	    2.200	    2.233	  0.025%	 77.352%	     0.000	        1	[xception/block8_sepconv1/separable_conv2d/depthwise1]:50
	                 CONV_2D	         6810.168	   97.167	   97.734	  1.110%	 78.462%	     0.000	        1	[xception/block8_sepconv2_act/Relu;xception/block8_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block8_sepconv1/separable_conv2d]:51
	       DEPTHWISE_CONV_2D	         6907.915	    2.161	    2.246	  0.026%	 78.488%	     0.000	        1	[xception/block8_sepconv2/separable_conv2d/depthwise1]:52
	                 CONV_2D	         6910.173	   95.909	   96.060	  1.091%	 79.579%	     0.000	        1	[xception/block8_sepconv3_act/Relu;xception/block8_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block8_sepconv2/separable_conv2d]:53
	       DEPTHWISE_CONV_2D	         7006.245	    2.232	    2.223	  0.025%	 79.604%	     0.000	        1	[xception/block8_sepconv3/separable_conv2d/depthwise1]:54
	                 CONV_2D	         7008.479	   95.571	   95.700	  1.087%	 80.691%	     0.000	        1	[xception/block8_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block8_sepconv3/separable_conv2d]:55
	                     ADD	         7104.191	    2.401	    2.441	  0.028%	 80.719%	     0.000	        1	[xception/add_10/add]:56
	                    RELU	         7106.642	    9.142	    8.674	  0.099%	 80.818%	     0.000	        1	[xception/block9_sepconv1_act/Relu]:57
	       DEPTHWISE_CONV_2D	         7115.328	    2.255	    2.249	  0.026%	 80.843%	     0.000	        1	[xception/block9_sepconv1/separable_conv2d/depthwise1]:58
	                 CONV_2D	         7117.589	   98.380	   98.485	  1.119%	 81.962%	     0.000	        1	[xception/block9_sepconv2_act/Relu;xception/block9_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block9_sepconv1/separable_conv2d]:59
	       DEPTHWISE_CONV_2D	         7216.086	    2.174	    2.199	  0.025%	 81.987%	     0.000	        1	[xception/block9_sepconv2/separable_conv2d/depthwise1]:60
	                 CONV_2D	         7218.297	   94.798	   94.998	  1.079%	 83.066%	     0.000	        1	[xception/block9_sepconv3_act/Relu;xception/block9_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block9_sepconv2/separable_conv2d]:61
	       DEPTHWISE_CONV_2D	         7313.307	    2.158	    2.301	  0.026%	 83.092%	     0.000	        1	[xception/block9_sepconv3/separable_conv2d/depthwise1]:62
	                 CONV_2D	         7315.620	   96.127	   96.235	  1.093%	 84.185%	     0.000	        1	[xception/block9_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block9_sepconv3/separable_conv2d]:63
	                     ADD	         7411.868	    2.416	    2.437	  0.028%	 84.213%	     0.000	        1	[xception/add_11/add]:64
	                    RELU	         7414.315	    7.963	    8.390	  0.095%	 84.308%	     0.000	        1	[xception/block10_sepconv1_act/Relu]:65
	       DEPTHWISE_CONV_2D	         7422.716	    2.202	    2.217	  0.025%	 84.333%	     0.000	        1	[xception/block10_sepconv1/separable_conv2d/depthwise1]:66
	                 CONV_2D	         7424.947	   95.023	   95.131	  1.081%	 85.414%	     0.000	        1	[xception/block10_sepconv2_act/Relu;xception/block10_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block10_sepconv1/separable_conv2d]:67
	       DEPTHWISE_CONV_2D	         7520.090	    2.197	    2.247	  0.026%	 85.440%	     0.000	        1	[xception/block10_sepconv2/separable_conv2d/depthwise1]:68
	                 CONV_2D	         7522.352	   93.896	   93.961	  1.067%	 86.507%	     0.000	        1	[xception/block10_sepconv3_act/Relu;xception/block10_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block10_sepconv2/separable_conv2d]:69
	       DEPTHWISE_CONV_2D	         7616.326	    2.273	    2.261	  0.026%	 86.533%	     0.000	        1	[xception/block10_sepconv3/separable_conv2d/depthwise1]:70
	                 CONV_2D	         7618.598	   93.391	   93.162	  1.058%	 87.591%	     0.000	        1	[xception/block10_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block10_sepconv3/separable_conv2d]:71
	                     ADD	         7711.773	    2.427	    2.432	  0.028%	 87.618%	     0.000	        1	[xception/add_12/add]:72
	                    RELU	         7714.215	    7.668	    8.617	  0.098%	 87.716%	     0.000	        1	[xception/block11_sepconv1_act/Relu]:73
	       DEPTHWISE_CONV_2D	         7722.844	    2.211	    2.243	  0.025%	 87.742%	     0.000	        1	[xception/block11_sepconv1/separable_conv2d/depthwise1]:74
	                 CONV_2D	         7725.099	   96.225	   96.112	  1.092%	 88.834%	     0.000	        1	[xception/block11_sepconv2_act/Relu;xception/block11_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block11_sepconv1/separable_conv2d]:75
	       DEPTHWISE_CONV_2D	         7821.222	    2.275	    2.237	  0.025%	 88.859%	     0.000	        1	[xception/block11_sepconv2/separable_conv2d/depthwise1]:76
	                 CONV_2D	         7823.471	   95.827	   95.369	  1.083%	 89.942%	     0.000	        1	[xception/block11_sepconv3_act/Relu;xception/block11_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block11_sepconv2/separable_conv2d]:77
	       DEPTHWISE_CONV_2D	         7918.853	    2.179	    2.244	  0.025%	 89.968%	     0.000	        1	[xception/block11_sepconv3/separable_conv2d/depthwise1]:78
	                 CONV_2D	         7921.108	   95.724	   95.176	  1.081%	 91.049%	     0.000	        1	[xception/block11_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block11_sepconv3/separable_conv2d]:79
	                     ADD	         8016.300	    2.460	    2.432	  0.028%	 91.077%	     0.000	        1	[xception/add_13/add]:80
	                    RELU	         8018.741	    8.477	    8.486	  0.096%	 91.173%	     0.000	        1	[xception/block12_sepconv1_act/Relu]:81
	       DEPTHWISE_CONV_2D	         8027.238	    2.366	    2.239	  0.025%	 91.198%	     0.000	        1	[xception/block12_sepconv1/separable_conv2d/depthwise1]:82
	                 CONV_2D	         8029.490	   97.351	   96.805	  1.100%	 92.298%	     0.000	        1	[xception/block12_sepconv2_act/Relu;xception/block12_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block12_sepconv1/separable_conv2d]:83
	       DEPTHWISE_CONV_2D	         8126.307	    2.305	    2.237	  0.025%	 92.324%	     0.000	        1	[xception/block12_sepconv2/separable_conv2d/depthwise1]:84
	                 CONV_2D	         8128.557	   95.784	   95.390	  1.084%	 93.407%	     0.000	        1	[xception/block12_sepconv3_act/Relu;xception/block12_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block12_sepconv2/separable_conv2d]:85
	       DEPTHWISE_CONV_2D	         8223.959	    2.405	    2.250	  0.026%	 93.433%	     0.000	        1	[xception/block12_sepconv3/separable_conv2d/depthwise1]:86
	                 CONV_2D	         8226.221	   97.954	   97.084	  1.103%	 94.535%	     0.000	        1	[xception/block12_sepconv3_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block12_sepconv3/separable_conv2d]:87
	                     ADD	         8323.317	    2.429	    2.428	  0.028%	 94.563%	     0.000	        1	[xception/add_14/add]:88
	                    RELU	         8325.755	    9.427	    8.499	  0.097%	 94.660%	     0.000	        1	[xception/block13_sepconv1_act/Relu]:89
	       DEPTHWISE_CONV_2D	         8334.266	    2.191	    2.251	  0.026%	 94.685%	     0.000	        1	[xception/block13_sepconv1/separable_conv2d/depthwise1]:90
	                 CONV_2D	         8336.528	   95.865	   96.045	  1.091%	 95.776%	     0.000	        1	[xception/block13_sepconv2_act/Relu;xception/block13_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block13_sepconv1/separable_conv2d]:91
	       DEPTHWISE_CONV_2D	         8432.586	    2.245	    2.218	  0.025%	 95.801%	     0.000	        1	[xception/block13_sepconv2/separable_conv2d/depthwise1]:92
	                 CONV_2D	         8434.816	  133.991	  134.239	  1.525%	 97.326%	     0.000	        1	[xception/block13_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_300/FusedBatchNormV3/ReadVariableOp;xception/block13_sepconv2/separable_conv2d]:93
	             MAX_POOL_2D	         8569.067	   26.412	   26.501	  0.301%	 97.627%	     0.000	        1	[xception/block13_pool/MaxPool]:94
	                 CONV_2D	         8595.580	   37.868	   37.907	  0.431%	 98.058%	     0.000	        1	[xception/batch_normalization_300/FusedBatchNormV3;xception/batch_normalization_300/FusedBatchNormV3/ReadVariableOp;xception/conv2d_300/Conv2D]:95
	                     ADD	         8633.499	    0.980	    0.964	  0.011%	 98.069%	     0.000	        1	[xception/add_15/add]:96
	       DEPTHWISE_CONV_2D	         8634.472	    0.810	    0.817	  0.009%	 98.078%	     0.000	        1	[xception/block14_sepconv1/separable_conv2d/depthwise1]:97
	                 CONV_2D	         8635.299	   55.343	   55.374	  0.629%	 98.707%	     0.000	        1	[xception/block14_sepconv1_act/Relu;xception/block14_sepconv1_bn/FusedBatchNormV3;xception/block14_sepconv1_bn/FusedBatchNormV3/ReadVariableOp;xception/block14_sepconv1/separable_conv2d]:98
	       DEPTHWISE_CONV_2D	         8690.685	    1.320	    1.354	  0.015%	 98.723%	     0.000	        1	[xception/block14_sepconv2/separable_conv2d/depthwise1]:99
	                 CONV_2D	         8692.051	   77.839	   77.939	  0.885%	 99.608%	     0.000	        1	[xception/block14_sepconv2_act/Relu;xception/block14_sepconv2_bn/FusedBatchNormV3;xception/block14_sepconv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block14_sepconv2/separable_conv2d]:100
	                    MEAN	         8770.002	   33.380	   33.384	  0.379%	 99.987%	     0.000	        1	[xception/avg_pool/Mean]:101
	         FULLY_CONNECTED	         8803.396	    0.542	    0.551	  0.006%	 99.993%	     0.000	        1	[xception/predictions/MatMul;xception/predictions/BiasAdd]:102
	                 SOFTMAX	         8803.954	    0.582	    0.584	  0.007%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:103

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          789.319	 1133.128	 1129.765	 12.833%	 12.833%	     0.000	        1	[xception/block2_sepconv2_act/Relu;xception/block2_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv1/separable_conv2d]:3
	                 CONV_2D	         1942.579	 1111.628	 1113.516	 12.649%	 25.482%	     0.000	        1	[xception/block2_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/block2_sepconv2/separable_conv2d]:5
	                 CONV_2D	           14.430	  646.862	  651.517	  7.401%	 32.883%	     0.000	        1	[xception/block1_conv2_act/Relu;xception/block1_conv2_bn/FusedBatchNormV3;xception/block1_conv2_bn/FusedBatchNormV3/ReadVariableOp;xception/block1_conv2/Conv2D]:1
	                 CONV_2D	         3657.929	  527.010	  529.462	  6.014%	 38.898%	     0.000	        1	[xception/block3_sepconv2_act/Relu;xception/block3_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv1/separable_conv2d]:11
	                 CONV_2D	         4199.307	  521.436	  523.010	  5.941%	 44.839%	     0.000	        1	[xception/block3_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_298/FusedBatchNormV3/ReadVariableOp;xception/block3_sepconv2/separable_conv2d]:13
	                 CONV_2D	         4990.299	  358.539	  361.225	  4.103%	 48.942%	     0.000	        1	[xception/block4_sepconv2_act/Relu;xception/block4_sepconv1_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv1/separable_conv2d]:19
	                 CONV_2D	         5359.962	  353.939	  353.025	  4.010%	 52.952%	     0.000	        1	[xception/block4_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_299/FusedBatchNormV3/ReadVariableOp;xception/block4_sepconv2/separable_conv2d]:21
	                 CONV_2D	         3337.007	  286.078	  285.542	  3.244%	 56.196%	     0.000	        1	[xception/batch_normalization_297/FusedBatchNormV3;xception/batch_normalization_297/FusedBatchNormV3/ReadVariableOp;xception/conv2d_297/Conv2D]:7
	             MAX_POOL_2D	         3056.108	  282.817	  280.886	  3.191%	 59.386%	     0.000	        1	[xception/block2_pool/MaxPool]:6
	                 CONV_2D	         8434.816	  133.991	  134.239	  1.525%	 60.911%	     0.000	        1	[xception/block13_sepconv2_bn/FusedBatchNormV3;xception/batch_normalization_300/FusedBatchNormV3/ReadVariableOp;xception/block13_sepconv2/separable_conv2d]:93

Number of nodes executed: 104
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       40	  7890.128	    89.628%	    89.628%	     0.000	       40
	             MAX_POOL_2D	        4	   498.090	     5.658%	    95.286%	     0.000	        4
	       DEPTHWISE_CONV_2D	       34	   236.614	     2.688%	    97.974%	     0.000	       34
	                    RELU	       11	   111.279	     1.264%	    99.238%	     0.000	       11
	                    MEAN	        1	    33.383	     0.379%	    99.617%	     0.000	        1
	                     ADD	       12	    32.587	     0.370%	    99.987%	     0.000	       12
	                 SOFTMAX	        1	     0.584	     0.007%	    99.994%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.551	     0.006%	   100.000%	     0.000	        1

Timings (microseconds): count=18 first=8789052 curr=8794230 min=8784965 max=8822550 avg=8.80327e+06 std=10281
Memory (bytes): count=0
104 nodes observed



