STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [10]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/f32i8/VGG19.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/f32i8/VGG19.tflite
INFO: Initialized TensorFlow Lite runtime.
Applying Conv Low-Precision for Kernel shape (64, 27, ), Input shape (50176, 3, ), and Output shape (50176, 64, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 16)
0
	Allocating LowPrecision Activations Tensors with Shape of (50176, 16)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
Applying Conv Low-Precision for Kernel shape (64, 576, ), Input shape (50176, 64, ), and Output shape (50176, 64, ), and the ID is 1
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (64, 144)
	Allocating LowPrecision Activations Tensors with Shape of (50176, 144)
Applying Conv Low-Precision for Kernel shape (128, 576, ), Input shape (12544, 64, ), and Output shape 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 144)
(12544, 128, ), and the ID is 2
	Allocating LowPrecision Activations Tensors with Shape of (12544, 144)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (128, 288)
Applying Conv Low-Precision for Kernel shape (128, 1152, ), Input shape (12544, 128, ), and Output shape (12544, 128, ), and the ID is 3
	Allocating LowPrecision Activations Tensors with Shape of (12544, 288)
Applying Conv Low-Precision for Kernel shape (256, 1152, ), Input shape (3136, 128, ), and Output shape (3136, 256, ), and the ID is 4
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 288)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 288)
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 5
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (256, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 6
	Allocating LowPrecision Weight Tensors with Shape of (256, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (256, 2304, ), Input shape (3136, 256, ), and Output shape (3136, 256, ), and the ID is 7
	Allocating LowPrecision Weight Tensors with Shape of (256, 576)
	Allocating LowPrecision Activations Tensors with Shape of (3136, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 2304, ), Input shape (784, 256, ), and Output shape (784, 512, ), and the ID is 8
	Allocating LowPrecision Weight Tensors with Shape of (512, 576)
	Allocating LowPrecision Activations Tensors with Shape of (784, 576)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 9
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
10
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (784, 512, ), and Output shape (784, 512, ), and the ID is 11
	Allocating LowPrecision Activations Tensors with Shape of (784, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 12
	Allocating LowPrecision Activations Tensors with Shape of (196, 1152)
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 13
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (196, 1152)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 14
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (196, 1152)
Applying Conv Low-Precision for Kernel shape (512, 4608, ), Input shape (196, 512, ), and Output shape (196, 512, ), and the ID is 15
	Changing Input Shape
	Reserving LowPrecision Weight Tensors
	Reserving LowPrecision Activation Tensors
	Allocating LowPrecision Weight Tensors with Shape of (512, 1152)
	Allocating LowPrecision Activations Tensors with Shape of (196, 1152)
Applying Low-Precision for shape (4096, 25088, ) and Input shape (1, 25088, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (4096, 6272)
	Transformed Activation Shape From: (1, 25088) To: (1, 6272)
Applying Low-Precision for shape (4096, 4096, ) and Input shape (1, 4096, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (4096, 1024)
	Transformed Activation Shape From: (1, 4096) To: (1, 1024)
Applying Low-Precision for shape (1000, 4096, ) and Input shape (1, 4096, ) With 7 Number of Temporaries Tensors
	Transformed Filter Shape: (1000, 1024)
	Transformed Activation Shape From: (1, 4096) To: (1, 1024)
The input model file size (MB): 143.732
Initialized session in 516.388ms.
Running benchmark for at least 10 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=10 first=7303303 curr=7084849 min=7081490 max=7303303 avg=7.10553e+06 std=65937

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=7081716 curr=7086746 min=7074068 max=7098383 avg=7.08341e+06 std=4815

Inference timings in us: Init: 516388, First inference: 7303303, Warmup (avg): 7.10553e+06, Inference (avg): 7.08341e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=146.465 overall=224.016
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  512.746	  512.746	100.000%	100.000%	147456.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	  512.746	  512.746	100.000%	100.000%	147456.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	   512.746	   100.000%	   100.000%	147456.000	        1

Timings (microseconds): count=1 curr=512746
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.022	 1409.987	 1411.762	 19.932%	 19.932%	     0.000	        1	[vgg19/block1_conv1/Relu;vgg19/block1_conv1/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv1/Conv2D]:0
	                 CONV_2D	         1411.796	 1514.154	 1514.862	 21.387%	 41.319%	     0.000	        1	[vgg19/block1_conv2/Relu;vgg19/block1_conv2/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv2/Conv2D]:1
	             MAX_POOL_2D	         2926.670	  210.095	  211.092	  2.980%	 44.299%	     0.000	        1	[vgg19/block1_pool/MaxPool]:2
	                 CONV_2D	         3137.774	  667.411	  667.709	  9.427%	 53.726%	     0.000	        1	[vgg19/block2_conv1/Relu;vgg19/block2_conv1/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv1/Conv2D]:3
	                 CONV_2D	         3805.495	  707.519	  707.804	  9.993%	 63.719%	     0.000	        1	[vgg19/block2_conv2/Relu;vgg19/block2_conv2/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv2/Conv2D]:4
	             MAX_POOL_2D	         4513.311	   81.090	   80.885	  1.142%	 64.861%	     0.000	        1	[vgg19/block2_pool/MaxPool]:5
	                 CONV_2D	         4594.208	  327.210	  325.710	  4.598%	 69.459%	     0.000	        1	[vgg19/block3_conv1/Relu;vgg19/block3_conv1/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv1/Conv2D]:6
	                 CONV_2D	         4919.930	  360.056	  359.823	  5.080%	 74.539%	     0.000	        1	[vgg19/block3_conv2/Relu;vgg19/block3_conv2/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv2/Conv2D]:7
	                 CONV_2D	         5279.765	  360.059	  359.952	  5.082%	 79.621%	     0.000	        1	[vgg19/block3_conv3/Relu;vgg19/block3_conv3/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv3/Conv2D]:8
	                 CONV_2D	         5639.729	  360.672	  360.745	  5.093%	 84.714%	     0.000	        1	[vgg19/block3_conv4/Relu;vgg19/block3_conv4/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv4/Conv2D]:9
	             MAX_POOL_2D	         6000.485	   34.266	   34.264	  0.484%	 85.198%	     0.000	        1	[vgg19/block3_pool/MaxPool]:10
	                 CONV_2D	         6034.761	  171.804	  172.048	  2.429%	 87.627%	     0.000	        1	[vgg19/block4_conv1/Relu;vgg19/block4_conv1/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv1/Conv2D]:11
	                 CONV_2D	         6206.821	  207.662	  207.662	  2.932%	 90.559%	     0.000	        1	[vgg19/block4_conv2/Relu;vgg19/block4_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv2/Conv2D]:12
	                 CONV_2D	         6414.494	  209.470	  209.475	  2.957%	 93.516%	     0.000	        1	[vgg19/block4_conv3/Relu;vgg19/block4_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv3/Conv2D]:13
	                 CONV_2D	         6623.982	  209.294	  209.306	  2.955%	 96.471%	     0.000	        1	[vgg19/block4_conv4/Relu;vgg19/block4_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv4/Conv2D]:14
	             MAX_POOL_2D	         6833.300	   15.485	   15.638	  0.221%	 96.692%	     0.000	        1	[vgg19/block4_pool/MaxPool]:15
	                 CONV_2D	         6848.949	   51.516	   51.474	  0.727%	 97.419%	     0.000	        1	[vgg19/block5_conv1/Relu;vgg19/block5_conv1/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv1/Conv2D]:16
	                 CONV_2D	         6900.435	   51.831	   51.668	  0.729%	 98.148%	     0.000	        1	[vgg19/block5_conv2/Relu;vgg19/block5_conv2/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv2/Conv2D]:17
	                 CONV_2D	         6952.115	   51.945	   51.937	  0.733%	 98.881%	     0.000	        1	[vgg19/block5_conv3/Relu;vgg19/block5_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv3/Conv2D]:18
	                 CONV_2D	         7004.064	   51.939	   51.545	  0.728%	 99.609%	     0.000	        1	[vgg19/block5_conv4/Relu;vgg19/block5_conv4/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block5_conv4/Conv2D]:19
	             MAX_POOL_2D	         7055.620	    4.002	    4.012	  0.057%	 99.666%	     0.000	        1	[vgg19/block5_pool/MaxPool]:20
	                 RESHAPE	         7059.640	    0.046	    0.040	  0.001%	 99.666%	     0.000	        1	[vgg19/flatten/Reshape]:21
	         FULLY_CONNECTED	         7059.686	   19.003	   18.945	  0.267%	 99.934%	     0.000	        1	[vgg19/fc1/MatMul;vgg19/fc1/Relu;vgg19/fc1/BiasAdd]:22
	         FULLY_CONNECTED	         7078.641	    3.329	    3.277	  0.046%	 99.980%	     0.000	        1	[vgg19/fc2/MatMul;vgg19/fc2/Relu;vgg19/fc2/BiasAdd]:23
	         FULLY_CONNECTED	         7081.926	    0.903	    0.834	  0.012%	 99.992%	     0.000	        1	[vgg19/predictions/MatMul;vgg19/predictions/BiasAdd]:24
	                 SOFTMAX	         7082.769	    0.587	    0.588	  0.008%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:25

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	         1411.796	 1514.154	 1514.862	 21.387%	 21.387%	     0.000	        1	[vgg19/block1_conv2/Relu;vgg19/block1_conv2/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv2/Conv2D]:1
	                 CONV_2D	            0.022	 1409.987	 1411.762	 19.932%	 41.319%	     0.000	        1	[vgg19/block1_conv1/Relu;vgg19/block1_conv1/BiasAdd;vgg19/block1_conv1/BiasAdd/ReadVariableOp;vgg19/block1_conv1/Conv2D]:0
	                 CONV_2D	         3805.495	  707.519	  707.804	  9.993%	 51.312%	     0.000	        1	[vgg19/block2_conv2/Relu;vgg19/block2_conv2/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv2/Conv2D]:4
	                 CONV_2D	         3137.774	  667.411	  667.709	  9.427%	 60.738%	     0.000	        1	[vgg19/block2_conv1/Relu;vgg19/block2_conv1/BiasAdd;vgg19/block2_conv1/BiasAdd/ReadVariableOp;vgg19/block2_conv1/Conv2D]:3
	                 CONV_2D	         5639.729	  360.672	  360.745	  5.093%	 65.831%	     0.000	        1	[vgg19/block3_conv4/Relu;vgg19/block3_conv4/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv4/Conv2D]:9
	                 CONV_2D	         5279.765	  360.059	  359.952	  5.082%	 70.913%	     0.000	        1	[vgg19/block3_conv3/Relu;vgg19/block3_conv3/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv3/Conv2D]:8
	                 CONV_2D	         4919.930	  360.056	  359.823	  5.080%	 75.993%	     0.000	        1	[vgg19/block3_conv2/Relu;vgg19/block3_conv2/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv2/Conv2D]:7
	                 CONV_2D	         4594.208	  327.210	  325.710	  4.598%	 80.592%	     0.000	        1	[vgg19/block3_conv1/Relu;vgg19/block3_conv1/BiasAdd;vgg19/block3_conv1/BiasAdd/ReadVariableOp;vgg19/block3_conv1/Conv2D]:6
	             MAX_POOL_2D	         2926.670	  210.095	  211.092	  2.980%	 83.572%	     0.000	        1	[vgg19/block1_pool/MaxPool]:2
	                 CONV_2D	         6414.494	  209.470	  209.475	  2.957%	 86.529%	     0.000	        1	[vgg19/block4_conv3/Relu;vgg19/block4_conv3/BiasAdd;vgg19/block4_conv1/BiasAdd/ReadVariableOp;vgg19/block4_conv3/Conv2D]:13

Number of nodes executed: 26
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       16	  6713.476	    94.782%	    94.782%	     0.000	       16
	             MAX_POOL_2D	        5	   345.887	     4.883%	    99.666%	     0.000	        5
	         FULLY_CONNECTED	        3	    23.055	     0.325%	    99.991%	     0.000	        3
	                 SOFTMAX	        1	     0.588	     0.008%	    99.999%	     0.000	        1
	                 RESHAPE	        1	     0.040	     0.001%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=7081335 curr=7086400 min=7073723 max=7098033 avg=7.08306e+06 std=4817
Memory (bytes): count=0
26 nodes observed



