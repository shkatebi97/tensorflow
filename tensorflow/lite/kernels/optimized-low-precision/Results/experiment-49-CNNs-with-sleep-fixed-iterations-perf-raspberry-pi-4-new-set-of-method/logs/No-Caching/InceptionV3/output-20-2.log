STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [2]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/InceptionV3.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/InceptionV3.tflite
INFO: Initialized TensorFlow Lite runtime.
NOT Applying Conv Low-Precision for Kernel shape (27, 32, ), Input shape (89401, 3, ), and Output shape (22201, 32, ), and the ID is 0
	Changing Input Shape
	New Input Shape: (22201, 27, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 32, ), Input shape (22201, 32, ), and Output shape (21609, 32, ), and the ID is 1
	Changing Input Shape
	New Input Shape: (21609, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (21609, 32, ), and Output shape (21609, 64, ), and the ID is 2
	Changing Input Shape
	New Input Shape: (21609, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 80, ), Input shape (5329, 64, ), and Output shape (5329, 80, ), and the ID is 3
	Changing Input Shape
	New Input Shape: (5329, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (720, 192, ), Input shape (5329, 80, ), and Output shape (5041, 192, ), and the ID is 4
	Changing Input Shape
	New Input Shape: (5041, 720, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 32, ), Input shape (1225, 192, ), and Output shape (1225, 32, ), and the ID is 5
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 64, ), Input shape (1225, 192, ), and Output shape (1225, 64, ), and the ID is 6
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 48, ), Input shape (1225, 192, ), and Output shape (1225, 48, ), and the ID is 7
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1200, 64, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 8
	Changing Input Shape
	New Input Shape: (1225, 1200, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 64, ), Input shape (1225, 192, ), and Output shape (1225, 64, ), and the ID is 9
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 10
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (1225, 96, ), and the ID is 11
	Changing Input Shape
	New Input Shape: (1225, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (1225, 256, ), and Output shape (1225, 64, ), and the ID is 12
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (1225, 256, ), and Output shape (1225, 64, ), and the ID is 13
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 48, ), Input shape (1225, 256, ), and Output shape (1225, 48, ), and the ID is 14
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1200, 64, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 15
	Changing Input Shape
	New Input Shape: (1225, 1200, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (1225, 256, ), and Output shape (1225, 64, ), and the ID is 16
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 17
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (1225, 96, ), and the ID is 18
	Changing Input Shape
	New Input Shape: (1225, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 19
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 20
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 48, ), Input shape (1225, 288, ), and Output shape (1225, 48, ), and the ID is 21
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1200, 64, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 22
	Changing Input Shape
	New Input Shape: (1225, 1200, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 23
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 24
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (1225, 96, ), and the ID is 25
	Changing Input Shape
	New Input Shape: (1225, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2592, 384, ), Input shape (1225, 288, ), and Output shape (289, 384, ), and the ID is 26
	Changing Input Shape
	New Input Shape: (289, 2592, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 27
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 28
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (289, 96, ), and the ID is 29
	Changing Input Shape
	New Input Shape: (289, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 30
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 31
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 128, ), Input shape (289, 768, ), and Output shape (289, 128, ), and the ID is 32
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 33
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 192, ), Input shape (289, 128, ), and Output shape (289, 192, ), and the ID is 34
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 128, ), Input shape (289, 768, ), and Output shape (289, 128, ), and the ID is 35
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 36
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 37
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 38
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 192, ), Input shape (289, 128, ), and Output shape (289, 192, ), and the ID is 39
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 40
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 41
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 42
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 43
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 44
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 45
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 46
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 47
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 48
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 49
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 50
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 51
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 52
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 53
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 54
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 55
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 56
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 57
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 58
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 59
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 60
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 61
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 62
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 63
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 64
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 65
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 66
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 67
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 68
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 69
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 70
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1728, 320, ), Input shape (289, 192, ), and Output shape (64, 320, ), and the ID is 71
	Changing Input Shape
	New Input Shape: (64, 1728, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 72
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 73
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 74
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1728, 192, ), Input shape (289, 192, ), and Output shape (64, 192, ), and the ID is 75
	Changing Input Shape
	New Input Shape: (64, 1728, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 192, ), Input shape (64, 1280, ), and Output shape (64, 192, ), and the ID is 76
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 320, ), Input shape (64, 1280, ), and Output shape (64, 320, ), and the ID is 77
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 384, ), Input shape (64, 1280, ), and Output shape (64, 384, ), and the ID is 78
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 79
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 80
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 448, ), Input shape (64, 1280, ), and Output shape (64, 448, ), and the ID is 81
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (4032, 384, ), Input shape (64, 448, ), and Output shape (64, 384, ), and the ID is 82
	Changing Input Shape
	New Input Shape: (64, 4032, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 83
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 84
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 192, ), Input shape (64, 2048, ), and Output shape (64, 192, ), and the ID is 85
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 320, ), Input shape (64, 2048, ), and Output shape (64, 320, ), and the ID is 86
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 384, ), Input shape (64, 2048, ), and Output shape (64, 384, ), and the ID is 87
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 88
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 89
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 448, ), Input shape (64, 2048, ), and Output shape (64, 448, ), and the ID is 90
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (4032, 384, ), Input shape (64, 448, ), and Output shape (64, 384, ), and the ID is 91
	Changing Input Shape
	New Input Shape: (64, 4032, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 92
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 93
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
The input model file size (MB): 24.2886
Initialized session in 32.615ms.
Running benchmark for at least 2 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=2 first=1334157 curr=1274968 min=1274968 max=1334157 avg=1.30456e+06 std=29594

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=1276162 curr=1277161 min=1273554 max=1279343 avg=1.27648e+06 std=1413

Inference timings in us: Init: 32615, First inference: 1334157, Warmup (avg): 1.30456e+06, Inference (avg): 1.27648e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=8.78125 overall=42.9766
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   18.278	   18.278	100.000%	100.000%	   384.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   18.278	   18.278	100.000%	100.000%	   384.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	    18.278	   100.000%	   100.000%	   384.000	        1

Timings (microseconds): count=1 curr=18278
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.018	   14.008	   14.011	  1.098%	  1.098%	     0.000	        1	[inception_v3/activation/Relu;inception_v3/batch_normalization/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d/Conv2D]:0
	                 CONV_2D	           14.034	   43.149	   43.287	  3.393%	  4.491%	     0.000	        1	[inception_v3/activation_1/Relu;inception_v3/batch_normalization_1/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_1/Conv2D]:1
	                 CONV_2D	           57.327	   71.431	   71.135	  5.575%	 10.066%	     0.000	        1	[inception_v3/activation_2/Relu;inception_v3/batch_normalization_2/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_2/Conv2D]:2
	             MAX_POOL_2D	          128.468	    9.054	    9.036	  0.708%	 10.775%	     0.000	        1	[inception_v3/max_pooling2d/MaxPool]:3
	                 CONV_2D	          137.511	    6.169	    6.235	  0.489%	 11.263%	     0.000	        1	[inception_v3/activation_3/Relu;inception_v3/batch_normalization_3/FusedBatchNormV3;inception_v3/batch_normalization_3/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_3/Conv2D]:4
	                 CONV_2D	          143.753	   98.287	   97.886	  7.672%	 18.936%	     0.000	        1	[inception_v3/activation_4/Relu;inception_v3/batch_normalization_4/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_4/Conv2D]:5
	             MAX_POOL_2D	          241.647	    5.896	    5.959	  0.467%	 19.403%	     0.000	        1	[inception_v3/max_pooling2d_1/MaxPool]:6
	         AVERAGE_POOL_2D	          247.611	   45.289	   45.318	  3.552%	 22.955%	     0.000	        1	[inception_v3/average_pooling2d/AvgPool]:7
	                 CONV_2D	          292.933	    1.568	    1.539	  0.121%	 23.075%	     0.000	        1	[inception_v3/activation_11/Relu;inception_v3/batch_normalization_11/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_11/Conv2D]:8
	                 CONV_2D	          294.475	    2.545	    2.542	  0.199%	 23.274%	     0.000	        1	[inception_v3/activation_5/Relu;inception_v3/batch_normalization_5/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_5/Conv2D]:9
	                 CONV_2D	          297.021	    1.869	    1.915	  0.150%	 23.425%	     0.000	        1	[inception_v3/activation_6/Relu;inception_v3/batch_normalization_6/FusedBatchNormV3;inception_v3/batch_normalization_13/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_6/Conv2D]:10
	                 CONV_2D	          298.938	   14.615	   14.660	  1.149%	 24.574%	     0.000	        1	[inception_v3/activation_7/Relu;inception_v3/batch_normalization_7/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_7/Conv2D]:11
	                 CONV_2D	          313.603	    2.690	    2.601	  0.204%	 24.777%	     0.000	        1	[inception_v3/activation_8/Relu;inception_v3/batch_normalization_8/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_8/Conv2D]:12
	                 CONV_2D	          316.208	   10.264	   10.319	  0.809%	 25.586%	     0.000	        1	[inception_v3/activation_9/Relu;inception_v3/batch_normalization_9/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_9/Conv2D]:13
	                 CONV_2D	          326.533	   15.068	   15.025	  1.178%	 26.764%	     0.000	        1	[inception_v3/activation_10/Relu;inception_v3/batch_normalization_10/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_10/Conv2D]:14
	           CONCATENATION	          341.564	    0.362	    0.413	  0.032%	 26.796%	     0.000	        1	[inception_v3/mixed0/concat]:15
	         AVERAGE_POOL_2D	          341.979	   60.168	   60.284	  4.725%	 31.521%	     0.000	        1	[inception_v3/average_pooling2d_1/AvgPool]:16
	                 CONV_2D	          402.267	    3.320	    3.295	  0.258%	 31.780%	     0.000	        1	[inception_v3/activation_18/Relu;inception_v3/batch_normalization_18/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_18/Conv2D]:17
	                 CONV_2D	          405.567	    3.220	    3.231	  0.253%	 32.033%	     0.000	        1	[inception_v3/activation_12/Relu;inception_v3/batch_normalization_12/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_12/Conv2D]:18
	                 CONV_2D	          408.803	    2.430	    2.432	  0.191%	 32.223%	     0.000	        1	[inception_v3/activation_13/Relu;inception_v3/batch_normalization_13/FusedBatchNormV3;inception_v3/batch_normalization_13/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_13/Conv2D]:19
	                 CONV_2D	          411.238	   14.737	   14.638	  1.147%	 33.371%	     0.000	        1	[inception_v3/activation_14/Relu;inception_v3/batch_normalization_14/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_14/Conv2D]:20
	                 CONV_2D	          425.882	    3.284	    3.312	  0.260%	 33.630%	     0.000	        1	[inception_v3/activation_15/Relu;inception_v3/batch_normalization_15/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_15/Conv2D]:21
	                 CONV_2D	          429.198	   10.344	   10.312	  0.808%	 34.439%	     0.000	        1	[inception_v3/activation_16/Relu;inception_v3/batch_normalization_16/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_16/Conv2D]:22
	                 CONV_2D	          439.516	   14.983	   14.934	  1.171%	 35.609%	     0.000	        1	[inception_v3/activation_17/Relu;inception_v3/batch_normalization_17/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_17/Conv2D]:23
	           CONCATENATION	          454.455	    0.580	    0.580	  0.045%	 35.655%	     0.000	        1	[inception_v3/mixed1/concat]:24
	         AVERAGE_POOL_2D	          455.038	   67.932	   67.978	  5.328%	 40.983%	     0.000	        1	[inception_v3/average_pooling2d_2/AvgPool]:25
	                 CONV_2D	          523.022	    3.674	    3.690	  0.289%	 41.272%	     0.000	        1	[inception_v3/activation_25/Relu;inception_v3/batch_normalization_25/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_25/Conv2D]:26
	                 CONV_2D	          526.717	    3.636	    3.665	  0.287%	 41.559%	     0.000	        1	[inception_v3/activation_19/Relu;inception_v3/batch_normalization_19/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_19/Conv2D]:27
	                 CONV_2D	          530.386	    2.825	    2.763	  0.217%	 41.776%	     0.000	        1	[inception_v3/activation_20/Relu;inception_v3/batch_normalization_20/FusedBatchNormV3;inception_v3/batch_normalization_13/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_20/Conv2D]:28
	                 CONV_2D	          533.152	   14.675	   14.666	  1.149%	 42.925%	     0.000	        1	[inception_v3/activation_21/Relu;inception_v3/batch_normalization_21/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_21/Conv2D]:29
	                 CONV_2D	          547.824	    3.702	    3.733	  0.293%	 43.218%	     0.000	        1	[inception_v3/activation_22/Relu;inception_v3/batch_normalization_22/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_22/Conv2D]:30
	                 CONV_2D	          551.561	   10.335	   10.291	  0.807%	 44.024%	     0.000	        1	[inception_v3/activation_23/Relu;inception_v3/batch_normalization_23/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_23/Conv2D]:31
	                 CONV_2D	          561.857	   14.979	   15.001	  1.176%	 45.200%	     0.000	        1	[inception_v3/activation_24/Relu;inception_v3/batch_normalization_24/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_24/Conv2D]:32
	           CONCATENATION	          576.863	    0.570	    0.563	  0.044%	 45.244%	     0.000	        1	[inception_v3/mixed2/concat]:33
	                 CONV_2D	          577.428	   38.423	   38.589	  3.025%	 48.269%	     0.000	        1	[inception_v3/activation_26/Relu;inception_v3/batch_normalization_26/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_26/Conv2D]:34
	                 CONV_2D	          616.023	    3.547	    3.599	  0.282%	 48.551%	     0.000	        1	[inception_v3/activation_27/Relu;inception_v3/batch_normalization_27/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_27/Conv2D]:35
	                 CONV_2D	          619.626	   10.367	   10.308	  0.808%	 49.359%	     0.000	        1	[inception_v3/activation_28/Relu;inception_v3/batch_normalization_28/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_28/Conv2D]:36
	                 CONV_2D	          629.941	    3.547	    3.571	  0.280%	 49.639%	     0.000	        1	[inception_v3/activation_29/Relu;inception_v3/batch_normalization_29/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_29/Conv2D]:37
	             MAX_POOL_2D	          633.517	    2.096	    2.149	  0.168%	 49.807%	     0.000	        1	[inception_v3/max_pooling2d_2/MaxPool]:38
	           CONCATENATION	          635.668	    0.251	    0.203	  0.016%	 49.823%	     0.000	        1	[inception_v3/mixed3/concat]:39
	         AVERAGE_POOL_2D	          635.873	   40.951	   40.964	  3.211%	 53.034%	     0.000	        1	[inception_v3/average_pooling2d_3/AvgPool]:40
	                 CONV_2D	          676.841	    5.871	    5.916	  0.464%	 53.497%	     0.000	        1	[inception_v3/activation_39/Relu;inception_v3/batch_normalization_39/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_39/Conv2D]:41
	                 CONV_2D	          682.762	    5.844	    5.852	  0.459%	 53.956%	     0.000	        1	[inception_v3/activation_30/Relu;inception_v3/batch_normalization_30/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_30/Conv2D]:42
	                 CONV_2D	          688.618	    3.963	    3.959	  0.310%	 54.266%	     0.000	        1	[inception_v3/activation_31/Relu;inception_v3/batch_normalization_31/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_31/Conv2D]:43
	                 CONV_2D	          692.581	    4.699	    4.708	  0.369%	 54.635%	     0.000	        1	[inception_v3/activation_32/Relu;inception_v3/batch_normalization_32/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_32/Conv2D]:44
	                 CONV_2D	          697.293	    6.892	    6.837	  0.536%	 55.171%	     0.000	        1	[inception_v3/activation_33/Relu;inception_v3/batch_normalization_33/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_33/Conv2D]:45
	                 CONV_2D	          704.138	    4.188	    4.056	  0.318%	 55.489%	     0.000	        1	[inception_v3/activation_34/Relu;inception_v3/batch_normalization_34/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_34/Conv2D]:46
	                 CONV_2D	          708.198	    4.804	    4.743	  0.372%	 55.861%	     0.000	        1	[inception_v3/activation_35/Relu;inception_v3/batch_normalization_35/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_35/Conv2D]:47
	                 CONV_2D	          712.946	    4.620	    4.652	  0.365%	 56.226%	     0.000	        1	[inception_v3/activation_36/Relu;inception_v3/batch_normalization_36/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_36/Conv2D]:48
	                 CONV_2D	          717.602	    4.683	    4.717	  0.370%	 56.595%	     0.000	        1	[inception_v3/activation_37/Relu;inception_v3/batch_normalization_37/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_37/Conv2D]:49
	                 CONV_2D	          722.324	    6.832	    6.786	  0.532%	 57.127%	     0.000	        1	[inception_v3/activation_38/Relu;inception_v3/batch_normalization_38/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_38/Conv2D]:50
	           CONCATENATION	          729.115	    0.272	    0.236	  0.018%	 57.146%	     0.000	        1	[inception_v3/mixed4/concat]:51
	         AVERAGE_POOL_2D	          729.353	   40.999	   41.027	  3.216%	 60.361%	     0.000	        1	[inception_v3/average_pooling2d_4/AvgPool]:52
	                 CONV_2D	          770.384	    5.946	    5.931	  0.465%	 60.826%	     0.000	        1	[inception_v3/activation_49/Relu;inception_v3/batch_normalization_49/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_49/Conv2D]:53
	                 CONV_2D	          776.320	    5.763	    5.784	  0.453%	 61.279%	     0.000	        1	[inception_v3/activation_40/Relu;inception_v3/batch_normalization_40/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_40/Conv2D]:54
	                 CONV_2D	          782.108	    4.924	    4.858	  0.381%	 61.660%	     0.000	        1	[inception_v3/activation_41/Relu;inception_v3/batch_normalization_41/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_41/Conv2D]:55
	                 CONV_2D	          786.975	    7.151	    7.190	  0.564%	 62.224%	     0.000	        1	[inception_v3/activation_42/Relu;inception_v3/batch_normalization_42/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_42/Conv2D]:56
	                 CONV_2D	          794.170	    8.486	    8.527	  0.668%	 62.892%	     0.000	        1	[inception_v3/activation_43/Relu;inception_v3/batch_normalization_43/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_43/Conv2D]:57
	                 CONV_2D	          802.703	    4.949	    4.991	  0.391%	 63.283%	     0.000	        1	[inception_v3/activation_44/Relu;inception_v3/batch_normalization_44/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_44/Conv2D]:58
	                 CONV_2D	          807.698	    7.145	    7.237	  0.567%	 63.850%	     0.000	        1	[inception_v3/activation_45/Relu;inception_v3/batch_normalization_45/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_45/Conv2D]:59
	                 CONV_2D	          814.940	    7.250	    7.207	  0.565%	 64.415%	     0.000	        1	[inception_v3/activation_46/Relu;inception_v3/batch_normalization_46/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_46/Conv2D]:60
	                 CONV_2D	          822.152	    7.169	    7.206	  0.565%	 64.980%	     0.000	        1	[inception_v3/activation_47/Relu;inception_v3/batch_normalization_47/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_47/Conv2D]:61
	                 CONV_2D	          829.364	    8.559	    8.504	  0.667%	 65.647%	     0.000	        1	[inception_v3/activation_48/Relu;inception_v3/batch_normalization_48/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_48/Conv2D]:62
	           CONCATENATION	          837.874	    0.228	    0.218	  0.017%	 65.664%	     0.000	        1	[inception_v3/mixed5/concat]:63
	         AVERAGE_POOL_2D	          838.093	   40.932	   40.972	  3.211%	 68.875%	     0.000	        1	[inception_v3/average_pooling2d_5/AvgPool]:64
	                 CONV_2D	          879.070	    5.960	    5.952	  0.466%	 69.342%	     0.000	        1	[inception_v3/activation_59/Relu;inception_v3/batch_normalization_59/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_59/Conv2D]:65
	                 CONV_2D	          885.026	    5.751	    5.813	  0.456%	 69.797%	     0.000	        1	[inception_v3/activation_50/Relu;inception_v3/batch_normalization_50/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_50/Conv2D]:66
	                 CONV_2D	          890.844	    4.967	    4.891	  0.383%	 70.181%	     0.000	        1	[inception_v3/activation_51/Relu;inception_v3/batch_normalization_51/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_51/Conv2D]:67
	                 CONV_2D	          895.739	    7.152	    7.174	  0.562%	 70.743%	     0.000	        1	[inception_v3/activation_52/Relu;inception_v3/batch_normalization_52/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_52/Conv2D]:68
	                 CONV_2D	          902.919	    8.522	    8.580	  0.673%	 71.415%	     0.000	        1	[inception_v3/activation_53/Relu;inception_v3/batch_normalization_53/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_53/Conv2D]:69
	                 CONV_2D	          911.505	    4.962	    4.969	  0.389%	 71.805%	     0.000	        1	[inception_v3/activation_54/Relu;inception_v3/batch_normalization_54/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_54/Conv2D]:70
	                 CONV_2D	          916.482	    7.287	    7.259	  0.569%	 72.374%	     0.000	        1	[inception_v3/activation_55/Relu;inception_v3/batch_normalization_55/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_55/Conv2D]:71
	                 CONV_2D	          923.747	    7.099	    7.208	  0.565%	 72.939%	     0.000	        1	[inception_v3/activation_56/Relu;inception_v3/batch_normalization_56/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_56/Conv2D]:72
	                 CONV_2D	          930.960	    7.128	    7.219	  0.566%	 73.505%	     0.000	        1	[inception_v3/activation_57/Relu;inception_v3/batch_normalization_57/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_57/Conv2D]:73
	                 CONV_2D	          938.184	    8.572	    8.486	  0.665%	 74.170%	     0.000	        1	[inception_v3/activation_58/Relu;inception_v3/batch_normalization_58/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_58/Conv2D]:74
	           CONCATENATION	          946.676	    0.202	    0.223	  0.017%	 74.187%	     0.000	        1	[inception_v3/mixed6/concat]:75
	         AVERAGE_POOL_2D	          946.900	   41.049	   41.005	  3.214%	 77.401%	     0.000	        1	[inception_v3/average_pooling2d_6/AvgPool]:76
	                 CONV_2D	          987.909	    5.906	    5.965	  0.468%	 77.869%	     0.000	        1	[inception_v3/activation_69/Relu;inception_v3/batch_normalization_69/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_69/Conv2D]:77
	                 CONV_2D	          993.879	    5.736	    5.796	  0.454%	 78.323%	     0.000	        1	[inception_v3/activation_60/Relu;inception_v3/batch_normalization_60/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_60/Conv2D]:78
	                 CONV_2D	          999.678	    5.833	    5.779	  0.453%	 78.776%	     0.000	        1	[inception_v3/activation_61/Relu;inception_v3/batch_normalization_61/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_61/Conv2D]:79
	                 CONV_2D	         1005.462	   10.280	   10.267	  0.805%	 79.581%	     0.000	        1	[inception_v3/activation_62/Relu;inception_v3/batch_normalization_62/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_62/Conv2D]:80
	                 CONV_2D	         1015.734	   10.289	   10.244	  0.803%	 80.383%	     0.000	        1	[inception_v3/activation_63/Relu;inception_v3/batch_normalization_63/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_63/Conv2D]:81
	                 CONV_2D	         1025.983	    5.804	    5.851	  0.459%	 80.842%	     0.000	        1	[inception_v3/activation_64/Relu;inception_v3/batch_normalization_64/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_64/Conv2D]:82
	                 CONV_2D	         1031.838	   10.144	   10.272	  0.805%	 81.647%	     0.000	        1	[inception_v3/activation_65/Relu;inception_v3/batch_normalization_65/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_65/Conv2D]:83
	                 CONV_2D	         1042.116	   10.242	   10.241	  0.803%	 82.450%	     0.000	        1	[inception_v3/activation_66/Relu;inception_v3/batch_normalization_66/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_66/Conv2D]:84
	                 CONV_2D	         1052.362	   10.329	   10.302	  0.807%	 83.257%	     0.000	        1	[inception_v3/activation_67/Relu;inception_v3/batch_normalization_67/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_67/Conv2D]:85
	                 CONV_2D	         1062.674	   10.100	   10.208	  0.800%	 84.057%	     0.000	        1	[inception_v3/activation_68/Relu;inception_v3/batch_normalization_68/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_68/Conv2D]:86
	           CONCATENATION	         1072.887	    0.195	    0.208	  0.016%	 84.074%	     0.000	        1	[inception_v3/mixed7/concat]:87
	                 CONV_2D	         1073.097	    5.752	    5.840	  0.458%	 84.531%	     0.000	        1	[inception_v3/activation_70/Relu;inception_v3/batch_normalization_70/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_70/Conv2D]:88
	                 CONV_2D	         1078.943	    5.075	    5.185	  0.406%	 84.938%	     0.000	        1	[inception_v3/activation_71/Relu;inception_v3/batch_normalization_71/FusedBatchNormV3;inception_v3/batch_normalization_71/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_71/Conv2D]:89
	                 CONV_2D	         1084.132	    5.951	    5.938	  0.465%	 85.403%	     0.000	        1	[inception_v3/activation_72/Relu;inception_v3/batch_normalization_72/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_72/Conv2D]:90
	                 CONV_2D	         1090.075	   10.297	   10.309	  0.808%	 86.211%	     0.000	        1	[inception_v3/activation_73/Relu;inception_v3/batch_normalization_73/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_73/Conv2D]:91
	                 CONV_2D	         1100.390	   10.143	   10.263	  0.804%	 87.016%	     0.000	        1	[inception_v3/activation_74/Relu;inception_v3/batch_normalization_74/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_74/Conv2D]:92
	                 CONV_2D	         1110.659	    3.058	    3.066	  0.240%	 87.256%	     0.000	        1	[inception_v3/activation_75/Relu;inception_v3/batch_normalization_75/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_75/Conv2D]:93
	             MAX_POOL_2D	         1113.730	    1.223	    1.253	  0.098%	 87.354%	     0.000	        1	[inception_v3/max_pooling2d_3/MaxPool]:94
	           CONCATENATION	         1114.984	    0.069	    0.065	  0.005%	 87.359%	     0.000	        1	[inception_v3/mixed8/concat]:95
	         AVERAGE_POOL_2D	         1115.051	   13.912	   13.918	  1.091%	 88.450%	     0.000	        1	[inception_v3/average_pooling2d_7/AvgPool]:96
	                 CONV_2D	         1128.972	    2.322	    2.319	  0.182%	 88.632%	     0.000	        1	[inception_v3/activation_84/Relu;inception_v3/batch_normalization_84/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_84/Conv2D]:97
	                 CONV_2D	         1131.295	    3.802	    3.752	  0.294%	 88.926%	     0.000	        1	[inception_v3/activation_76/Relu;inception_v3/batch_normalization_76/FusedBatchNormV3;inception_v3/batch_normalization_71/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_76/Conv2D]:98
	                 CONV_2D	         1135.052	    4.510	    4.502	  0.353%	 89.279%	     0.000	        1	[inception_v3/activation_77/Relu;inception_v3/batch_normalization_77/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_77/Conv2D]:99
	                 CONV_2D	         1139.559	    4.049	    4.085	  0.320%	 89.599%	     0.000	        1	[inception_v3/activation_78/Relu;inception_v3/batch_normalization_78/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_78/Conv2D]:100
	                 CONV_2D	         1143.649	    4.101	    4.119	  0.323%	 89.922%	     0.000	        1	[inception_v3/activation_79/Relu;inception_v3/batch_normalization_79/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_79/Conv2D]:101
	           CONCATENATION	         1147.773	    0.070	    0.057	  0.004%	 89.926%	     0.000	        1	[inception_v3/mixed9_0/concat]:102
	                 CONV_2D	         1147.831	    5.355	    5.323	  0.417%	 90.344%	     0.000	        1	[inception_v3/activation_80/Relu;inception_v3/batch_normalization_80/FusedBatchNormV3;inception_v3/batch_normalization_80/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_80/Conv2D]:103
	                 CONV_2D	         1153.159	   13.827	   13.896	  1.089%	 91.433%	     0.000	        1	[inception_v3/activation_81/Relu;inception_v3/batch_normalization_81/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_81/Conv2D]:104
	                 CONV_2D	         1167.060	    4.086	    4.126	  0.323%	 91.756%	     0.000	        1	[inception_v3/activation_82/Relu;inception_v3/batch_normalization_82/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_82/Conv2D]:105
	                 CONV_2D	         1171.190	    4.075	    4.112	  0.322%	 92.078%	     0.000	        1	[inception_v3/activation_83/Relu;inception_v3/batch_normalization_83/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_83/Conv2D]:106
	           CONCATENATION	         1175.307	    0.056	    0.060	  0.005%	 92.083%	     0.000	        1	[inception_v3/concatenate/concat]:107
	           CONCATENATION	         1175.368	    0.138	    0.084	  0.007%	 92.090%	     0.000	        1	[inception_v3/mixed9/concat]:108
	         AVERAGE_POOL_2D	         1175.454	   22.305	   22.302	  1.748%	 93.838%	     0.000	        1	[inception_v3/average_pooling2d_8/AvgPool]:109
	                 CONV_2D	         1197.759	    3.658	    3.585	  0.281%	 94.119%	     0.000	        1	[inception_v3/activation_93/Relu;inception_v3/batch_normalization_93/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_93/Conv2D]:110
	                 CONV_2D	         1201.349	    5.841	    5.876	  0.461%	 94.579%	     0.000	        1	[inception_v3/activation_85/Relu;inception_v3/batch_normalization_85/FusedBatchNormV3;inception_v3/batch_normalization_71/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_85/Conv2D]:111
	                 CONV_2D	         1207.230	    7.086	    6.996	  0.548%	 95.127%	     0.000	        1	[inception_v3/activation_86/Relu;inception_v3/batch_normalization_86/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_86/Conv2D]:112
	                 CONV_2D	         1214.231	    4.198	    4.124	  0.323%	 95.451%	     0.000	        1	[inception_v3/activation_87/Relu;inception_v3/batch_normalization_87/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_87/Conv2D]:113
	                 CONV_2D	         1218.361	    4.074	    4.091	  0.321%	 95.771%	     0.000	        1	[inception_v3/activation_88/Relu;inception_v3/batch_normalization_88/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_88/Conv2D]:114
	           CONCATENATION	         1222.457	    0.070	    0.063	  0.005%	 95.776%	     0.000	        1	[inception_v3/mixed9_1/concat]:115
	                 CONV_2D	         1222.522	    8.189	    8.157	  0.639%	 96.416%	     0.000	        1	[inception_v3/activation_89/Relu;inception_v3/batch_normalization_89/FusedBatchNormV3;inception_v3/batch_normalization_80/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_89/Conv2D]:116
	                 CONV_2D	         1230.684	   13.914	   13.905	  1.090%	 97.506%	     0.000	        1	[inception_v3/activation_90/Relu;inception_v3/batch_normalization_90/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_90/Conv2D]:117
	                 CONV_2D	         1244.594	    4.185	    4.138	  0.324%	 97.830%	     0.000	        1	[inception_v3/activation_91/Relu;inception_v3/batch_normalization_91/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_91/Conv2D]:118
	                 CONV_2D	         1248.738	    4.006	    4.063	  0.318%	 98.148%	     0.000	        1	[inception_v3/activation_92/Relu;inception_v3/batch_normalization_92/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_92/Conv2D]:119
	           CONCATENATION	         1252.805	    0.069	    0.061	  0.005%	 98.153%	     0.000	        1	[inception_v3/concatenate_1/concat]:120
	           CONCATENATION	         1252.867	    0.075	    0.081	  0.006%	 98.159%	     0.000	        1	[inception_v3/mixed10/concat]:121
	                    MEAN	         1252.949	   22.531	   22.570	  1.769%	 99.928%	     0.000	        1	[inception_v3/avg_pool/Mean]:122
	         FULLY_CONNECTED	         1275.521	    0.833	    0.831	  0.065%	 99.994%	     0.000	        1	[inception_v3/predictions/MatMul;inception_v3/predictions/BiasAdd]:123
	                 SOFTMAX	         1276.357	    0.102	    0.082	  0.006%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:124

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          143.753	   98.287	   97.886	  7.672%	  7.672%	     0.000	        1	[inception_v3/activation_4/Relu;inception_v3/batch_normalization_4/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_4/Conv2D]:5
	                 CONV_2D	           57.327	   71.431	   71.135	  5.575%	 13.248%	     0.000	        1	[inception_v3/activation_2/Relu;inception_v3/batch_normalization_2/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_2/Conv2D]:2
	         AVERAGE_POOL_2D	          455.038	   67.932	   67.978	  5.328%	 18.576%	     0.000	        1	[inception_v3/average_pooling2d_2/AvgPool]:25
	         AVERAGE_POOL_2D	          341.979	   60.168	   60.284	  4.725%	 23.301%	     0.000	        1	[inception_v3/average_pooling2d_1/AvgPool]:16
	         AVERAGE_POOL_2D	          247.611	   45.289	   45.318	  3.552%	 26.853%	     0.000	        1	[inception_v3/average_pooling2d/AvgPool]:7
	                 CONV_2D	           14.034	   43.149	   43.287	  3.393%	 30.245%	     0.000	        1	[inception_v3/activation_1/Relu;inception_v3/batch_normalization_1/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_1/Conv2D]:1
	         AVERAGE_POOL_2D	          729.353	   40.999	   41.027	  3.216%	 33.461%	     0.000	        1	[inception_v3/average_pooling2d_4/AvgPool]:52
	         AVERAGE_POOL_2D	          946.900	   41.049	   41.005	  3.214%	 36.675%	     0.000	        1	[inception_v3/average_pooling2d_6/AvgPool]:76
	         AVERAGE_POOL_2D	          838.093	   40.932	   40.972	  3.211%	 39.886%	     0.000	        1	[inception_v3/average_pooling2d_5/AvgPool]:64
	         AVERAGE_POOL_2D	          635.873	   40.951	   40.964	  3.211%	 43.097%	     0.000	        1	[inception_v3/average_pooling2d_3/AvgPool]:40

Number of nodes executed: 125
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       94	   857.055	    67.178%	    67.178%	     0.000	       94
	         AVERAGE_POOL_2D	        9	   373.765	    29.296%	    96.474%	     0.000	        9
	                    MEAN	        1	    22.569	     1.769%	    98.243%	     0.000	        1
	             MAX_POOL_2D	        4	    18.396	     1.442%	    99.685%	     0.000	        4
	           CONCATENATION	       15	     3.106	     0.243%	    99.928%	     0.000	       15
	         FULLY_CONNECTED	        1	     0.831	     0.065%	    99.994%	     0.000	        1
	                 SOFTMAX	        1	     0.082	     0.006%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=1275525 curr=1276591 min=1272940 max=1278692 avg=1.27586e+06 std=1402
Memory (bytes): count=0
125 nodes observed



[ perf record: Woken up 110 times to write data ]
[ perf record: Captured and wrote 27.339 MB /tmp/data.record (112792 samples) ]

29.623

