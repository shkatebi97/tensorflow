STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [2]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/MobileNetV2.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/MobileNetV2.tflite
INFO: Initialized TensorFlow Lite runtime.
NOT Applying Conv Low-Precision for Kernel shape (27, 32, ), Input shape (50176, 3, ), and Output shape (12544, 32, ), and the ID is 0
	Changing Input Shape
	New Input Shape: (12544, 27, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 16, ), Input shape (12544, 32, ), and Output shape (12544, 16, ), and the ID is 1
	Changing Input Shape
	New Input Shape: (12544, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (16, 96, ), Input shape (12544, 16, ), and Output shape (12544, 96, ), and the ID is 2
	Changing Input Shape
	New Input Shape: (12544, 16, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 24, ), Input shape (3136, 96, ), and Output shape (3136, 24, ), and the ID is 3
	Changing Input Shape
	New Input Shape: (3136, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (24, 144, ), Input shape (3136, 24, ), and Output shape (3136, 144, ), and the ID is 4
	Changing Input Shape
	New Input Shape: (3136, 24, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (144, 24, ), Input shape (3136, 144, ), and Output shape (3136, 24, ), and the ID is 5
	Changing Input Shape
	New Input Shape: (3136, 144, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (24, 144, ), Input shape (3136, 24, ), and Output shape (3136, 144, ), and the ID is 6
	Changing Input Shape
	New Input Shape: (3136, 24, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (144, 32, ), Input shape (784, 144, ), and Output shape (784, 32, ), and the ID is 7
	Changing Input Shape
	New Input Shape: (784, 144, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 192, ), Input shape (784, 32, ), and Output shape (784, 192, ), and the ID is 8
	Changing Input Shape
	New Input Shape: (784, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 32, ), Input shape (784, 192, ), and Output shape (784, 32, ), and the ID is 9
	Changing Input Shape
	New Input Shape: (784, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 192, ), Input shape (784, 32, ), and Output shape (784, 192, ), and the ID is 10
	Changing Input Shape
	New Input Shape: (784, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 32, ), Input shape (784, 192, ), and Output shape (784, 32, ), and the ID is 11
	Changing Input Shape
	New Input Shape: (784, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 192, ), Input shape (784, 32, ), and Output shape (784, 192, ), and the ID is 12
	Changing Input Shape
	New Input Shape: (784, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 64, ), Input shape (196, 192, ), and Output shape (196, 64, ), and the ID is 13
	Changing Input Shape
	New Input Shape: (196, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 14
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 64, ), Input shape (196, 384, ), and Output shape (196, 64, ), and the ID is 15
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 16
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 64, ), Input shape (196, 384, ), and Output shape (196, 64, ), and the ID is 17
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 18
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 64, ), Input shape (196, 384, ), and Output shape (196, 64, ), and the ID is 19
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 20
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 96, ), Input shape (196, 384, ), and Output shape (196, 96, ), and the ID is 21
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (196, 96, ), and Output shape (196, 576, ), and the ID is 22
	Changing Input Shape
	New Input Shape: (196, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (196, 576, ), and Output shape (196, 96, ), and the ID is 23
	Changing Input Shape
	New Input Shape: (196, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (196, 96, ), and Output shape (196, 576, ), and the ID is 24
	Changing Input Shape
	New Input Shape: (196, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (196, 576, ), and Output shape (196, 96, ), and the ID is 25
	Changing Input Shape
	New Input Shape: (196, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (196, 96, ), and Output shape (196, 576, ), and the ID is 26
	Changing Input Shape
	New Input Shape: (196, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 160, ), Input shape (49, 576, ), and Output shape (49, 160, ), and the ID is 27
	Changing Input Shape
	New Input Shape: (49, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (160, 960, ), Input shape (49, 160, ), and Output shape (49, 960, ), and the ID is 28
	Changing Input Shape
	New Input Shape: (49, 160, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (960, 160, ), Input shape (49, 960, ), and Output shape (49, 160, ), and the ID is 29
	Changing Input Shape
	New Input Shape: (49, 960, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (160, 960, ), Input shape (49, 160, ), and Output shape (49, 960, ), and the ID is 30
	Changing Input Shape
	New Input Shape: (49, 160, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (960, 160, ), Input shape (49, 960, ), and Output shape (49, 160, ), and the ID is 31
	Changing Input Shape
	New Input Shape: (49, 960, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (160, 960, ), Input shape (49, 160, ), and Output shape (49, 960, ), and the ID is 32
	Changing Input Shape
	New Input Shape: (49, 160, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (960, 320, ), Input shape (49, 960, ), and Output shape (49, 320, ), and the ID is 33
	Changing Input Shape
	New Input Shape: (49, 960, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (320, 1280, ), Input shape (49, 320, ), and Output shape (49, 1280, ), and the ID is 34
	Changing Input Shape
	New Input Shape: (49, 320, )
	No Changes To Appliability.
The input model file size (MB): 3.94093
Initialized session in 20.328ms.
Running benchmark for at least 2 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=2 first=176493 curr=160635 min=160635 max=176493 avg=168564 std=7929

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=160835 curr=161083 min=160520 max=161406 avg=160883 std=196

Inference timings in us: Init: 20328, First inference: 176493, Warmup (avg): 168564, Inference (avg): 160883
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=4.52344 overall=11.7148
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	    7.683	    7.683	100.000%	100.000%	     0.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	    7.683	    7.683	100.000%	100.000%	     0.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	     7.683	   100.000%	   100.000%	     0.000	        1

Timings (microseconds): count=1 curr=7683
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.019	    7.773	    7.805	  4.858%	  4.858%	     0.000	        1	[mobilenetv2_1.00_224/Conv1_relu/Relu6;mobilenetv2_1.00_224/bn_Conv1/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/Conv1/Conv2D]:0
	       DEPTHWISE_CONV_2D	            7.830	    1.447	    1.511	  0.940%	  5.798%	     0.000	        1	[mobilenetv2_1.00_224/expanded_conv_depthwise_relu/Relu6;mobilenetv2_1.00_224/expanded_conv_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/expanded_conv_depthwise/depthwise;mobilenetv2_1.00_224/block_5_project_BN/FusedBatchNormV3]:1
	                 CONV_2D	            9.343	    4.642	    4.667	  2.905%	  8.703%	     0.000	        1	[mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/expanded_conv_project/Conv2D]:2
	                 CONV_2D	           14.015	   10.049	   10.042	  6.250%	 14.952%	     0.000	        1	[mobilenetv2_1.00_224/block_1_expand_relu/Relu6;mobilenetv2_1.00_224/block_1_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_expand/Conv2D]:3
	                     PAD	           24.062	   27.042	   27.047	 16.834%	 31.786%	     0.000	        1	[mobilenetv2_1.00_224/block_1_pad/Pad]:4
	       DEPTHWISE_CONV_2D	           51.115	    2.321	    2.359	  1.468%	 33.254%	     0.000	        1	[mobilenetv2_1.00_224/block_1_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_1_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_depthwise/depthwise;mobilenetv2_1.00_224/block_12_project_BN/FusedBatchNormV3]:5
	                 CONV_2D	           53.479	    2.176	    2.172	  1.352%	 34.606%	     0.000	        1	[mobilenetv2_1.00_224/block_1_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_1_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_project/Conv2D]:6
	                 CONV_2D	           55.656	    4.543	    4.475	  2.785%	 37.391%	     0.000	        1	[mobilenetv2_1.00_224/block_2_expand_relu/Relu6;mobilenetv2_1.00_224/block_2_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_expand/Conv2D]:7
	       DEPTHWISE_CONV_2D	           60.134	    2.127	    2.177	  1.355%	 38.746%	     0.000	        1	[mobilenetv2_1.00_224/block_2_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_depthwise/depthwise;mobilenetv2_1.00_224/block_3_depthwise_BN/FusedBatchNormV3]:8
	                 CONV_2D	           62.315	    2.834	    2.875	  1.789%	 40.535%	     0.000	        1	[mobilenetv2_1.00_224/block_2_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_1_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_project/Conv2D]:9
	                     ADD	           65.195	    7.169	    7.113	  4.427%	 44.962%	     0.000	        1	[mobilenetv2_1.00_224/block_2_add/add]:10
	                 CONV_2D	           72.309	    4.435	    4.439	  2.763%	 47.725%	     0.000	        1	[mobilenetv2_1.00_224/block_3_expand_relu/Relu6;mobilenetv2_1.00_224/block_3_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_expand/Conv2D]:11
	                     PAD	           76.753	   10.165	   10.205	  6.351%	 54.076%	     0.000	        1	[mobilenetv2_1.00_224/block_3_pad/Pad]:12
	       DEPTHWISE_CONV_2D	           86.962	    0.846	    0.892	  0.555%	 54.631%	     0.000	        1	[mobilenetv2_1.00_224/block_3_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_3_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_depthwise/depthwise]:13
	                 CONV_2D	           87.857	    0.848	    0.800	  0.498%	 55.129%	     0.000	        1	[mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_project/Conv2D]:14
	                 CONV_2D	           88.660	    1.405	    1.419	  0.883%	 56.012%	     0.000	        1	[mobilenetv2_1.00_224/block_4_expand_relu/Relu6;mobilenetv2_1.00_224/block_4_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_4_expand/Conv2D]:15
	       DEPTHWISE_CONV_2D	           90.080	    0.712	    0.716	  0.446%	 56.458%	     0.000	        1	[mobilenetv2_1.00_224/block_4_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_4_depthwise/depthwise;mobilenetv2_1.00_224/block_6_depthwise_BN/FusedBatchNormV3]:16
	                 CONV_2D	           90.797	    0.860	    0.898	  0.559%	 57.017%	     0.000	        1	[mobilenetv2_1.00_224/block_4_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_4_project/Conv2D]:17
	                     ADD	           91.698	    2.462	    2.394	  1.490%	 58.507%	     0.000	        1	[mobilenetv2_1.00_224/block_4_add/add]:18
	                 CONV_2D	           94.093	    1.420	    1.432	  0.891%	 59.398%	     0.000	        1	[mobilenetv2_1.00_224/block_5_expand_relu/Relu6;mobilenetv2_1.00_224/block_5_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_5_expand/Conv2D]:19
	       DEPTHWISE_CONV_2D	           95.527	    0.712	    0.698	  0.434%	 59.832%	     0.000	        1	[mobilenetv2_1.00_224/block_5_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_5_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_5_depthwise/depthwise;mobilenetv2_1.00_224/block_6_depthwise_BN/FusedBatchNormV3]:20
	                 CONV_2D	           96.226	    0.851	    0.879	  0.547%	 60.380%	     0.000	        1	[mobilenetv2_1.00_224/block_5_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_5_project/Conv2D]:21
	                     ADD	           97.108	    2.342	    2.389	  1.487%	 61.867%	     0.000	        1	[mobilenetv2_1.00_224/block_5_add/add]:22
	                 CONV_2D	           99.498	    1.420	    1.422	  0.885%	 62.751%	     0.000	        1	[mobilenetv2_1.00_224/block_6_expand_relu/Relu6;mobilenetv2_1.00_224/block_6_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_6_expand/Conv2D]:23
	                     PAD	          100.922	    3.537	    3.471	  2.160%	 64.912%	     0.000	        1	[mobilenetv2_1.00_224/block_6_pad/Pad]:24
	       DEPTHWISE_CONV_2D	          104.395	    0.267	    0.244	  0.152%	 65.064%	     0.000	        1	[mobilenetv2_1.00_224/block_6_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_6_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_6_depthwise/depthwise]:25
	                 CONV_2D	          104.640	    0.461	    0.425	  0.264%	 65.328%	     0.000	        1	[mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_6_project/Conv2D]:26
	                 CONV_2D	          105.067	    1.044	    1.060	  0.660%	 65.988%	     0.000	        1	[mobilenetv2_1.00_224/block_7_expand_relu/Relu6;mobilenetv2_1.00_224/block_7_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_7_expand/Conv2D]:27
	       DEPTHWISE_CONV_2D	          106.128	    0.343	    0.327	  0.203%	 66.191%	     0.000	        1	[mobilenetv2_1.00_224/block_7_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_7_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_7_depthwise/depthwise;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3]:28
	                 CONV_2D	          106.456	    0.749	    0.732	  0.455%	 66.646%	     0.000	        1	[mobilenetv2_1.00_224/block_7_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_7_project/Conv2D]:29
	                     ADD	          107.189	    1.210	    1.182	  0.735%	 67.382%	     0.000	        1	[mobilenetv2_1.00_224/block_7_add/add]:30
	                 CONV_2D	          108.372	    1.033	    1.047	  0.651%	 68.033%	     0.000	        1	[mobilenetv2_1.00_224/block_8_expand_relu/Relu6;mobilenetv2_1.00_224/block_8_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_8_expand/Conv2D]:31
	       DEPTHWISE_CONV_2D	          109.420	    0.322	    0.323	  0.201%	 68.234%	     0.000	        1	[mobilenetv2_1.00_224/block_8_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_8_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_8_depthwise/depthwise;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3]:32
	                 CONV_2D	          109.744	    0.713	    0.720	  0.448%	 68.682%	     0.000	        1	[mobilenetv2_1.00_224/block_8_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_8_project/Conv2D]:33
	                     ADD	          110.465	    1.169	    1.197	  0.745%	 69.427%	     0.000	        1	[mobilenetv2_1.00_224/block_8_add/add]:34
	                 CONV_2D	          111.664	    1.055	    1.043	  0.649%	 70.077%	     0.000	        1	[mobilenetv2_1.00_224/block_9_expand_relu/Relu6;mobilenetv2_1.00_224/block_9_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_9_expand/Conv2D]:35
	       DEPTHWISE_CONV_2D	          112.709	    0.317	    0.320	  0.199%	 70.275%	     0.000	        1	[mobilenetv2_1.00_224/block_9_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_9_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_9_depthwise/depthwise;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3]:36
	                 CONV_2D	          113.029	    0.710	    0.725	  0.451%	 70.726%	     0.000	        1	[mobilenetv2_1.00_224/block_9_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_9_project/Conv2D]:37
	                     ADD	          113.755	    1.168	    1.174	  0.731%	 71.457%	     0.000	        1	[mobilenetv2_1.00_224/block_9_add/add]:38
	                 CONV_2D	          114.931	    1.113	    1.043	  0.649%	 72.106%	     0.000	        1	[mobilenetv2_1.00_224/block_10_expand_relu/Relu6;mobilenetv2_1.00_224/block_10_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_10_expand/Conv2D]:39
	       DEPTHWISE_CONV_2D	          115.975	    0.318	    0.324	  0.202%	 72.308%	     0.000	        1	[mobilenetv2_1.00_224/block_10_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_10_depthwise/depthwise]:40
	                 CONV_2D	          116.300	    1.087	    1.068	  0.665%	 72.972%	     0.000	        1	[mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_10_project/Conv2D]:41
	                 CONV_2D	          117.370	    1.978	    1.995	  1.242%	 74.214%	     0.000	        1	[mobilenetv2_1.00_224/block_11_expand_relu/Relu6;mobilenetv2_1.00_224/block_11_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_11_expand/Conv2D]:42
	       DEPTHWISE_CONV_2D	          119.367	    0.483	    0.475	  0.295%	 74.509%	     0.000	        1	[mobilenetv2_1.00_224/block_11_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_11_depthwise/depthwise;mobilenetv2_1.00_224/block_13_depthwise_BN/FusedBatchNormV3]:43
	                 CONV_2D	          119.842	    1.531	    1.538	  0.957%	 75.466%	     0.000	        1	[mobilenetv2_1.00_224/block_11_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_11_project/Conv2D]:44
	                     ADD	          121.382	    1.752	    1.779	  1.107%	 76.573%	     0.000	        1	[mobilenetv2_1.00_224/block_11_add/add]:45
	                 CONV_2D	          123.162	    2.029	    2.036	  1.267%	 77.840%	     0.000	        1	[mobilenetv2_1.00_224/block_12_expand_relu/Relu6;mobilenetv2_1.00_224/block_12_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_12_expand/Conv2D]:46
	       DEPTHWISE_CONV_2D	          125.200	    0.480	    0.487	  0.303%	 78.144%	     0.000	        1	[mobilenetv2_1.00_224/block_12_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_12_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_12_depthwise/depthwise;mobilenetv2_1.00_224/block_13_depthwise_BN/FusedBatchNormV3]:47
	                 CONV_2D	          125.688	    1.517	    1.525	  0.949%	 79.093%	     0.000	        1	[mobilenetv2_1.00_224/block_12_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_12_project/Conv2D]:48
	                     ADD	          127.215	    1.839	    1.775	  1.105%	 80.198%	     0.000	        1	[mobilenetv2_1.00_224/block_12_add/add]:49
	                 CONV_2D	          128.992	    2.001	    2.029	  1.263%	 81.461%	     0.000	        1	[mobilenetv2_1.00_224/block_13_expand_relu/Relu6;mobilenetv2_1.00_224/block_13_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_13_expand/Conv2D]:50
	                     PAD	          131.023	    2.733	    2.736	  1.703%	 83.163%	     0.000	        1	[mobilenetv2_1.00_224/block_13_pad/Pad]:51
	       DEPTHWISE_CONV_2D	          133.760	    0.184	    0.180	  0.112%	 83.276%	     0.000	        1	[mobilenetv2_1.00_224/block_13_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_13_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_13_depthwise/depthwise]:52
	                 CONV_2D	          133.941	    0.755	    0.759	  0.472%	 83.748%	     0.000	        1	[mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_13_project/Conv2D]:53
	                 CONV_2D	          134.702	    1.421	    1.434	  0.892%	 84.640%	     0.000	        1	[mobilenetv2_1.00_224/block_14_expand_relu/Relu6;mobilenetv2_1.00_224/block_14_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_14_expand/Conv2D]:54
	       DEPTHWISE_CONV_2D	          136.137	    0.243	    0.242	  0.151%	 84.791%	     0.000	        1	[mobilenetv2_1.00_224/block_14_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_14_depthwise/depthwise;mobilenetv2_1.00_224/block_16_depthwise_BN/FusedBatchNormV3]:55
	                 CONV_2D	          136.381	    1.200	    1.185	  0.737%	 85.528%	     0.000	        1	[mobilenetv2_1.00_224/block_14_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_14_project/Conv2D]:56
	                     ADD	          137.567	    0.744	    0.744	  0.463%	 85.991%	     0.000	        1	[mobilenetv2_1.00_224/block_14_add/add]:57
	                 CONV_2D	          138.313	    1.515	    1.428	  0.889%	 86.880%	     0.000	        1	[mobilenetv2_1.00_224/block_15_expand_relu/Relu6;mobilenetv2_1.00_224/block_15_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_15_expand/Conv2D]:58
	       DEPTHWISE_CONV_2D	          139.743	    0.253	    0.241	  0.150%	 87.030%	     0.000	        1	[mobilenetv2_1.00_224/block_15_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_15_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_15_depthwise/depthwise;mobilenetv2_1.00_224/block_16_depthwise_BN/FusedBatchNormV3]:59
	                 CONV_2D	          139.985	    1.157	    1.159	  0.721%	 87.752%	     0.000	        1	[mobilenetv2_1.00_224/block_15_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_15_project/Conv2D]:60
	                     ADD	          141.146	    0.750	    0.755	  0.470%	 88.222%	     0.000	        1	[mobilenetv2_1.00_224/block_15_add/add]:61
	                 CONV_2D	          141.903	    1.384	    1.430	  0.890%	 89.112%	     0.000	        1	[mobilenetv2_1.00_224/block_16_expand_relu/Relu6;mobilenetv2_1.00_224/block_16_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_16_expand/Conv2D]:62
	       DEPTHWISE_CONV_2D	          143.336	    0.236	    0.238	  0.148%	 89.260%	     0.000	        1	[mobilenetv2_1.00_224/block_16_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_16_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_16_depthwise/depthwise]:63
	                 CONV_2D	          143.575	    2.310	    2.316	  1.441%	 90.701%	     0.000	        1	[mobilenetv2_1.00_224/block_16_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_16_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_16_project/Conv2D]:64
	                 CONV_2D	          145.894	    3.378	    3.410	  2.122%	 92.823%	     0.000	        1	[mobilenetv2_1.00_224/out_relu/Relu6;mobilenetv2_1.00_224/Conv_1_bn/FusedBatchNormV3;mobilenetv2_1.00_224/Conv_1_bn/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/Conv_1/Conv2D]:65
	                    MEAN	          149.309	   10.875	   10.861	  6.760%	 99.583%	     0.000	        1	[mobilenetv2_1.00_224/global_average_pooling2d/Mean]:66
	         FULLY_CONNECTED	          160.172	    0.589	    0.592	  0.368%	 99.951%	     0.000	        1	[mobilenetv2_1.00_224/predictions/MatMul;mobilenetv2_1.00_224/predictions/BiasAdd]:67
	                 SOFTMAX	          160.768	    0.075	    0.079	  0.049%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:68

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	           24.062	   27.042	   27.047	 16.834%	 16.834%	     0.000	        1	[mobilenetv2_1.00_224/block_1_pad/Pad]:4
	                    MEAN	          149.309	   10.875	   10.861	  6.760%	 23.593%	     0.000	        1	[mobilenetv2_1.00_224/global_average_pooling2d/Mean]:66
	                     PAD	           76.753	   10.165	   10.205	  6.351%	 29.944%	     0.000	        1	[mobilenetv2_1.00_224/block_3_pad/Pad]:12
	                 CONV_2D	           14.015	   10.049	   10.042	  6.250%	 36.194%	     0.000	        1	[mobilenetv2_1.00_224/block_1_expand_relu/Relu6;mobilenetv2_1.00_224/block_1_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_expand/Conv2D]:3
	                 CONV_2D	            0.019	    7.773	    7.805	  4.858%	 41.052%	     0.000	        1	[mobilenetv2_1.00_224/Conv1_relu/Relu6;mobilenetv2_1.00_224/bn_Conv1/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/Conv1/Conv2D]:0
	                     ADD	           65.195	    7.169	    7.113	  4.427%	 45.479%	     0.000	        1	[mobilenetv2_1.00_224/block_2_add/add]:10
	                 CONV_2D	            9.343	    4.642	    4.667	  2.905%	 48.383%	     0.000	        1	[mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/expanded_conv_project/Conv2D]:2
	                 CONV_2D	           55.656	    4.543	    4.475	  2.785%	 51.168%	     0.000	        1	[mobilenetv2_1.00_224/block_2_expand_relu/Relu6;mobilenetv2_1.00_224/block_2_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_expand/Conv2D]:7
	                 CONV_2D	           72.309	    4.435	    4.439	  2.763%	 53.931%	     0.000	        1	[mobilenetv2_1.00_224/block_3_expand_relu/Relu6;mobilenetv2_1.00_224/block_3_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_expand/Conv2D]:11
	                     PAD	          100.922	    3.537	    3.471	  2.160%	 56.091%	     0.000	        1	[mobilenetv2_1.00_224/block_6_pad/Pad]:24

Number of nodes executed: 69
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       35	    73.411	    45.700%	    45.700%	     0.000	       35
	                     PAD	        4	    43.456	    27.052%	    72.752%	     0.000	        4
	                     ADD	       10	    20.498	    12.760%	    85.512%	     0.000	       10
	       DEPTHWISE_CONV_2D	       17	    11.744	     7.311%	    92.823%	     0.000	       17
	                    MEAN	        1	    10.860	     6.761%	    99.584%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.591	     0.368%	    99.951%	     0.000	        1
	                 SOFTMAX	        1	     0.078	     0.049%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=160629 curr=160844 min=160273 max=161201 avg=160675 std=200
Memory (bytes): count=0
69 nodes observed



[ perf record: Woken up 14 times to write data ]
[ perf record: Captured and wrote 3.309 MB /tmp/data.record (14485 samples) ]

4.593

