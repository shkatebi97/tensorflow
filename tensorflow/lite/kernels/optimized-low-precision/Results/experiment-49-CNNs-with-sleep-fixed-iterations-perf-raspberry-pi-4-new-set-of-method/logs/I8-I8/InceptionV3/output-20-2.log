STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [2]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/InceptionV3.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/InceptionV3.tflite
INFO: Initialized TensorFlow Lite runtime.
NOT Applying Conv Low-Precision for Kernel shape (27, 32, ), Input shape (89401, 3, ), and Output shape (22201, 32, ), and the ID is 0
	Changing Input Shape
	New Input Shape: (22201, 27, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 32, ), Input shape (22201, 32, ), and Output shape (21609, 32, ), and the ID is 1
	Changing Input Shape
	New Input Shape: (21609, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (21609, 32, ), and Output shape (21609, 64, ), and the ID is 2
	Changing Input Shape
	New Input Shape: (21609, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 80, ), Input shape (5329, 64, ), and Output shape (5329, 80, ), and the ID is 3
	Changing Input Shape
	New Input Shape: (5329, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (720, 192, ), Input shape (5329, 80, ), and Output shape (5041, 192, ), and the ID is 4
	Changing Input Shape
	New Input Shape: (5041, 720, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 32, ), Input shape (1225, 192, ), and Output shape (1225, 32, ), and the ID is 5
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 64, ), Input shape (1225, 192, ), and Output shape (1225, 64, ), and the ID is 6
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 48, ), Input shape (1225, 192, ), and Output shape (1225, 48, ), and the ID is 7
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1200, 64, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 8
	Changing Input Shape
	New Input Shape: (1225, 1200, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 64, ), Input shape (1225, 192, ), and Output shape (1225, 64, ), and the ID is 9
	Changing Input Shape
	New Input Shape: (1225, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 10
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (1225, 96, ), and the ID is 11
	Changing Input Shape
	New Input Shape: (1225, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (1225, 256, ), and Output shape (1225, 64, ), and the ID is 12
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (1225, 256, ), and Output shape (1225, 64, ), and the ID is 13
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 48, ), Input shape (1225, 256, ), and Output shape (1225, 48, ), and the ID is 14
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1200, 64, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 15
	Changing Input Shape
	New Input Shape: (1225, 1200, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (256, 64, ), Input shape (1225, 256, ), and Output shape (1225, 64, ), and the ID is 16
	Changing Input Shape
	New Input Shape: (1225, 256, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 17
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (1225, 96, ), and the ID is 18
	Changing Input Shape
	New Input Shape: (1225, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 19
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 20
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 48, ), Input shape (1225, 288, ), and Output shape (1225, 48, ), and the ID is 21
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1200, 64, ), Input shape (1225, 48, ), and Output shape (1225, 64, ), and the ID is 22
	Changing Input Shape
	New Input Shape: (1225, 1200, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 23
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 24
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (1225, 96, ), and the ID is 25
	Changing Input Shape
	New Input Shape: (1225, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2592, 384, ), Input shape (1225, 288, ), and Output shape (289, 384, ), and the ID is 26
	Changing Input Shape
	New Input Shape: (289, 2592, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (288, 64, ), Input shape (1225, 288, ), and Output shape (1225, 64, ), and the ID is 27
	Changing Input Shape
	New Input Shape: (1225, 288, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (1225, 64, ), and Output shape (1225, 96, ), and the ID is 28
	Changing Input Shape
	New Input Shape: (1225, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (864, 96, ), Input shape (1225, 96, ), and Output shape (289, 96, ), and the ID is 29
	Changing Input Shape
	New Input Shape: (289, 864, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 30
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 31
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 128, ), Input shape (289, 768, ), and Output shape (289, 128, ), and the ID is 32
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 33
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 192, ), Input shape (289, 128, ), and Output shape (289, 192, ), and the ID is 34
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 128, ), Input shape (289, 768, ), and Output shape (289, 128, ), and the ID is 35
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 36
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 37
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 128, ), Input shape (289, 128, ), and Output shape (289, 128, ), and the ID is 38
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (896, 192, ), Input shape (289, 128, ), and Output shape (289, 192, ), and the ID is 39
	Changing Input Shape
	New Input Shape: (289, 896, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 40
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 41
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 42
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 43
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 44
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 45
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 46
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 47
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 48
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 49
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 50
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 51
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 52
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 53
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 54
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 160, ), Input shape (289, 768, ), and Output shape (289, 160, ), and the ID is 55
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 56
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 57
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 160, ), Input shape (289, 160, ), and Output shape (289, 160, ), and the ID is 58
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1120, 192, ), Input shape (289, 160, ), and Output shape (289, 192, ), and the ID is 59
	Changing Input Shape
	New Input Shape: (289, 1120, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 60
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 61
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 62
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 63
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 64
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 65
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 66
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 67
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 68
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 69
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 70
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1728, 320, ), Input shape (289, 192, ), and Output shape (64, 320, ), and the ID is 71
	Changing Input Shape
	New Input Shape: (64, 1728, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (768, 192, ), Input shape (289, 768, ), and Output shape (289, 192, ), and the ID is 72
	Changing Input Shape
	New Input Shape: (289, 768, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 73
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1344, 192, ), Input shape (289, 192, ), and Output shape (289, 192, ), and the ID is 74
	Changing Input Shape
	New Input Shape: (289, 1344, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1728, 192, ), Input shape (289, 192, ), and Output shape (64, 192, ), and the ID is 75
	Changing Input Shape
	New Input Shape: (64, 1728, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 192, ), Input shape (64, 1280, ), and Output shape (64, 192, ), and the ID is 76
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 320, ), Input shape (64, 1280, ), and Output shape (64, 320, ), and the ID is 77
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 384, ), Input shape (64, 1280, ), and Output shape (64, 384, ), and the ID is 78
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 79
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 80
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1280, 448, ), Input shape (64, 1280, ), and Output shape (64, 448, ), and the ID is 81
	Changing Input Shape
	New Input Shape: (64, 1280, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (4032, 384, ), Input shape (64, 448, ), and Output shape (64, 384, ), and the ID is 82
	Changing Input Shape
	New Input Shape: (64, 4032, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 83
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 84
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 192, ), Input shape (64, 2048, ), and Output shape (64, 192, ), and the ID is 85
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 320, ), Input shape (64, 2048, ), and Output shape (64, 320, ), and the ID is 86
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 384, ), Input shape (64, 2048, ), and Output shape (64, 384, ), and the ID is 87
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 88
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 89
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (2048, 448, ), Input shape (64, 2048, ), and Output shape (64, 448, ), and the ID is 90
	Changing Input Shape
	New Input Shape: (64, 2048, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (4032, 384, ), Input shape (64, 448, ), and Output shape (64, 384, ), and the ID is 91
	Changing Input Shape
	New Input Shape: (64, 4032, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 92
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (1152, 384, ), Input shape (64, 384, ), and Output shape (64, 384, ), and the ID is 93
	Changing Input Shape
	New Input Shape: (64, 1152, )
	No Changes To Appliability.
The input model file size (MB): 24.2886
Initialized session in 34.005ms.
Running benchmark for at least 2 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=2 first=1328153 curr=1268953 min=1268953 max=1328153 avg=1.29855e+06 std=29600

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=1272678 curr=1269180 min=1267767 max=1273091 avg=1.26957e+06 std=1710

Inference timings in us: Init: 34005, First inference: 1328153, Warmup (avg): 1.29855e+06, Inference (avg): 1.26957e+06
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=8.78125 overall=42.9766
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   19.047	   19.047	100.000%	100.000%	   384.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	   19.047	   19.047	100.000%	100.000%	   384.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	    19.047	   100.000%	   100.000%	   384.000	        1

Timings (microseconds): count=1 curr=19047
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.020	   13.919	   13.985	  1.102%	  1.102%	     0.000	        1	[inception_v3/activation/Relu;inception_v3/batch_normalization/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d/Conv2D]:0
	                 CONV_2D	           14.011	   42.514	   42.116	  3.319%	  4.421%	     0.000	        1	[inception_v3/activation_1/Relu;inception_v3/batch_normalization_1/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_1/Conv2D]:1
	                 CONV_2D	           56.132	   69.298	   69.391	  5.468%	  9.889%	     0.000	        1	[inception_v3/activation_2/Relu;inception_v3/batch_normalization_2/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_2/Conv2D]:2
	             MAX_POOL_2D	          125.529	    8.923	    8.909	  0.702%	 10.591%	     0.000	        1	[inception_v3/max_pooling2d/MaxPool]:3
	                 CONV_2D	          134.444	    6.293	    6.210	  0.489%	 11.081%	     0.000	        1	[inception_v3/activation_3/Relu;inception_v3/batch_normalization_3/FusedBatchNormV3;inception_v3/batch_normalization_3/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_3/Conv2D]:4
	                 CONV_2D	          140.660	   97.429	   97.406	  7.676%	 18.757%	     0.000	        1	[inception_v3/activation_4/Relu;inception_v3/batch_normalization_4/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_4/Conv2D]:5
	             MAX_POOL_2D	          238.072	    5.694	    5.679	  0.448%	 19.204%	     0.000	        1	[inception_v3/max_pooling2d_1/MaxPool]:6
	         AVERAGE_POOL_2D	          243.756	   45.067	   45.173	  3.560%	 22.764%	     0.000	        1	[inception_v3/average_pooling2d/AvgPool]:7
	                 CONV_2D	          288.934	    1.559	    1.516	  0.119%	 22.884%	     0.000	        1	[inception_v3/activation_11/Relu;inception_v3/batch_normalization_11/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_11/Conv2D]:8
	                 CONV_2D	          290.454	    2.520	    2.469	  0.195%	 23.078%	     0.000	        1	[inception_v3/activation_5/Relu;inception_v3/batch_normalization_5/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_5/Conv2D]:9
	                 CONV_2D	          292.925	    1.917	    1.919	  0.151%	 23.229%	     0.000	        1	[inception_v3/activation_6/Relu;inception_v3/batch_normalization_6/FusedBatchNormV3;inception_v3/batch_normalization_13/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_6/Conv2D]:10
	                 CONV_2D	          294.846	   14.649	   14.570	  1.148%	 24.378%	     0.000	        1	[inception_v3/activation_7/Relu;inception_v3/batch_normalization_7/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_7/Conv2D]:11
	                 CONV_2D	          309.422	    2.597	    2.533	  0.200%	 24.577%	     0.000	        1	[inception_v3/activation_8/Relu;inception_v3/batch_normalization_8/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_8/Conv2D]:12
	                 CONV_2D	          311.958	   10.248	   10.306	  0.812%	 25.389%	     0.000	        1	[inception_v3/activation_9/Relu;inception_v3/batch_normalization_9/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_9/Conv2D]:13
	                 CONV_2D	          322.269	   15.056	   14.915	  1.175%	 26.565%	     0.000	        1	[inception_v3/activation_10/Relu;inception_v3/batch_normalization_10/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_10/Conv2D]:14
	           CONCATENATION	          337.189	    0.336	    0.338	  0.027%	 26.591%	     0.000	        1	[inception_v3/mixed0/concat]:15
	         AVERAGE_POOL_2D	          337.529	   60.157	   60.118	  4.738%	 31.329%	     0.000	        1	[inception_v3/average_pooling2d_1/AvgPool]:16
	                 CONV_2D	          397.652	    3.217	    3.244	  0.256%	 31.584%	     0.000	        1	[inception_v3/activation_18/Relu;inception_v3/batch_normalization_18/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_18/Conv2D]:17
	                 CONV_2D	          400.900	    3.163	    3.176	  0.250%	 31.835%	     0.000	        1	[inception_v3/activation_12/Relu;inception_v3/batch_normalization_12/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_12/Conv2D]:18
	                 CONV_2D	          404.079	    2.388	    2.427	  0.191%	 32.026%	     0.000	        1	[inception_v3/activation_13/Relu;inception_v3/batch_normalization_13/FusedBatchNormV3;inception_v3/batch_normalization_13/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_13/Conv2D]:19
	                 CONV_2D	          406.510	   14.668	   14.551	  1.147%	 33.173%	     0.000	        1	[inception_v3/activation_14/Relu;inception_v3/batch_normalization_14/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_14/Conv2D]:20
	                 CONV_2D	          421.066	    3.209	    3.230	  0.255%	 33.427%	     0.000	        1	[inception_v3/activation_15/Relu;inception_v3/batch_normalization_15/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_15/Conv2D]:21
	                 CONV_2D	          424.300	   10.379	   10.278	  0.810%	 34.237%	     0.000	        1	[inception_v3/activation_16/Relu;inception_v3/batch_normalization_16/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_16/Conv2D]:22
	                 CONV_2D	          434.585	   15.045	   14.920	  1.176%	 35.413%	     0.000	        1	[inception_v3/activation_17/Relu;inception_v3/batch_normalization_17/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_17/Conv2D]:23
	           CONCATENATION	          449.510	    0.383	    0.413	  0.033%	 35.446%	     0.000	        1	[inception_v3/mixed1/concat]:24
	         AVERAGE_POOL_2D	          449.927	   67.830	   67.775	  5.341%	 40.787%	     0.000	        1	[inception_v3/average_pooling2d_2/AvgPool]:25
	                 CONV_2D	          517.707	    3.646	    3.621	  0.285%	 41.072%	     0.000	        1	[inception_v3/activation_25/Relu;inception_v3/batch_normalization_25/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_25/Conv2D]:26
	                 CONV_2D	          521.332	    3.630	    3.602	  0.284%	 41.356%	     0.000	        1	[inception_v3/activation_19/Relu;inception_v3/batch_normalization_19/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_19/Conv2D]:27
	                 CONV_2D	          524.939	    2.844	    2.770	  0.218%	 41.574%	     0.000	        1	[inception_v3/activation_20/Relu;inception_v3/batch_normalization_20/FusedBatchNormV3;inception_v3/batch_normalization_13/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_20/Conv2D]:28
	                 CONV_2D	          527.712	   14.564	   14.549	  1.147%	 42.720%	     0.000	        1	[inception_v3/activation_21/Relu;inception_v3/batch_normalization_21/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_21/Conv2D]:29
	                 CONV_2D	          542.266	    3.657	    3.671	  0.289%	 43.010%	     0.000	        1	[inception_v3/activation_22/Relu;inception_v3/batch_normalization_22/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_22/Conv2D]:30
	                 CONV_2D	          545.941	   10.311	   10.298	  0.812%	 43.821%	     0.000	        1	[inception_v3/activation_23/Relu;inception_v3/batch_normalization_23/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_23/Conv2D]:31
	                 CONV_2D	          556.245	   14.957	   14.909	  1.175%	 44.996%	     0.000	        1	[inception_v3/activation_24/Relu;inception_v3/batch_normalization_24/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_24/Conv2D]:32
	           CONCATENATION	          571.160	    0.415	    0.377	  0.030%	 45.026%	     0.000	        1	[inception_v3/mixed2/concat]:33
	                 CONV_2D	          571.540	   38.741	   38.588	  3.041%	 48.067%	     0.000	        1	[inception_v3/activation_26/Relu;inception_v3/batch_normalization_26/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_26/Conv2D]:34
	                 CONV_2D	          610.134	    3.658	    3.584	  0.282%	 48.349%	     0.000	        1	[inception_v3/activation_27/Relu;inception_v3/batch_normalization_27/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_27/Conv2D]:35
	                 CONV_2D	          613.722	   10.437	   10.289	  0.811%	 49.160%	     0.000	        1	[inception_v3/activation_28/Relu;inception_v3/batch_normalization_28/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_28/Conv2D]:36
	                 CONV_2D	          624.016	    3.532	    3.550	  0.280%	 49.440%	     0.000	        1	[inception_v3/activation_29/Relu;inception_v3/batch_normalization_29/FusedBatchNormV3;inception_v3/batch_normalization_10/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_29/Conv2D]:37
	             MAX_POOL_2D	          627.573	    2.024	    2.054	  0.162%	 49.602%	     0.000	        1	[inception_v3/max_pooling2d_2/MaxPool]:38
	           CONCATENATION	          629.629	    0.243	    0.213	  0.017%	 49.618%	     0.000	        1	[inception_v3/mixed3/concat]:39
	         AVERAGE_POOL_2D	          629.844	   40.861	   40.886	  3.222%	 52.840%	     0.000	        1	[inception_v3/average_pooling2d_3/AvgPool]:40
	                 CONV_2D	          670.735	    5.876	    5.925	  0.467%	 53.307%	     0.000	        1	[inception_v3/activation_39/Relu;inception_v3/batch_normalization_39/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_39/Conv2D]:41
	                 CONV_2D	          676.664	    5.913	    5.857	  0.462%	 53.769%	     0.000	        1	[inception_v3/activation_30/Relu;inception_v3/batch_normalization_30/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_30/Conv2D]:42
	                 CONV_2D	          682.525	    3.957	    3.964	  0.312%	 54.081%	     0.000	        1	[inception_v3/activation_31/Relu;inception_v3/batch_normalization_31/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_31/Conv2D]:43
	                 CONV_2D	          686.493	    4.974	    4.758	  0.375%	 54.456%	     0.000	        1	[inception_v3/activation_32/Relu;inception_v3/batch_normalization_32/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_32/Conv2D]:44
	                 CONV_2D	          691.255	    6.848	    6.838	  0.539%	 54.995%	     0.000	        1	[inception_v3/activation_33/Relu;inception_v3/batch_normalization_33/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_33/Conv2D]:45
	                 CONV_2D	          698.098	    3.988	    4.056	  0.320%	 55.315%	     0.000	        1	[inception_v3/activation_34/Relu;inception_v3/batch_normalization_34/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_34/Conv2D]:46
	                 CONV_2D	          702.157	    4.700	    4.761	  0.375%	 55.690%	     0.000	        1	[inception_v3/activation_35/Relu;inception_v3/batch_normalization_35/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_35/Conv2D]:47
	                 CONV_2D	          706.922	    4.705	    4.656	  0.367%	 56.057%	     0.000	        1	[inception_v3/activation_36/Relu;inception_v3/batch_normalization_36/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_36/Conv2D]:48
	                 CONV_2D	          711.583	    4.959	    4.713	  0.371%	 56.428%	     0.000	        1	[inception_v3/activation_37/Relu;inception_v3/batch_normalization_37/FusedBatchNormV3;inception_v3/batch_normalization_31/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_37/Conv2D]:49
	                 CONV_2D	          716.300	    6.795	    6.744	  0.531%	 56.960%	     0.000	        1	[inception_v3/activation_38/Relu;inception_v3/batch_normalization_38/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_38/Conv2D]:50
	           CONCATENATION	          723.050	    0.212	    0.226	  0.018%	 56.977%	     0.000	        1	[inception_v3/mixed4/concat]:51
	         AVERAGE_POOL_2D	          723.278	   40.992	   40.976	  3.229%	 60.206%	     0.000	        1	[inception_v3/average_pooling2d_4/AvgPool]:52
	                 CONV_2D	          764.258	    5.918	    5.917	  0.466%	 60.673%	     0.000	        1	[inception_v3/activation_49/Relu;inception_v3/batch_normalization_49/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_49/Conv2D]:53
	                 CONV_2D	          770.179	    5.850	    5.790	  0.456%	 61.129%	     0.000	        1	[inception_v3/activation_40/Relu;inception_v3/batch_normalization_40/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_40/Conv2D]:54
	                 CONV_2D	          775.974	    4.832	    4.839	  0.381%	 61.510%	     0.000	        1	[inception_v3/activation_41/Relu;inception_v3/batch_normalization_41/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_41/Conv2D]:55
	                 CONV_2D	          780.819	    7.157	    7.195	  0.567%	 62.077%	     0.000	        1	[inception_v3/activation_42/Relu;inception_v3/batch_normalization_42/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_42/Conv2D]:56
	                 CONV_2D	          788.019	    8.571	    8.521	  0.671%	 62.749%	     0.000	        1	[inception_v3/activation_43/Relu;inception_v3/batch_normalization_43/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_43/Conv2D]:57
	                 CONV_2D	          796.545	    5.030	    4.952	  0.390%	 63.139%	     0.000	        1	[inception_v3/activation_44/Relu;inception_v3/batch_normalization_44/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_44/Conv2D]:58
	                 CONV_2D	          801.501	    7.256	    7.240	  0.571%	 63.709%	     0.000	        1	[inception_v3/activation_45/Relu;inception_v3/batch_normalization_45/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_45/Conv2D]:59
	                 CONV_2D	          808.746	    7.142	    7.207	  0.568%	 64.277%	     0.000	        1	[inception_v3/activation_46/Relu;inception_v3/batch_normalization_46/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_46/Conv2D]:60
	                 CONV_2D	          815.958	    7.137	    7.213	  0.568%	 64.846%	     0.000	        1	[inception_v3/activation_47/Relu;inception_v3/batch_normalization_47/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_47/Conv2D]:61
	                 CONV_2D	          823.176	    8.584	    8.498	  0.670%	 65.515%	     0.000	        1	[inception_v3/activation_48/Relu;inception_v3/batch_normalization_48/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_48/Conv2D]:62
	           CONCATENATION	          831.679	    0.178	    0.206	  0.016%	 65.532%	     0.000	        1	[inception_v3/mixed5/concat]:63
	         AVERAGE_POOL_2D	          831.887	   40.918	   40.915	  3.224%	 68.756%	     0.000	        1	[inception_v3/average_pooling2d_5/AvgPool]:64
	                 CONV_2D	          872.807	    5.908	    5.948	  0.469%	 69.225%	     0.000	        1	[inception_v3/activation_59/Relu;inception_v3/batch_normalization_59/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_59/Conv2D]:65
	                 CONV_2D	          878.759	    5.741	    5.796	  0.457%	 69.681%	     0.000	        1	[inception_v3/activation_50/Relu;inception_v3/batch_normalization_50/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_50/Conv2D]:66
	                 CONV_2D	          884.559	    4.809	    4.851	  0.382%	 70.064%	     0.000	        1	[inception_v3/activation_51/Relu;inception_v3/batch_normalization_51/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_51/Conv2D]:67
	                 CONV_2D	          889.414	    7.356	    7.209	  0.568%	 70.632%	     0.000	        1	[inception_v3/activation_52/Relu;inception_v3/batch_normalization_52/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_52/Conv2D]:68
	                 CONV_2D	          896.628	    8.559	    8.516	  0.671%	 71.303%	     0.000	        1	[inception_v3/activation_53/Relu;inception_v3/batch_normalization_53/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_53/Conv2D]:69
	                 CONV_2D	          905.149	    5.084	    4.937	  0.389%	 71.692%	     0.000	        1	[inception_v3/activation_54/Relu;inception_v3/batch_normalization_54/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_54/Conv2D]:70
	                 CONV_2D	          910.089	    7.271	    7.232	  0.570%	 72.262%	     0.000	        1	[inception_v3/activation_55/Relu;inception_v3/batch_normalization_55/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_55/Conv2D]:71
	                 CONV_2D	          917.326	    7.200	    7.178	  0.566%	 72.827%	     0.000	        1	[inception_v3/activation_56/Relu;inception_v3/batch_normalization_56/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_56/Conv2D]:72
	                 CONV_2D	          924.508	    7.207	    7.264	  0.572%	 73.400%	     0.000	        1	[inception_v3/activation_57/Relu;inception_v3/batch_normalization_57/FusedBatchNormV3;inception_v3/batch_normalization_41/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_57/Conv2D]:73
	                 CONV_2D	          931.777	    8.525	    8.470	  0.667%	 74.067%	     0.000	        1	[inception_v3/activation_58/Relu;inception_v3/batch_normalization_58/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_58/Conv2D]:74
	           CONCATENATION	          940.253	    0.213	    0.214	  0.017%	 74.084%	     0.000	        1	[inception_v3/mixed6/concat]:75
	         AVERAGE_POOL_2D	          940.470	   41.119	   40.966	  3.228%	 77.313%	     0.000	        1	[inception_v3/average_pooling2d_6/AvgPool]:76
	                 CONV_2D	          981.440	    6.007	    5.934	  0.468%	 77.780%	     0.000	        1	[inception_v3/activation_69/Relu;inception_v3/batch_normalization_69/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_69/Conv2D]:77
	                 CONV_2D	          987.378	    5.895	    5.782	  0.456%	 78.236%	     0.000	        1	[inception_v3/activation_60/Relu;inception_v3/batch_normalization_60/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_60/Conv2D]:78
	                 CONV_2D	          993.163	    5.717	    5.775	  0.455%	 78.691%	     0.000	        1	[inception_v3/activation_61/Relu;inception_v3/batch_normalization_61/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_61/Conv2D]:79
	                 CONV_2D	          998.942	   10.249	   10.241	  0.807%	 79.498%	     0.000	        1	[inception_v3/activation_62/Relu;inception_v3/batch_normalization_62/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_62/Conv2D]:80
	                 CONV_2D	         1009.188	   10.275	   10.255	  0.808%	 80.306%	     0.000	        1	[inception_v3/activation_63/Relu;inception_v3/batch_normalization_63/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_63/Conv2D]:81
	                 CONV_2D	         1019.448	    5.927	    5.823	  0.459%	 80.765%	     0.000	        1	[inception_v3/activation_64/Relu;inception_v3/batch_normalization_64/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_64/Conv2D]:82
	                 CONV_2D	         1025.275	   10.220	   10.298	  0.812%	 81.576%	     0.000	        1	[inception_v3/activation_65/Relu;inception_v3/batch_normalization_65/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_65/Conv2D]:83
	                 CONV_2D	         1035.578	   10.471	   10.247	  0.808%	 82.384%	     0.000	        1	[inception_v3/activation_66/Relu;inception_v3/batch_normalization_66/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_66/Conv2D]:84
	                 CONV_2D	         1045.831	   10.193	   10.264	  0.809%	 83.193%	     0.000	        1	[inception_v3/activation_67/Relu;inception_v3/batch_normalization_67/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_67/Conv2D]:85
	                 CONV_2D	         1056.100	   10.209	   10.208	  0.804%	 83.997%	     0.000	        1	[inception_v3/activation_68/Relu;inception_v3/batch_normalization_68/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_68/Conv2D]:86
	           CONCATENATION	         1066.316	    0.208	    0.208	  0.016%	 84.014%	     0.000	        1	[inception_v3/mixed7/concat]:87
	                 CONV_2D	         1066.526	    5.846	    5.863	  0.462%	 84.476%	     0.000	        1	[inception_v3/activation_70/Relu;inception_v3/batch_normalization_70/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_70/Conv2D]:88
	                 CONV_2D	         1072.394	    5.377	    5.185	  0.409%	 84.884%	     0.000	        1	[inception_v3/activation_71/Relu;inception_v3/batch_normalization_71/FusedBatchNormV3;inception_v3/batch_normalization_71/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_71/Conv2D]:89
	                 CONV_2D	         1077.584	    5.802	    5.867	  0.462%	 85.347%	     0.000	        1	[inception_v3/activation_72/Relu;inception_v3/batch_normalization_72/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_72/Conv2D]:90
	                 CONV_2D	         1083.455	   10.254	   10.212	  0.805%	 86.151%	     0.000	        1	[inception_v3/activation_73/Relu;inception_v3/batch_normalization_73/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_73/Conv2D]:91
	                 CONV_2D	         1093.673	   10.277	   10.269	  0.809%	 86.961%	     0.000	        1	[inception_v3/activation_74/Relu;inception_v3/batch_normalization_74/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_74/Conv2D]:92
	                 CONV_2D	         1103.947	    3.048	    3.084	  0.243%	 87.204%	     0.000	        1	[inception_v3/activation_75/Relu;inception_v3/batch_normalization_75/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_75/Conv2D]:93
	             MAX_POOL_2D	         1107.036	    1.242	    1.236	  0.097%	 87.301%	     0.000	        1	[inception_v3/max_pooling2d_3/MaxPool]:94
	           CONCATENATION	         1108.273	    0.060	    0.062	  0.005%	 87.306%	     0.000	        1	[inception_v3/mixed8/concat]:95
	         AVERAGE_POOL_2D	         1108.336	   13.927	   13.889	  1.095%	 88.400%	     0.000	        1	[inception_v3/average_pooling2d_7/AvgPool]:96
	                 CONV_2D	         1122.228	    2.406	    2.321	  0.183%	 88.583%	     0.000	        1	[inception_v3/activation_84/Relu;inception_v3/batch_normalization_84/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_84/Conv2D]:97
	                 CONV_2D	         1124.551	    3.724	    3.779	  0.298%	 88.881%	     0.000	        1	[inception_v3/activation_76/Relu;inception_v3/batch_normalization_76/FusedBatchNormV3;inception_v3/batch_normalization_71/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_76/Conv2D]:98
	                 CONV_2D	         1128.336	    4.574	    4.525	  0.357%	 89.238%	     0.000	        1	[inception_v3/activation_77/Relu;inception_v3/batch_normalization_77/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_77/Conv2D]:99
	                 CONV_2D	         1132.865	    4.034	    4.116	  0.324%	 89.562%	     0.000	        1	[inception_v3/activation_78/Relu;inception_v3/batch_normalization_78/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_78/Conv2D]:100
	                 CONV_2D	         1136.987	    4.095	    4.119	  0.325%	 89.887%	     0.000	        1	[inception_v3/activation_79/Relu;inception_v3/batch_normalization_79/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_79/Conv2D]:101
	           CONCATENATION	         1141.111	    0.053	    0.061	  0.005%	 89.891%	     0.000	        1	[inception_v3/mixed9_0/concat]:102
	                 CONV_2D	         1141.173	    5.348	    5.328	  0.420%	 90.311%	     0.000	        1	[inception_v3/activation_80/Relu;inception_v3/batch_normalization_80/FusedBatchNormV3;inception_v3/batch_normalization_80/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_80/Conv2D]:103
	                 CONV_2D	         1146.507	   13.952	   13.964	  1.100%	 91.412%	     0.000	        1	[inception_v3/activation_81/Relu;inception_v3/batch_normalization_81/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_81/Conv2D]:104
	                 CONV_2D	         1160.476	    4.095	    4.093	  0.323%	 91.734%	     0.000	        1	[inception_v3/activation_82/Relu;inception_v3/batch_normalization_82/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_82/Conv2D]:105
	                 CONV_2D	         1164.574	    4.028	    4.102	  0.323%	 92.058%	     0.000	        1	[inception_v3/activation_83/Relu;inception_v3/batch_normalization_83/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_83/Conv2D]:106
	           CONCATENATION	         1168.685	    0.055	    0.056	  0.004%	 92.062%	     0.000	        1	[inception_v3/concatenate/concat]:107
	           CONCATENATION	         1168.742	    0.077	    0.083	  0.007%	 92.068%	     0.000	        1	[inception_v3/mixed9/concat]:108
	         AVERAGE_POOL_2D	         1168.826	   22.207	   22.234	  1.752%	 93.821%	     0.000	        1	[inception_v3/average_pooling2d_8/AvgPool]:109
	                 CONV_2D	         1191.063	    3.638	    3.556	  0.280%	 94.101%	     0.000	        1	[inception_v3/activation_93/Relu;inception_v3/batch_normalization_93/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_93/Conv2D]:110
	                 CONV_2D	         1194.623	    5.781	    5.853	  0.461%	 94.562%	     0.000	        1	[inception_v3/activation_85/Relu;inception_v3/batch_normalization_85/FusedBatchNormV3;inception_v3/batch_normalization_71/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_85/Conv2D]:111
	                 CONV_2D	         1200.481	    6.978	    6.952	  0.548%	 95.110%	     0.000	        1	[inception_v3/activation_86/Relu;inception_v3/batch_normalization_86/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_86/Conv2D]:112
	                 CONV_2D	         1207.439	    4.276	    4.126	  0.325%	 95.435%	     0.000	        1	[inception_v3/activation_87/Relu;inception_v3/batch_normalization_87/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_87/Conv2D]:113
	                 CONV_2D	         1211.571	    4.072	    4.079	  0.321%	 95.756%	     0.000	        1	[inception_v3/activation_88/Relu;inception_v3/batch_normalization_88/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_88/Conv2D]:114
	           CONCATENATION	         1215.655	    0.051	    0.056	  0.004%	 95.761%	     0.000	        1	[inception_v3/mixed9_1/concat]:115
	                 CONV_2D	         1215.712	    8.161	    8.149	  0.642%	 96.403%	     0.000	        1	[inception_v3/activation_89/Relu;inception_v3/batch_normalization_89/FusedBatchNormV3;inception_v3/batch_normalization_80/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_89/Conv2D]:116
	                 CONV_2D	         1223.866	   13.913	   13.914	  1.096%	 97.499%	     0.000	        1	[inception_v3/activation_90/Relu;inception_v3/batch_normalization_90/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_90/Conv2D]:117
	                 CONV_2D	         1237.785	    4.207	    4.150	  0.327%	 97.826%	     0.000	        1	[inception_v3/activation_91/Relu;inception_v3/batch_normalization_91/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_91/Conv2D]:118
	                 CONV_2D	         1241.939	    3.995	    4.040	  0.318%	 98.145%	     0.000	        1	[inception_v3/activation_92/Relu;inception_v3/batch_normalization_92/FusedBatchNormV3;inception_v3/batch_normalization_26/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_92/Conv2D]:119
	           CONCATENATION	         1245.984	    0.052	    0.057	  0.004%	 98.149%	     0.000	        1	[inception_v3/concatenate_1/concat]:120
	           CONCATENATION	         1246.042	    0.090	    0.080	  0.006%	 98.156%	     0.000	        1	[inception_v3/mixed10/concat]:121
	                    MEAN	         1246.123	   22.536	   22.523	  1.775%	 99.931%	     0.000	        1	[inception_v3/avg_pool/Mean]:122
	         FULLY_CONNECTED	         1268.650	    0.841	    0.787	  0.062%	 99.993%	     0.000	        1	[inception_v3/predictions/MatMul;inception_v3/predictions/BiasAdd]:123
	                 SOFTMAX	         1269.441	    0.097	    0.094	  0.007%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:124

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	          140.660	   97.429	   97.406	  7.676%	  7.676%	     0.000	        1	[inception_v3/activation_4/Relu;inception_v3/batch_normalization_4/FusedBatchNormV3;inception_v3/batch_normalization_30/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_4/Conv2D]:5
	                 CONV_2D	           56.132	   69.298	   69.391	  5.468%	 13.144%	     0.000	        1	[inception_v3/activation_2/Relu;inception_v3/batch_normalization_2/FusedBatchNormV3;inception_v3/batch_normalization_12/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_2/Conv2D]:2
	         AVERAGE_POOL_2D	          449.927	   67.830	   67.775	  5.341%	 18.485%	     0.000	        1	[inception_v3/average_pooling2d_2/AvgPool]:25
	         AVERAGE_POOL_2D	          337.529	   60.157	   60.118	  4.738%	 23.223%	     0.000	        1	[inception_v3/average_pooling2d_1/AvgPool]:16
	         AVERAGE_POOL_2D	          243.756	   45.067	   45.173	  3.560%	 26.783%	     0.000	        1	[inception_v3/average_pooling2d/AvgPool]:7
	                 CONV_2D	           14.011	   42.514	   42.116	  3.319%	 30.101%	     0.000	        1	[inception_v3/activation_1/Relu;inception_v3/batch_normalization_1/FusedBatchNormV3;inception_v3/batch_normalization_11/FusedBatchNormV3/ReadVariableOp;inception_v3/conv2d_1/Conv2D]:1
	         AVERAGE_POOL_2D	          723.278	   40.992	   40.976	  3.229%	 33.331%	     0.000	        1	[inception_v3/average_pooling2d_4/AvgPool]:52
	         AVERAGE_POOL_2D	          940.470	   41.119	   40.966	  3.228%	 36.559%	     0.000	        1	[inception_v3/average_pooling2d_6/AvgPool]:76
	         AVERAGE_POOL_2D	          831.887	   40.918	   40.915	  3.224%	 39.783%	     0.000	        1	[inception_v3/average_pooling2d_5/AvgPool]:64
	         AVERAGE_POOL_2D	          629.844	   40.861	   40.886	  3.222%	 43.005%	     0.000	        1	[inception_v3/average_pooling2d_3/AvgPool]:40

Number of nodes executed: 125
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       94	   852.056	    67.149%	    67.149%	     0.000	       94
	         AVERAGE_POOL_2D	        9	   372.928	    29.390%	    96.539%	     0.000	        9
	                    MEAN	        1	    22.523	     1.775%	    98.314%	     0.000	        1
	             MAX_POOL_2D	        4	    17.877	     1.409%	    99.722%	     0.000	        4
	           CONCATENATION	       15	     2.641	     0.208%	    99.931%	     0.000	       15
	         FULLY_CONNECTED	        1	     0.787	     0.062%	    99.993%	     0.000	        1
	                 SOFTMAX	        1	     0.094	     0.007%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=1272002 curr=1268567 min=1267196 max=1272470 avg=1.26897e+06 std=1698
Memory (bytes): count=0
125 nodes observed



[ perf record: Woken up 109 times to write data ]
[ perf record: Captured and wrote 27.190 MB /tmp/data.record (112176 samples) ]

29.629

