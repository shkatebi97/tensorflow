STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [20]
Min runs duration (seconds): [1e-09]
Num threads: [1]
Use caching: [1]
Min warmup runs: [2]
Min warmup runs duration (seconds): [1e-09]
Graph: [/home/pi/Desktop/run-CNNs/models/i8i8/MobileNetV2.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Use xnnpack: [0]
Loaded model /home/pi/Desktop/run-CNNs/models/i8i8/MobileNetV2.tflite
INFO: Initialized TensorFlow Lite runtime.
NOT Applying Conv Low-Precision for Kernel shape (27, 32, ), Input shape (50176, 3, ), and Output shape (12544, 32, ), and the ID is 0
	Changing Input Shape
	New Input Shape: (12544, 27, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 16, ), Input shape (12544, 32, ), and Output shape (12544, 16, ), and the ID is 1
	Changing Input Shape
	New Input Shape: (12544, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (16, 96, ), Input shape (12544, 16, ), and Output shape (12544, 96, ), and the ID is 2
	Changing Input Shape
	New Input Shape: (12544, 16, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 24, ), Input shape (3136, 96, ), and Output shape (3136, 24, ), and the ID is 3
	Changing Input Shape
	New Input Shape: (3136, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (24, 144, ), Input shape (3136, 24, ), and Output shape (3136, 144, ), and the ID is 4
	Changing Input Shape
	New Input Shape: (3136, 24, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (144, 24, ), Input shape (3136, 144, ), and Output shape (3136, 24, ), and the ID is 5
	Changing Input Shape
	New Input Shape: (3136, 144, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (24, 144, ), Input shape (3136, 24, ), and Output shape (3136, 144, ), and the ID is 6
	Changing Input Shape
	New Input Shape: (3136, 24, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (144, 32, ), Input shape (784, 144, ), and Output shape (784, 32, ), and the ID is 7
	Changing Input Shape
	New Input Shape: (784, 144, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 192, ), Input shape (784, 32, ), and Output shape (784, 192, ), and the ID is 8
	Changing Input Shape
	New Input Shape: (784, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 32, ), Input shape (784, 192, ), and Output shape (784, 32, ), and the ID is 9
	Changing Input Shape
	New Input Shape: (784, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 192, ), Input shape (784, 32, ), and Output shape (784, 192, ), and the ID is 10
	Changing Input Shape
	New Input Shape: (784, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 32, ), Input shape (784, 192, ), and Output shape (784, 32, ), and the ID is 11
	Changing Input Shape
	New Input Shape: (784, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (32, 192, ), Input shape (784, 32, ), and Output shape (784, 192, ), and the ID is 12
	Changing Input Shape
	New Input Shape: (784, 32, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (192, 64, ), Input shape (196, 192, ), and Output shape (196, 64, ), and the ID is 13
	Changing Input Shape
	New Input Shape: (196, 192, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 14
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 64, ), Input shape (196, 384, ), and Output shape (196, 64, ), and the ID is 15
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 16
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 64, ), Input shape (196, 384, ), and Output shape (196, 64, ), and the ID is 17
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 18
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 64, ), Input shape (196, 384, ), and Output shape (196, 64, ), and the ID is 19
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (64, 384, ), Input shape (196, 64, ), and Output shape (196, 384, ), and the ID is 20
	Changing Input Shape
	New Input Shape: (196, 64, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (384, 96, ), Input shape (196, 384, ), and Output shape (196, 96, ), and the ID is 21
	Changing Input Shape
	New Input Shape: (196, 384, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (196, 96, ), and Output shape (196, 576, ), and the ID is 22
	Changing Input Shape
	New Input Shape: (196, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (196, 576, ), and Output shape (196, 96, ), and the ID is 23
	Changing Input Shape
	New Input Shape: (196, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (196, 96, ), and Output shape (196, 576, ), and the ID is 24
	Changing Input Shape
	New Input Shape: (196, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 96, ), Input shape (196, 576, ), and Output shape (196, 96, ), and the ID is 25
	Changing Input Shape
	New Input Shape: (196, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (96, 576, ), Input shape (196, 96, ), and Output shape (196, 576, ), and the ID is 26
	Changing Input Shape
	New Input Shape: (196, 96, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (576, 160, ), Input shape (49, 576, ), and Output shape (49, 160, ), and the ID is 27
	Changing Input Shape
	New Input Shape: (49, 576, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (160, 960, ), Input shape (49, 160, ), and Output shape (49, 960, ), and the ID is 28
	Changing Input Shape
	New Input Shape: (49, 160, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (960, 160, ), Input shape (49, 960, ), and Output shape (49, 160, ), and the ID is 29
	Changing Input Shape
	New Input Shape: (49, 960, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (160, 960, ), Input shape (49, 160, ), and Output shape (49, 960, ), and the ID is 30
	Changing Input Shape
	New Input Shape: (49, 160, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (960, 160, ), Input shape (49, 960, ), and Output shape (49, 160, ), and the ID is 31
	Changing Input Shape
	New Input Shape: (49, 960, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (160, 960, ), Input shape (49, 160, ), and Output shape (49, 960, ), and the ID is 32
	Changing Input Shape
	New Input Shape: (49, 160, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (960, 320, ), Input shape (49, 960, ), and Output shape (49, 320, ), and the ID is 33
	Changing Input Shape
	New Input Shape: (49, 960, )
	No Changes To Appliability.
NOT Applying Conv Low-Precision for Kernel shape (320, 1280, ), Input shape (49, 320, ), and Output shape (49, 1280, ), and the ID is 34
	Changing Input Shape
	New Input Shape: (49, 320, )
	No Changes To Appliability.
The input model file size (MB): 3.94093
Initialized session in 22.161ms.
Running benchmark for at least 2 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=2 first=185953 curr=162095 min=162095 max=185953 avg=174024 std=11929

Running benchmark for at least 20 iterations and at least 1e-09 seconds but terminate if exceeding 150 seconds.
count=20 first=162786 curr=162699 min=162057 max=162836 avg=162508 std=213

Inference timings in us: Init: 22161, First inference: 185953, Warmup (avg): 174024, Inference (avg): 162508
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=4.5 overall=11.707
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	    9.163	    9.163	100.000%	100.000%	     0.000	        1	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	         AllocateTensors	            0.000	    9.163	    9.163	100.000%	100.000%	     0.000	        1	AllocateTensors/0

Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         AllocateTensors	        1	     9.163	   100.000%	   100.000%	     0.000	        1

Timings (microseconds): count=1 curr=9163
Memory (bytes): count=0
1 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.016	    7.920	    7.913	  4.876%	  4.876%	     0.000	        1	[mobilenetv2_1.00_224/Conv1_relu/Relu6;mobilenetv2_1.00_224/bn_Conv1/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/Conv1/Conv2D]:0
	       DEPTHWISE_CONV_2D	            7.935	    1.471	    1.494	  0.920%	  5.796%	     0.000	        1	[mobilenetv2_1.00_224/expanded_conv_depthwise_relu/Relu6;mobilenetv2_1.00_224/expanded_conv_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/expanded_conv_depthwise/depthwise;mobilenetv2_1.00_224/block_5_project_BN/FusedBatchNormV3]:1
	                 CONV_2D	            9.431	    4.684	    4.732	  2.916%	  8.712%	     0.000	        1	[mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/expanded_conv_project/Conv2D]:2
	                 CONV_2D	           14.169	   10.112	   10.052	  6.194%	 14.905%	     0.000	        1	[mobilenetv2_1.00_224/block_1_expand_relu/Relu6;mobilenetv2_1.00_224/block_1_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_expand/Conv2D]:3
	                     PAD	           24.227	   27.117	   27.022	 16.649%	 31.555%	     0.000	        1	[mobilenetv2_1.00_224/block_1_pad/Pad]:4
	       DEPTHWISE_CONV_2D	           51.255	    2.357	    2.401	  1.480%	 33.034%	     0.000	        1	[mobilenetv2_1.00_224/block_1_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_1_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_depthwise/depthwise;mobilenetv2_1.00_224/block_12_project_BN/FusedBatchNormV3]:5
	                 CONV_2D	           53.660	    2.255	    2.234	  1.377%	 34.411%	     0.000	        1	[mobilenetv2_1.00_224/block_1_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_1_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_project/Conv2D]:6
	                 CONV_2D	           55.897	    4.410	    4.446	  2.739%	 37.150%	     0.000	        1	[mobilenetv2_1.00_224/block_2_expand_relu/Relu6;mobilenetv2_1.00_224/block_2_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_expand/Conv2D]:7
	       DEPTHWISE_CONV_2D	           60.347	    2.158	    2.233	  1.376%	 38.526%	     0.000	        1	[mobilenetv2_1.00_224/block_2_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_depthwise/depthwise;mobilenetv2_1.00_224/block_3_depthwise_BN/FusedBatchNormV3]:8
	                 CONV_2D	           62.583	    3.071	    2.967	  1.828%	 40.354%	     0.000	        1	[mobilenetv2_1.00_224/block_2_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_1_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_project/Conv2D]:9
	                     ADD	           65.557	    7.451	    7.492	  4.616%	 44.970%	     0.000	        1	[mobilenetv2_1.00_224/block_2_add/add]:10
	                 CONV_2D	           73.051	    4.507	    4.456	  2.745%	 47.716%	     0.000	        1	[mobilenetv2_1.00_224/block_3_expand_relu/Relu6;mobilenetv2_1.00_224/block_3_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_expand/Conv2D]:11
	                     PAD	           77.511	   10.295	   10.205	  6.288%	 54.003%	     0.000	        1	[mobilenetv2_1.00_224/block_3_pad/Pad]:12
	       DEPTHWISE_CONV_2D	           87.726	    0.931	    0.879	  0.542%	 54.545%	     0.000	        1	[mobilenetv2_1.00_224/block_3_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_3_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_depthwise/depthwise]:13
	                 CONV_2D	           88.607	    0.829	    0.820	  0.505%	 55.051%	     0.000	        1	[mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_project/Conv2D]:14
	                 CONV_2D	           89.430	    1.457	    1.434	  0.883%	 55.934%	     0.000	        1	[mobilenetv2_1.00_224/block_4_expand_relu/Relu6;mobilenetv2_1.00_224/block_4_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_4_expand/Conv2D]:15
	       DEPTHWISE_CONV_2D	           90.865	    0.705	    0.724	  0.446%	 56.380%	     0.000	        1	[mobilenetv2_1.00_224/block_4_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_4_depthwise/depthwise;mobilenetv2_1.00_224/block_6_depthwise_BN/FusedBatchNormV3]:16
	                 CONV_2D	           91.591	    0.870	    0.915	  0.564%	 56.944%	     0.000	        1	[mobilenetv2_1.00_224/block_4_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_4_project/Conv2D]:17
	                     ADD	           92.508	    2.484	    2.489	  1.534%	 58.478%	     0.000	        1	[mobilenetv2_1.00_224/block_4_add/add]:18
	                 CONV_2D	           94.998	    1.481	    1.509	  0.930%	 59.408%	     0.000	        1	[mobilenetv2_1.00_224/block_5_expand_relu/Relu6;mobilenetv2_1.00_224/block_5_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_5_expand/Conv2D]:19
	       DEPTHWISE_CONV_2D	           96.509	    0.694	    0.699	  0.431%	 59.839%	     0.000	        1	[mobilenetv2_1.00_224/block_5_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_5_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_5_depthwise/depthwise;mobilenetv2_1.00_224/block_6_depthwise_BN/FusedBatchNormV3]:20
	                 CONV_2D	           97.209	    0.885	    0.877	  0.541%	 60.379%	     0.000	        1	[mobilenetv2_1.00_224/block_5_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_5_project/Conv2D]:21
	                     ADD	           98.089	    2.497	    2.493	  1.536%	 61.915%	     0.000	        1	[mobilenetv2_1.00_224/block_5_add/add]:22
	                 CONV_2D	          100.583	    1.432	    1.433	  0.883%	 62.798%	     0.000	        1	[mobilenetv2_1.00_224/block_6_expand_relu/Relu6;mobilenetv2_1.00_224/block_6_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_6_expand/Conv2D]:23
	                     PAD	          102.018	    3.462	    3.468	  2.137%	 64.935%	     0.000	        1	[mobilenetv2_1.00_224/block_6_pad/Pad]:24
	       DEPTHWISE_CONV_2D	          105.487	    0.241	    0.239	  0.147%	 65.083%	     0.000	        1	[mobilenetv2_1.00_224/block_6_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_6_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_4_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_6_depthwise/depthwise]:25
	                 CONV_2D	          105.728	    0.424	    0.419	  0.258%	 65.340%	     0.000	        1	[mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_6_project/Conv2D]:26
	                 CONV_2D	          106.148	    1.074	    1.061	  0.654%	 65.994%	     0.000	        1	[mobilenetv2_1.00_224/block_7_expand_relu/Relu6;mobilenetv2_1.00_224/block_7_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_7_expand/Conv2D]:27
	       DEPTHWISE_CONV_2D	          107.210	    0.322	    0.327	  0.201%	 66.196%	     0.000	        1	[mobilenetv2_1.00_224/block_7_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_7_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_7_depthwise/depthwise;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3]:28
	                 CONV_2D	          107.538	    0.725	    0.732	  0.451%	 66.647%	     0.000	        1	[mobilenetv2_1.00_224/block_7_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_7_project/Conv2D]:29
	                     ADD	          108.272	    1.236	    1.251	  0.771%	 67.417%	     0.000	        1	[mobilenetv2_1.00_224/block_7_add/add]:30
	                 CONV_2D	          109.524	    1.129	    1.064	  0.656%	 68.073%	     0.000	        1	[mobilenetv2_1.00_224/block_8_expand_relu/Relu6;mobilenetv2_1.00_224/block_8_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_8_expand/Conv2D]:31
	       DEPTHWISE_CONV_2D	          110.589	    0.332	    0.321	  0.198%	 68.271%	     0.000	        1	[mobilenetv2_1.00_224/block_8_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_8_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_8_depthwise/depthwise;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3]:32
	                 CONV_2D	          110.912	    0.711	    0.723	  0.446%	 68.717%	     0.000	        1	[mobilenetv2_1.00_224/block_8_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_8_project/Conv2D]:33
	                     ADD	          111.637	    1.233	    1.238	  0.763%	 69.480%	     0.000	        1	[mobilenetv2_1.00_224/block_8_add/add]:34
	                 CONV_2D	          112.876	    1.036	    1.048	  0.646%	 70.125%	     0.000	        1	[mobilenetv2_1.00_224/block_9_expand_relu/Relu6;mobilenetv2_1.00_224/block_9_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_9_expand/Conv2D]:35
	       DEPTHWISE_CONV_2D	          113.925	    0.320	    0.328	  0.202%	 70.327%	     0.000	        1	[mobilenetv2_1.00_224/block_9_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_9_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_9_depthwise/depthwise;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3]:36
	                 CONV_2D	          114.254	    0.726	    0.720	  0.444%	 70.771%	     0.000	        1	[mobilenetv2_1.00_224/block_9_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_6_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_9_project/Conv2D]:37
	                     ADD	          114.976	    1.233	    1.245	  0.767%	 71.539%	     0.000	        1	[mobilenetv2_1.00_224/block_9_add/add]:38
	                 CONV_2D	          116.223	    1.031	    1.038	  0.640%	 72.178%	     0.000	        1	[mobilenetv2_1.00_224/block_10_expand_relu/Relu6;mobilenetv2_1.00_224/block_10_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_10_expand/Conv2D]:39
	       DEPTHWISE_CONV_2D	          117.263	    0.327	    0.323	  0.199%	 72.377%	     0.000	        1	[mobilenetv2_1.00_224/block_10_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_10_depthwise/depthwise]:40
	                 CONV_2D	          117.587	    1.113	    1.059	  0.652%	 73.030%	     0.000	        1	[mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_10_project/Conv2D]:41
	                 CONV_2D	          118.647	    2.009	    2.040	  1.257%	 74.287%	     0.000	        1	[mobilenetv2_1.00_224/block_11_expand_relu/Relu6;mobilenetv2_1.00_224/block_11_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_11_expand/Conv2D]:42
	       DEPTHWISE_CONV_2D	          120.689	    0.473	    0.487	  0.300%	 74.587%	     0.000	        1	[mobilenetv2_1.00_224/block_11_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_11_depthwise/depthwise;mobilenetv2_1.00_224/block_13_depthwise_BN/FusedBatchNormV3]:43
	                 CONV_2D	          121.177	    1.589	    1.549	  0.954%	 75.541%	     0.000	        1	[mobilenetv2_1.00_224/block_11_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_11_project/Conv2D]:44
	                     ADD	          122.729	    1.856	    1.874	  1.155%	 76.696%	     0.000	        1	[mobilenetv2_1.00_224/block_11_add/add]:45
	                 CONV_2D	          124.604	    2.024	    2.035	  1.254%	 77.950%	     0.000	        1	[mobilenetv2_1.00_224/block_12_expand_relu/Relu6;mobilenetv2_1.00_224/block_12_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_12_expand/Conv2D]:46
	       DEPTHWISE_CONV_2D	          126.641	    0.479	    0.478	  0.294%	 78.244%	     0.000	        1	[mobilenetv2_1.00_224/block_12_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_12_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_12_depthwise/depthwise;mobilenetv2_1.00_224/block_13_depthwise_BN/FusedBatchNormV3]:47
	                 CONV_2D	          127.120	    1.514	    1.535	  0.946%	 79.190%	     0.000	        1	[mobilenetv2_1.00_224/block_12_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_12_project/Conv2D]:48
	                     ADD	          128.658	    1.843	    1.864	  1.148%	 80.338%	     0.000	        1	[mobilenetv2_1.00_224/block_12_add/add]:49
	                 CONV_2D	          130.523	    2.026	    2.048	  1.262%	 81.600%	     0.000	        1	[mobilenetv2_1.00_224/block_13_expand_relu/Relu6;mobilenetv2_1.00_224/block_13_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_13_expand/Conv2D]:50
	                     PAD	          132.572	    2.785	    2.750	  1.694%	 83.294%	     0.000	        1	[mobilenetv2_1.00_224/block_13_pad/Pad]:51
	       DEPTHWISE_CONV_2D	          135.324	    0.179	    0.176	  0.109%	 83.403%	     0.000	        1	[mobilenetv2_1.00_224/block_13_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_13_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_11_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_13_depthwise/depthwise]:52
	                 CONV_2D	          135.501	    0.737	    0.743	  0.458%	 83.861%	     0.000	        1	[mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_13_project/Conv2D]:53
	                 CONV_2D	          136.246	    1.380	    1.406	  0.866%	 84.727%	     0.000	        1	[mobilenetv2_1.00_224/block_14_expand_relu/Relu6;mobilenetv2_1.00_224/block_14_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_14_expand/Conv2D]:54
	       DEPTHWISE_CONV_2D	          137.653	    0.234	    0.241	  0.148%	 84.875%	     0.000	        1	[mobilenetv2_1.00_224/block_14_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_14_depthwise/depthwise;mobilenetv2_1.00_224/block_16_depthwise_BN/FusedBatchNormV3]:55
	                 CONV_2D	          137.895	    1.226	    1.200	  0.740%	 85.615%	     0.000	        1	[mobilenetv2_1.00_224/block_14_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_14_project/Conv2D]:56
	                     ADD	          139.098	    0.778	    0.795	  0.490%	 86.105%	     0.000	        1	[mobilenetv2_1.00_224/block_14_add/add]:57
	                 CONV_2D	          139.894	    1.403	    1.425	  0.878%	 86.983%	     0.000	        1	[mobilenetv2_1.00_224/block_15_expand_relu/Relu6;mobilenetv2_1.00_224/block_15_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_15_expand/Conv2D]:58
	       DEPTHWISE_CONV_2D	          141.322	    0.236	    0.238	  0.146%	 87.129%	     0.000	        1	[mobilenetv2_1.00_224/block_15_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_15_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_15_depthwise/depthwise;mobilenetv2_1.00_224/block_16_depthwise_BN/FusedBatchNormV3]:59
	                 CONV_2D	          141.560	    1.147	    1.143	  0.704%	 87.833%	     0.000	        1	[mobilenetv2_1.00_224/block_15_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_13_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_15_project/Conv2D]:60
	                     ADD	          142.705	    0.789	    0.804	  0.496%	 88.329%	     0.000	        1	[mobilenetv2_1.00_224/block_15_add/add]:61
	                 CONV_2D	          143.510	    1.410	    1.408	  0.867%	 89.196%	     0.000	        1	[mobilenetv2_1.00_224/block_16_expand_relu/Relu6;mobilenetv2_1.00_224/block_16_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_16_expand/Conv2D]:62
	       DEPTHWISE_CONV_2D	          144.920	    0.241	    0.245	  0.151%	 89.347%	     0.000	        1	[mobilenetv2_1.00_224/block_16_depthwise_relu/Relu6;mobilenetv2_1.00_224/block_16_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_14_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_16_depthwise/depthwise]:63
	                 CONV_2D	          145.166	    2.394	    2.319	  1.429%	 90.776%	     0.000	        1	[mobilenetv2_1.00_224/block_16_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_16_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_16_project/Conv2D]:64
	                 CONV_2D	          147.488	    3.481	    3.389	  2.088%	 92.865%	     0.000	        1	[mobilenetv2_1.00_224/out_relu/Relu6;mobilenetv2_1.00_224/Conv_1_bn/FusedBatchNormV3;mobilenetv2_1.00_224/Conv_1_bn/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/Conv_1/Conv2D]:65
	                    MEAN	          150.883	   10.818	   10.836	  6.676%	 99.541%	     0.000	        1	[mobilenetv2_1.00_224/global_average_pooling2d/Mean]:66
	         FULLY_CONNECTED	          161.720	    0.634	    0.661	  0.407%	 99.948%	     0.000	        1	[mobilenetv2_1.00_224/predictions/MatMul;mobilenetv2_1.00_224/predictions/BiasAdd]:67
	                 SOFTMAX	          162.385	    0.135	    0.084	  0.052%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:68

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                     PAD	           24.227	   27.117	   27.022	 16.649%	 16.649%	     0.000	        1	[mobilenetv2_1.00_224/block_1_pad/Pad]:4
	                    MEAN	          150.883	   10.818	   10.836	  6.676%	 23.326%	     0.000	        1	[mobilenetv2_1.00_224/global_average_pooling2d/Mean]:66
	                     PAD	           77.511	   10.295	   10.205	  6.288%	 29.613%	     0.000	        1	[mobilenetv2_1.00_224/block_3_pad/Pad]:12
	                 CONV_2D	           14.169	   10.112	   10.052	  6.194%	 35.807%	     0.000	        1	[mobilenetv2_1.00_224/block_1_expand_relu/Relu6;mobilenetv2_1.00_224/block_1_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_10_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_1_expand/Conv2D]:3
	                 CONV_2D	            0.016	    7.920	    7.913	  4.876%	 40.683%	     0.000	        1	[mobilenetv2_1.00_224/Conv1_relu/Relu6;mobilenetv2_1.00_224/bn_Conv1/FusedBatchNormV3;mobilenetv2_1.00_224/block_3_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/Conv1/Conv2D]:0
	                     ADD	           65.557	    7.451	    7.492	  4.616%	 45.299%	     0.000	        1	[mobilenetv2_1.00_224/block_2_add/add]:10
	                 CONV_2D	            9.431	    4.684	    4.732	  2.916%	 48.215%	     0.000	        1	[mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3;mobilenetv2_1.00_224/expanded_conv_project_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/expanded_conv_project/Conv2D]:2
	                 CONV_2D	           73.051	    4.507	    4.456	  2.745%	 50.960%	     0.000	        1	[mobilenetv2_1.00_224/block_3_expand_relu/Relu6;mobilenetv2_1.00_224/block_3_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_3_expand/Conv2D]:11
	                 CONV_2D	           55.897	    4.410	    4.446	  2.739%	 53.699%	     0.000	        1	[mobilenetv2_1.00_224/block_2_expand_relu/Relu6;mobilenetv2_1.00_224/block_2_expand_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_2_depthwise_BN/FusedBatchNormV3/ReadVariableOp;mobilenetv2_1.00_224/block_2_expand/Conv2D]:7
	                     PAD	          102.018	    3.462	    3.468	  2.137%	 55.836%	     0.000	        1	[mobilenetv2_1.00_224/block_6_pad/Pad]:24

Number of nodes executed: 69
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       35	    73.879	    45.528%	    45.528%	     0.000	       35
	                     PAD	        4	    43.445	    26.773%	    72.300%	     0.000	        4
	                     ADD	       10	    21.544	    13.276%	    85.577%	     0.000	       10
	       DEPTHWISE_CONV_2D	       17	    11.826	     7.288%	    92.864%	     0.000	       17
	                    MEAN	        1	    10.835	     6.677%	    99.542%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.660	     0.407%	    99.948%	     0.000	        1
	                 SOFTMAX	        1	     0.084	     0.052%	   100.000%	     0.000	        1

Timings (microseconds): count=20 first=162568 curr=162509 min=161872 max=162624 avg=162301 std=213
Memory (bytes): count=0
69 nodes observed



[ perf record: Woken up 14 times to write data ]
[ perf record: Captured and wrote 3.345 MB /tmp/data.record (14648 samples) ]

4.657

